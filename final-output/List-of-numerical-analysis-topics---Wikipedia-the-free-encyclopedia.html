<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>List-of-numerical-analysis-topics---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>List of numerical analysis topics</h1>
<p>This is a <b>list of numerical analysis topics</b>.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 General</li>
<li>2 Error</li>
<li>3 Elementary and special functions</li>
<li>4 Numerical linear algebra
<ul>
<li>4.1 Basic concepts</li>
<li>4.2 Solving systems of linear equations</li>
<li>4.3 Eigenvalue algorithms</li>
<li>4.4 Other concepts and algorithms</li>
</ul>
</li>
<li>5 Interpolation and approximation
<ul>
<li>5.1 Polynomial interpolation</li>
<li>5.2 Spline interpolation</li>
<li>5.3 Trigonometric interpolation</li>
<li>5.4 Other interpolants</li>
<li>5.5 Approximation theory</li>
<li>5.6 Miscellaneous</li>
</ul>
</li>
<li>6 Finding roots of nonlinear equations</li>
<li>7 Optimization
<ul>
<li>7.1 Basic concepts</li>
<li>7.2 Linear programming</li>
<li>7.3 Convex optimization</li>
<li>7.4 Nonlinear programming</li>
<li>7.5 Optimal control and infinite-dimensional optimization</li>
<li>7.6 Uncertainty and randomness</li>
<li>7.7 Theoretical aspects</li>
<li>7.8 Applications</li>
<li>7.9 Miscellaneous</li>
</ul>
</li>
<li>8 Numerical quadrature (integration)</li>
<li>9 Numerical methods for ordinary differential equations</li>
<li>10 Numerical methods for partial differential equations
<ul>
<li>10.1 Finite difference methods</li>
<li>10.2 Finite element methods</li>
<li>10.3 Other methods</li>
<li>10.4 Techniques for improving these methods</li>
<li>10.5 Grids and meshes</li>
<li>10.6 Analysis</li>
</ul>
</li>
<li>11 Monte Carlo method</li>
<li>12 Applications</li>
<li>13 Software</li>
</ul>
<ul>
<li>4.1 Basic concepts</li>
<li>4.2 Solving systems of linear equations</li>
<li>4.3 Eigenvalue algorithms</li>
<li>4.4 Other concepts and algorithms</li>
</ul>
<ul>
<li>5.1 Polynomial interpolation</li>
<li>5.2 Spline interpolation</li>
<li>5.3 Trigonometric interpolation</li>
<li>5.4 Other interpolants</li>
<li>5.5 Approximation theory</li>
<li>5.6 Miscellaneous</li>
</ul>
<ul>
<li>7.1 Basic concepts</li>
<li>7.2 Linear programming</li>
<li>7.3 Convex optimization</li>
<li>7.4 Nonlinear programming</li>
<li>7.5 Optimal control and infinite-dimensional optimization</li>
<li>7.6 Uncertainty and randomness</li>
<li>7.7 Theoretical aspects</li>
<li>7.8 Applications</li>
<li>7.9 Miscellaneous</li>
</ul>
<ul>
<li>10.1 Finite difference methods</li>
<li>10.2 Finite element methods</li>
<li>10.3 Other methods</li>
<li>10.4 Techniques for improving these methods</li>
<li>10.5 Grids and meshes</li>
<li>10.6 Analysis</li>
</ul>
<p></p>
<h2>General</h2>
<ul>
<li>Iterative method</li>
<li>Rate of convergence — the speed at which a convergent sequence approaches its limit
<ul>
<li>Order of accuracy — rate at which numerical solution of differential equation converges to exact solution</li>
</ul>
</li>
<li>Series acceleration — methods to accelerate the speed of convergence of a series
<ul>
<li>Aitken's delta-squared process — most useful for linearly converging sequences</li>
<li>Minimum polynomial extrapolation — for vector sequences</li>
<li>Richardson extrapolation</li>
<li>Shanks transformation — similar to Aitken's delta-squared process, but applied to the partial sums</li>
<li>Van Wijngaarden transformation — for accelerating the convergence of an alternating series</li>
</ul>
</li>
<li>Abramowitz and Stegun — book containing formulas and tables of many special functions
<ul>
<li>Digital Library of Mathematical Functions — successor of book by Abramowitz and Stegun</li>
</ul>
</li>
<li>Curse of dimensionality</li>
<li>Local convergence and global convergence — whether you need a good initial guess to get convergence</li>
<li>Superconvergence</li>
<li>Discretization</li>
<li>Difference quotient</li>
<li>Complexity:
<ul>
<li>Computational complexity of mathematical operations</li>
<li>Smoothed analysis — measuring the expected performance of algorithms under slight random perturbations of worst-case inputs</li>
</ul>
</li>
<li>Symbolic-numeric computation — combination of symbolic and numeric methods</li>
<li>Cultural and historical aspects:
<ul>
<li>History of numerical solution of differential equations using computers</li>
<li>Hundred-dollar, Hundred-digit Challenge problems — list of ten problems proposed by Nick Trefethen in 2002</li>
<li>International Workshops on Lattice QCD and Numerical Analysis</li>
<li>Timeline of numerical analysis after 1945</li>
</ul>
</li>
<li>General classes of methods:
<ul>
<li>Collocation method — discretizes a continuous equation by requiring it only to hold at certain points</li>
<li>Level set method
<ul>
<li>Level set (data structures) — data structures for representing level sets</li>
</ul>
</li>
<li>Sinc numerical methods — methods based on the sinc function, sinc(<i>x</i>) = sin(<i>x</i>) / <i>x</i></li>
<li>ABS methods</li>
</ul>
</li>
</ul>
<ul>
<li>Order of accuracy — rate at which numerical solution of differential equation converges to exact solution</li>
</ul>
<ul>
<li>Aitken's delta-squared process — most useful for linearly converging sequences</li>
<li>Minimum polynomial extrapolation — for vector sequences</li>
<li>Richardson extrapolation</li>
<li>Shanks transformation — similar to Aitken's delta-squared process, but applied to the partial sums</li>
<li>Van Wijngaarden transformation — for accelerating the convergence of an alternating series</li>
</ul>
<ul>
<li>Digital Library of Mathematical Functions — successor of book by Abramowitz and Stegun</li>
</ul>
<ul>
<li>Computational complexity of mathematical operations</li>
<li>Smoothed analysis — measuring the expected performance of algorithms under slight random perturbations of worst-case inputs</li>
</ul>
<ul>
<li>History of numerical solution of differential equations using computers</li>
<li>Hundred-dollar, Hundred-digit Challenge problems — list of ten problems proposed by Nick Trefethen in 2002</li>
<li>International Workshops on Lattice QCD and Numerical Analysis</li>
<li>Timeline of numerical analysis after 1945</li>
</ul>
<ul>
<li>Collocation method — discretizes a continuous equation by requiring it only to hold at certain points</li>
<li>Level set method
<ul>
<li>Level set (data structures) — data structures for representing level sets</li>
</ul>
</li>
<li>Sinc numerical methods — methods based on the sinc function, sinc(<i>x</i>) = sin(<i>x</i>) / <i>x</i></li>
<li>ABS methods</li>
</ul>
<ul>
<li>Level set (data structures) — data structures for representing level sets</li>
</ul>
<h2>Error</h2>
<p>Error analysis (mathematics)</p>
<ul>
<li>Approximation</li>
<li>Approximation error</li>
<li>Condition number</li>
<li>Discretization error</li>
<li>Floating point number
<ul>
<li>Guard digit — extra precision introduced during a computation to reduce round-off error</li>
<li>Truncation — rounding a floating-point number by discarding all digits after a certain digit</li>
<li>Round-off error
<ul>
<li>Numeric precision in Microsoft Excel</li>
</ul>
</li>
<li>Arbitrary-precision arithmetic</li>
</ul>
</li>
<li>Interval arithmetic — represent every number by two floating-point numbers guaranteed to have the unknown number between them
<ul>
<li>Interval contractor — maps interval to subinterval which still contains the unknown exact answer</li>
<li>Interval propagation — contracting interval domains without removing any value consistent with the constraints
<ul>
<li>See also: Interval boundary element method, Interval finite element</li>
</ul>
</li>
</ul>
</li>
<li>Loss of significance</li>
<li>Numerical error</li>
<li>Numerical stability</li>
<li>Error propagation:
<ul>
<li>Propagation of uncertainty
<ul>
<li>List of uncertainty propagation software</li>
</ul>
</li>
<li>Significance arithmetic</li>
<li>Residual (numerical analysis)</li>
</ul>
</li>
<li>Relative change and difference — the relative difference between <i>x</i> and <i>y</i> is |<i>x</i> − <i>y</i>| / max(|<i>x</i>|, |<i>y</i>|)</li>
<li>Significant figures
<ul>
<li>False precision — giving more significant figures than appropriate</li>
</ul>
</li>
<li>Truncation error — error committed by doing only a finite numbers of steps</li>
<li>Well-posed problem</li>
<li>Affine arithmetic</li>
</ul>
<ul>
<li>Guard digit — extra precision introduced during a computation to reduce round-off error</li>
<li>Truncation — rounding a floating-point number by discarding all digits after a certain digit</li>
<li>Round-off error
<ul>
<li>Numeric precision in Microsoft Excel</li>
</ul>
</li>
<li>Arbitrary-precision arithmetic</li>
</ul>
<ul>
<li>Numeric precision in Microsoft Excel</li>
</ul>
<ul>
<li>Interval contractor — maps interval to subinterval which still contains the unknown exact answer</li>
<li>Interval propagation — contracting interval domains without removing any value consistent with the constraints
<ul>
<li>See also: Interval boundary element method, Interval finite element</li>
</ul>
</li>
</ul>
<ul>
<li>See also: Interval boundary element method, Interval finite element</li>
</ul>
<ul>
<li>Propagation of uncertainty
<ul>
<li>List of uncertainty propagation software</li>
</ul>
</li>
<li>Significance arithmetic</li>
<li>Residual (numerical analysis)</li>
</ul>
<ul>
<li>List of uncertainty propagation software</li>
</ul>
<ul>
<li>False precision — giving more significant figures than appropriate</li>
</ul>
<h2>Elementary and special functions</h2>
<ul>
<li>Summation:
<ul>
<li>Kahan summation algorithm</li>
<li>Pairwise summation — slightly worse than Kahan summation but cheaper</li>
<li>Binary splitting</li>
</ul>
</li>
<li>Multiplication:
<ul>
<li>Multiplication algorithm — general discussion, simple methods</li>
<li>Karatsuba algorithm — the first algorithm which is faster than straightforward multiplication</li>
<li>Toom–Cook multiplication — generalization of Karatsuba multiplication</li>
<li>Schönhage–Strassen algorithm — based on Fourier transform, asymptotically very fast</li>
<li>Fürer's algorithm — asymptotically slightly faster than Schönhage–Strassen</li>
</ul>
</li>
<li>Division algorithm — for computing quotient and/or remainder of two numbers
<ul>
<li>Long division</li>
<li>Restoring division</li>
<li>Non-restoring division</li>
<li>SRT division</li>
<li>Newton–Raphson division: uses Newton's method to find the reciprocal of D, and multiply that reciprocal by N to find the final quotient Q.</li>
<li>Goldschmidt division</li>
</ul>
</li>
<li>Exponentiation:
<ul>
<li>Exponentiation by squaring</li>
<li>Addition-chain exponentiation</li>
</ul>
</li>
<li>Multiplicative inverse Algorithms: for computing a number's multiplicative inverse (reciprocal).
<ul>
<li>Newton's method</li>
</ul>
</li>
<li>Polynomials:
<ul>
<li>Horner's method</li>
<li>Estrin's scheme — modification of the Horner scheme with more possibilities for parallelization</li>
<li>Clenshaw algorithm</li>
<li>De Casteljau's algorithm</li>
</ul>
</li>
<li>Square roots and other roots:
<ul>
<li>Integer square root</li>
<li>Methods of computing square roots</li>
<li><i>n</i>th root algorithm</li>
<li>Shifting <i>n</i>th root algorithm — similar to long division</li>
<li>hypot — the function (<i>x</i> + <i>y</i>)</li>
<li>Alpha max plus beta min algorithm — approximates hypot(x,y)</li>
<li>Fast inverse square root — calculates 1 / √<i>x</i> using details of the IEEE floating-point system</li>
</ul>
</li>
<li>Elementary functions (exponential, logarithm, trigonometric functions):
<ul>
<li>Trigonometric tables — different methods for generating them</li>
<li>CORDIC — shift-and-add algorithm using a table of arc tangents</li>
<li>BKM algorithm — shift-and-add algorithm using a table of logarithms and complex numbers</li>
</ul>
</li>
<li>Gamma function:
<ul>
<li>Lanczos approximation</li>
<li>Spouge's approximation — modification of Stirling's approximation; easier to apply than Lanczos</li>
</ul>
</li>
<li>AGM method — computes arithmetic–geometric mean; related methods compute special functions</li>
<li>FEE method (Fast E-function Evaluation) — fast summation of series like the power series for e</li>
<li>Gal's accurate tables — table of function values with unequal spacing to reduce round-off error</li>
<li>Spigot algorithm — algorithms that can compute individual digits of a real number</li>
<li>Approximations of π:
<ul>
<li>Liu Hui's π algorithm — first algorithm that can compute π to arbitrary precision</li>
<li>Leibniz formula for π — alternating series with very slow convergence</li>
<li>Wallis product — infinite product converging slowly to π/2</li>
<li>Viète's formula — more complicated infinite product which converges faster</li>
<li>Gauss–Legendre algorithm — iteration which converges quadratically to π, based on arithmetic–geometric mean</li>
<li>Borwein's algorithm — iteration which converges quartically to 1/π, and other algorithms</li>
<li>Chudnovsky algorithm — fast algorithm that calculates a hypergeometric series</li>
<li>Bailey–Borwein–Plouffe formula — can be used to compute individual hexadecimal digits of π</li>
<li>Bellard's formula — faster version of Bailey–Borwein–Plouffe formula</li>
<li>List of formulae involving π</li>
</ul>
</li>
</ul>
<ul>
<li>Kahan summation algorithm</li>
<li>Pairwise summation — slightly worse than Kahan summation but cheaper</li>
<li>Binary splitting</li>
</ul>
<ul>
<li>Multiplication algorithm — general discussion, simple methods</li>
<li>Karatsuba algorithm — the first algorithm which is faster than straightforward multiplication</li>
<li>Toom–Cook multiplication — generalization of Karatsuba multiplication</li>
<li>Schönhage–Strassen algorithm — based on Fourier transform, asymptotically very fast</li>
<li>Fürer's algorithm — asymptotically slightly faster than Schönhage–Strassen</li>
</ul>
<ul>
<li>Long division</li>
<li>Restoring division</li>
<li>Non-restoring division</li>
<li>SRT division</li>
<li>Newton–Raphson division: uses Newton's method to find the reciprocal of D, and multiply that reciprocal by N to find the final quotient Q.</li>
<li>Goldschmidt division</li>
</ul>
<ul>
<li>Exponentiation by squaring</li>
<li>Addition-chain exponentiation</li>
</ul>
<ul>
<li>Newton's method</li>
</ul>
<ul>
<li>Horner's method</li>
<li>Estrin's scheme — modification of the Horner scheme with more possibilities for parallelization</li>
<li>Clenshaw algorithm</li>
<li>De Casteljau's algorithm</li>
</ul>
<ul>
<li>Integer square root</li>
<li>Methods of computing square roots</li>
<li><i>n</i>th root algorithm</li>
<li>Shifting <i>n</i>th root algorithm — similar to long division</li>
<li>hypot — the function (<i>x</i> + <i>y</i>)</li>
<li>Alpha max plus beta min algorithm — approximates hypot(x,y)</li>
<li>Fast inverse square root — calculates 1 / √<i>x</i> using details of the IEEE floating-point system</li>
</ul>
<ul>
<li>Trigonometric tables — different methods for generating them</li>
<li>CORDIC — shift-and-add algorithm using a table of arc tangents</li>
<li>BKM algorithm — shift-and-add algorithm using a table of logarithms and complex numbers</li>
</ul>
<ul>
<li>Lanczos approximation</li>
<li>Spouge's approximation — modification of Stirling's approximation; easier to apply than Lanczos</li>
</ul>
<ul>
<li>Liu Hui's π algorithm — first algorithm that can compute π to arbitrary precision</li>
<li>Leibniz formula for π — alternating series with very slow convergence</li>
<li>Wallis product — infinite product converging slowly to π/2</li>
<li>Viète's formula — more complicated infinite product which converges faster</li>
<li>Gauss–Legendre algorithm — iteration which converges quadratically to π, based on arithmetic–geometric mean</li>
<li>Borwein's algorithm — iteration which converges quartically to 1/π, and other algorithms</li>
<li>Chudnovsky algorithm — fast algorithm that calculates a hypergeometric series</li>
<li>Bailey–Borwein–Plouffe formula — can be used to compute individual hexadecimal digits of π</li>
<li>Bellard's formula — faster version of Bailey–Borwein–Plouffe formula</li>
<li>List of formulae involving π</li>
</ul>
<h2>Numerical linear algebra</h2>
<p>Numerical linear algebra — study of numerical algorithms for linear algebra problems</p>
<h3>Basic concepts</h3>
<ul>
<li>Types of matrices appearing in numerical analysis:
<ul>
<li>Sparse matrix
<ul>
<li>Band matrix</li>
<li>Bidiagonal matrix</li>
<li>Tridiagonal matrix</li>
<li>Pentadiagonal matrix</li>
<li>Skyline matrix</li>
</ul>
</li>
<li>Circulant matrix</li>
<li>Triangular matrix</li>
<li>Diagonally dominant matrix</li>
<li>Block matrix — matrix composed of smaller matrices</li>
<li>Stieltjes matrix — symmetric positive definite with non-positive off-diagonal entries</li>
<li>Hilbert matrix — example of a matrix which is extremely ill-conditioned (and thus difficult to handle)</li>
<li>Wilkinson matrix — example of a symmetric tridiagonal matrix with pairs of nearly, but not exactly, equal eigenvalues</li>
<li>Convergent matrix – square matrix whose successive powers approach the zero matrix</li>
</ul>
</li>
<li>Algorithms for matrix multiplication:
<ul>
<li>Strassen algorithm</li>
<li>Coppersmith–Winograd algorithm</li>
<li>Cannon's algorithm — a distributed algorithm, especially suitable for processors laid out in a 2d grid</li>
<li>Freivalds' algorithm — a randomized algorithm for checking the result of a multiplication</li>
</ul>
</li>
<li>Matrix decompositions:
<ul>
<li>LU decomposition — lower triangular times upper triangular</li>
<li>QR decomposition — orthogonal matrix times triangular matrix
<ul>
<li>RRQR factorization — rank-revealing QR factorization, can be used to compute rank of a matrix</li>
</ul>
</li>
<li>Polar decomposition — unitary matrix times positive-semidefinite Hermitian matrix</li>
<li>Decompositions by similarity:
<ul>
<li>Eigendecomposition — decomposition in terms of eigenvectors and eigenvalues</li>
<li>Jordan normal form — bidiagonal matrix of a certain form; generalizes the eigendecomposition
<ul>
<li>Weyr canonical form — permutation of Jordan normal form</li>
</ul>
</li>
<li>Jordan–Chevalley decomposition — sum of commuting nilpotent matrix and diagonalizable matrix</li>
<li>Schur decomposition — similarity transform bringing the matrix to a triangular matrix</li>
</ul>
</li>
<li>Singular value decomposition — unitary matrix times diagonal matrix times unitary matrix</li>
</ul>
</li>
<li>Matrix splitting – expressing a given matrix as a sum or difference of matrices</li>
</ul>
<ul>
<li>Sparse matrix
<ul>
<li>Band matrix</li>
<li>Bidiagonal matrix</li>
<li>Tridiagonal matrix</li>
<li>Pentadiagonal matrix</li>
<li>Skyline matrix</li>
</ul>
</li>
<li>Circulant matrix</li>
<li>Triangular matrix</li>
<li>Diagonally dominant matrix</li>
<li>Block matrix — matrix composed of smaller matrices</li>
<li>Stieltjes matrix — symmetric positive definite with non-positive off-diagonal entries</li>
<li>Hilbert matrix — example of a matrix which is extremely ill-conditioned (and thus difficult to handle)</li>
<li>Wilkinson matrix — example of a symmetric tridiagonal matrix with pairs of nearly, but not exactly, equal eigenvalues</li>
<li>Convergent matrix – square matrix whose successive powers approach the zero matrix</li>
</ul>
<ul>
<li>Band matrix</li>
<li>Bidiagonal matrix</li>
<li>Tridiagonal matrix</li>
<li>Pentadiagonal matrix</li>
<li>Skyline matrix</li>
</ul>
<ul>
<li>Strassen algorithm</li>
<li>Coppersmith–Winograd algorithm</li>
<li>Cannon's algorithm — a distributed algorithm, especially suitable for processors laid out in a 2d grid</li>
<li>Freivalds' algorithm — a randomized algorithm for checking the result of a multiplication</li>
</ul>
<ul>
<li>LU decomposition — lower triangular times upper triangular</li>
<li>QR decomposition — orthogonal matrix times triangular matrix
<ul>
<li>RRQR factorization — rank-revealing QR factorization, can be used to compute rank of a matrix</li>
</ul>
</li>
<li>Polar decomposition — unitary matrix times positive-semidefinite Hermitian matrix</li>
<li>Decompositions by similarity:
<ul>
<li>Eigendecomposition — decomposition in terms of eigenvectors and eigenvalues</li>
<li>Jordan normal form — bidiagonal matrix of a certain form; generalizes the eigendecomposition
<ul>
<li>Weyr canonical form — permutation of Jordan normal form</li>
</ul>
</li>
<li>Jordan–Chevalley decomposition — sum of commuting nilpotent matrix and diagonalizable matrix</li>
<li>Schur decomposition — similarity transform bringing the matrix to a triangular matrix</li>
</ul>
</li>
<li>Singular value decomposition — unitary matrix times diagonal matrix times unitary matrix</li>
</ul>
<ul>
<li>RRQR factorization — rank-revealing QR factorization, can be used to compute rank of a matrix</li>
</ul>
<ul>
<li>Eigendecomposition — decomposition in terms of eigenvectors and eigenvalues</li>
<li>Jordan normal form — bidiagonal matrix of a certain form; generalizes the eigendecomposition
<ul>
<li>Weyr canonical form — permutation of Jordan normal form</li>
</ul>
</li>
<li>Jordan–Chevalley decomposition — sum of commuting nilpotent matrix and diagonalizable matrix</li>
<li>Schur decomposition — similarity transform bringing the matrix to a triangular matrix</li>
</ul>
<ul>
<li>Weyr canonical form — permutation of Jordan normal form</li>
</ul>
<h3>Solving systems of linear equations</h3>
<ul>
<li>Gaussian elimination
<ul>
<li>Row echelon form — matrix in which all entries below a nonzero entry are zero</li>
<li>Bareiss algorithm — variant which ensures that all entries remain integers if the initial matrix has integer entries</li>
<li>Tridiagonal matrix algorithm — simplified form of Gaussian elimination for tridiagonal matrices</li>
</ul>
</li>
<li>LU decomposition — write a matrix as a product of an upper- and a lower-triangular matrix
<ul>
<li>Crout matrix decomposition</li>
<li>LU reduction — a special parallelized version of a LU decomposition algorithm</li>
</ul>
</li>
<li>Block LU decomposition</li>
<li>Cholesky decomposition — for solving a system with a positive definite matrix
<ul>
<li>Minimum degree algorithm</li>
<li>Symbolic Cholesky decomposition</li>
</ul>
</li>
<li>Iterative refinement — procedure to turn an inaccurate solution in a more accurate one</li>
<li>Direct methods for sparse matrices:
<ul>
<li>Frontal solver — used in finite element methods</li>
<li>Nested dissection — for symmetric matrices, based on graph partitioning</li>
</ul>
</li>
<li>Levinson recursion — for Toeplitz matrices</li>
<li>SPIKE algorithm — hybrid parallel solver for narrow-banded matrices</li>
<li>Cyclic reduction — eliminate even or odd rows or columns, repeat</li>
<li>Iterative methods:
<ul>
<li>Jacobi method</li>
<li>Gauss–Seidel method
<ul>
<li>Successive over-relaxation (SOR) — a technique to accelerate the Gauss–Seidel method
<ul>
<li>Symmetric successive overrelaxation (SSOR) — variant of SOR for symmetric matrices</li>
</ul>
</li>
<li>Backfitting algorithm — iterative procedure used to fit a generalized additive model, often equivalent to Gauss–Seidel</li>
</ul>
</li>
<li>Modified Richardson iteration</li>
<li>Conjugate gradient method (CG) — assumes that the matrix is positive definite
<ul>
<li>Derivation of the conjugate gradient method</li>
<li>Nonlinear conjugate gradient method — generalization for nonlinear optimization problems</li>
</ul>
</li>
<li>Biconjugate gradient method (BiCG)
<ul>
<li>Biconjugate gradient stabilized method (BiCGSTAB) — variant of BiCG with better convergence</li>
</ul>
</li>
<li>Conjugate residual method — similar to CG but only assumed that the matrix is symmetric</li>
<li>Generalized minimal residual method (GMRES) — based on the Arnoldi iteration</li>
<li>Chebyshev iteration — avoids inner products but needs bounds on the spectrum</li>
<li>Stone's method (SIP – Srongly Implicit Procedure) — uses an incomplete LU decomposition</li>
<li>Kaczmarz method</li>
<li>Preconditioner
<ul>
<li>Incomplete Cholesky factorization — sparse approximation to the Cholesky factorization</li>
<li>Incomplete LU factorization — sparse approximation to the LU factorization</li>
</ul>
</li>
<li>Uzawa iteration — for saddle node problems</li>
</ul>
</li>
<li>Underdetermined and overdetermined systems (systems that have no or more than one solution):
<ul>
<li>Numerical computation of null space — find all solutions of an underdetermined system</li>
<li>Moore–Penrose pseudoinverse — for finding solution with smallest 2-norm (for underdetermined systems) or smallest residual</li>
<li>Sparse approximation — for finding the sparsest solution (i.e., the solution with as many zeros as possible)</li>
</ul>
</li>
</ul>
<ul>
<li>Row echelon form — matrix in which all entries below a nonzero entry are zero</li>
<li>Bareiss algorithm — variant which ensures that all entries remain integers if the initial matrix has integer entries</li>
<li>Tridiagonal matrix algorithm — simplified form of Gaussian elimination for tridiagonal matrices</li>
</ul>
<ul>
<li>Crout matrix decomposition</li>
<li>LU reduction — a special parallelized version of a LU decomposition algorithm</li>
</ul>
<ul>
<li>Minimum degree algorithm</li>
<li>Symbolic Cholesky decomposition</li>
</ul>
<ul>
<li>Frontal solver — used in finite element methods</li>
<li>Nested dissection — for symmetric matrices, based on graph partitioning</li>
</ul>
<ul>
<li>Jacobi method</li>
<li>Gauss–Seidel method
<ul>
<li>Successive over-relaxation (SOR) — a technique to accelerate the Gauss–Seidel method
<ul>
<li>Symmetric successive overrelaxation (SSOR) — variant of SOR for symmetric matrices</li>
</ul>
</li>
<li>Backfitting algorithm — iterative procedure used to fit a generalized additive model, often equivalent to Gauss–Seidel</li>
</ul>
</li>
<li>Modified Richardson iteration</li>
<li>Conjugate gradient method (CG) — assumes that the matrix is positive definite
<ul>
<li>Derivation of the conjugate gradient method</li>
<li>Nonlinear conjugate gradient method — generalization for nonlinear optimization problems</li>
</ul>
</li>
<li>Biconjugate gradient method (BiCG)
<ul>
<li>Biconjugate gradient stabilized method (BiCGSTAB) — variant of BiCG with better convergence</li>
</ul>
</li>
<li>Conjugate residual method — similar to CG but only assumed that the matrix is symmetric</li>
<li>Generalized minimal residual method (GMRES) — based on the Arnoldi iteration</li>
<li>Chebyshev iteration — avoids inner products but needs bounds on the spectrum</li>
<li>Stone's method (SIP – Srongly Implicit Procedure) — uses an incomplete LU decomposition</li>
<li>Kaczmarz method</li>
<li>Preconditioner
<ul>
<li>Incomplete Cholesky factorization — sparse approximation to the Cholesky factorization</li>
<li>Incomplete LU factorization — sparse approximation to the LU factorization</li>
</ul>
</li>
<li>Uzawa iteration — for saddle node problems</li>
</ul>
<ul>
<li>Successive over-relaxation (SOR) — a technique to accelerate the Gauss–Seidel method
<ul>
<li>Symmetric successive overrelaxation (SSOR) — variant of SOR for symmetric matrices</li>
</ul>
</li>
<li>Backfitting algorithm — iterative procedure used to fit a generalized additive model, often equivalent to Gauss–Seidel</li>
</ul>
<ul>
<li>Symmetric successive overrelaxation (SSOR) — variant of SOR for symmetric matrices</li>
</ul>
<ul>
<li>Derivation of the conjugate gradient method</li>
<li>Nonlinear conjugate gradient method — generalization for nonlinear optimization problems</li>
</ul>
<ul>
<li>Biconjugate gradient stabilized method (BiCGSTAB) — variant of BiCG with better convergence</li>
</ul>
<ul>
<li>Incomplete Cholesky factorization — sparse approximation to the Cholesky factorization</li>
<li>Incomplete LU factorization — sparse approximation to the LU factorization</li>
</ul>
<ul>
<li>Numerical computation of null space — find all solutions of an underdetermined system</li>
<li>Moore–Penrose pseudoinverse — for finding solution with smallest 2-norm (for underdetermined systems) or smallest residual</li>
<li>Sparse approximation — for finding the sparsest solution (i.e., the solution with as many zeros as possible)</li>
</ul>
<h3>Eigenvalue algorithms</h3>
<p>Eigenvalue algorithm — a numerical algorithm for locating the eigenvalues of a matrix</p>
<ul>
<li>Power iteration</li>
<li>Inverse iteration</li>
<li>Rayleigh quotient iteration</li>
<li>Arnoldi iteration — based on Krylov subspaces</li>
<li>Lanczos algorithm — Arnoldi, specialized for positive-definite matrices
<ul>
<li>Block Lanczos algorithm — for when matrix is over a finite field</li>
</ul>
</li>
<li>QR algorithm</li>
<li>Jacobi eigenvalue algorithm — select a small submatrix which can be diagonalized exactly, and repeat
<ul>
<li>Jacobi rotation — the building block, almost a Givens rotation</li>
<li>Jacobi method for complex Hermitian matrices</li>
</ul>
</li>
<li>Divide-and-conquer eigenvalue algorithm</li>
<li>Folded spectrum method</li>
<li>LOBPCG — Locally Optimal Block Preconditioned Conjugate Gradient Method</li>
<li>Eigenvalue perturbation — stability of eigenvalues under perturbations of the matrix</li>
</ul>
<ul>
<li>Block Lanczos algorithm — for when matrix is over a finite field</li>
</ul>
<ul>
<li>Jacobi rotation — the building block, almost a Givens rotation</li>
<li>Jacobi method for complex Hermitian matrices</li>
</ul>
<h3>Other concepts and algorithms</h3>
<ul>
<li>Orthogonalization algorithms:
<ul>
<li>Gram–Schmidt process</li>
<li>Householder transformation
<ul>
<li>Householder operator — analogue of Householder transformation for general inner product spaces</li>
</ul>
</li>
<li>Givens rotation</li>
</ul>
</li>
<li>Krylov subspace</li>
<li>Block matrix pseudoinverse</li>
<li>Bidiagonalization</li>
<li>Cuthill–McKee algorithm — permutes rows/columns in sparse matrix to yield a narrow band matrix</li>
<li>In-place matrix transposition — computing the transpose of a matrix without using much additional storage</li>
<li>Pivot element — entry in a matrix on which the algorithm concentrates</li>
<li>Matrix-free methods — methods that only access the matrix by evaluating matrix-vector products</li>
</ul>
<ul>
<li>Gram–Schmidt process</li>
<li>Householder transformation
<ul>
<li>Householder operator — analogue of Householder transformation for general inner product spaces</li>
</ul>
</li>
<li>Givens rotation</li>
</ul>
<ul>
<li>Householder operator — analogue of Householder transformation for general inner product spaces</li>
</ul>
<h2>Interpolation and approximation</h2>
<p>Interpolation — construct a function going through some given data points</p>
<ul>
<li>Nearest-neighbor interpolation — takes the value of the nearest neighbor</li>
</ul>
<h3>Polynomial interpolation</h3>
<p>Polynomial interpolation — interpolation by polynomials</p>
<ul>
<li>Linear interpolation</li>
<li>Runge's phenomenon</li>
<li>Vandermonde matrix</li>
<li>Chebyshev polynomials</li>
<li>Chebyshev nodes</li>
<li>Lebesgue constant (interpolation)</li>
<li>Different forms for the interpolant:
<ul>
<li>Newton polynomial
<ul>
<li>Divided differences</li>
<li>Neville's algorithm — for evaluating the interpolant; based on the Newton form</li>
</ul>
</li>
<li>Lagrange polynomial</li>
<li>Bernstein polynomial — especially useful for approximation</li>
<li>Brahmagupta's interpolation formula — seventh-century formula for quadratic interpolation</li>
</ul>
</li>
<li>Extensions to multiple dimensions:
<ul>
<li>Bilinear interpolation</li>
<li>Trilinear interpolation</li>
<li>Bicubic interpolation</li>
<li>Tricubic interpolation</li>
<li>Padua points — set of points in <b>R</b> with unique polynomial interpolant and minimal growth of Lebesgue constant</li>
</ul>
</li>
<li>Hermite interpolation</li>
<li>Birkhoff interpolation</li>
<li>Abel–Goncharov interpolation</li>
</ul>
<ul>
<li>Newton polynomial
<ul>
<li>Divided differences</li>
<li>Neville's algorithm — for evaluating the interpolant; based on the Newton form</li>
</ul>
</li>
<li>Lagrange polynomial</li>
<li>Bernstein polynomial — especially useful for approximation</li>
<li>Brahmagupta's interpolation formula — seventh-century formula for quadratic interpolation</li>
</ul>
<ul>
<li>Divided differences</li>
<li>Neville's algorithm — for evaluating the interpolant; based on the Newton form</li>
</ul>
<ul>
<li>Bilinear interpolation</li>
<li>Trilinear interpolation</li>
<li>Bicubic interpolation</li>
<li>Tricubic interpolation</li>
<li>Padua points — set of points in <b>R</b> with unique polynomial interpolant and minimal growth of Lebesgue constant</li>
</ul>
<h3>Spline interpolation</h3>
<p>Spline interpolation — interpolation by piecewise polynomials</p>
<ul>
<li>Spline (mathematics) — the piecewise polynomials used as interpolants</li>
<li>Perfect spline — polynomial spline of degree <i>m</i> whose <i>m</i>th derivate is ±1</li>
<li>Cubic Hermite spline
<ul>
<li>Centripetal Catmull–Rom spline — special case of cubic Hermite splines without self-intersections or cusps</li>
</ul>
</li>
<li>Monotone cubic interpolation</li>
<li>Hermite spline</li>
<li>Bézier curve
<ul>
<li>De Casteljau's algorithm</li>
<li>composite Bézier curve</li>
<li>Generalizations to more dimensions:
<ul>
<li>Bézier triangle — maps a triangle to <b>R</b></li>
<li>Bézier surface — maps a square to <b>R</b></li>
</ul>
</li>
</ul>
</li>
<li>B-spline
<ul>
<li>Box spline — multivariate generalization of B-splines</li>
<li>Truncated power function</li>
<li>De Boor's algorithm — generalizes De Casteljau's algorithm</li>
</ul>
</li>
<li>Non-uniform rational B-spline (NURBS)
<ul>
<li>T-spline — can be thought of as a NURBS surface for which a row of control points is allowed to terminate</li>
</ul>
</li>
<li>Kochanek–Bartels spline</li>
<li>Coons patch — type of manifold parametrization used to smoothly join other surfaces together</li>
<li>M-spline — a non-negative spline</li>
<li>I-spline — a monotone spline, defined in terms of M-splines</li>
<li>Smoothing spline — a spline fitted smoothly to noisy data</li>
<li>Blossom (functional) — a unique, affine, symmetric map associated to a polynomial or spline</li>
<li>See also: List of numerical computational geometry topics</li>
</ul>
<ul>
<li>Centripetal Catmull–Rom spline — special case of cubic Hermite splines without self-intersections or cusps</li>
</ul>
<ul>
<li>De Casteljau's algorithm</li>
<li>composite Bézier curve</li>
<li>Generalizations to more dimensions:
<ul>
<li>Bézier triangle — maps a triangle to <b>R</b></li>
<li>Bézier surface — maps a square to <b>R</b></li>
</ul>
</li>
</ul>
<ul>
<li>Bézier triangle — maps a triangle to <b>R</b></li>
<li>Bézier surface — maps a square to <b>R</b></li>
</ul>
<ul>
<li>Box spline — multivariate generalization of B-splines</li>
<li>Truncated power function</li>
<li>De Boor's algorithm — generalizes De Casteljau's algorithm</li>
</ul>
<ul>
<li>T-spline — can be thought of as a NURBS surface for which a row of control points is allowed to terminate</li>
</ul>
<h3>Trigonometric interpolation</h3>
<p>Trigonometric interpolation — interpolation by trigonometric polynomials</p>
<ul>
<li>Discrete Fourier transform — can be viewed as trigonometric interpolation at equidistant points
<ul>
<li>Relations between Fourier transforms and Fourier series</li>
</ul>
</li>
<li>Fast Fourier transform (FFT) — a fast method for computing the discrete Fourier transform
<ul>
<li>Bluestein's FFT algorithm</li>
<li>Bruun's FFT algorithm</li>
<li>Cooley–Tukey FFT algorithm</li>
<li>Split-radix FFT algorithm — variant of Cooley–Tukey that uses a blend of radices 2 and 4</li>
<li>Goertzel algorithm</li>
<li>Prime-factor FFT algorithm</li>
<li>Rader's FFT algorithm</li>
<li>Bit-reversal permutation — particular permutation of vectors with 2 entries used in many FFTs.</li>
<li>Butterfly diagram</li>
<li>Twiddle factor — the trigonometric constant coefficients that are multiplied by the data</li>
<li>Cyclotomic fast Fourier transform — for FFT over finite fields</li>
<li>Methods for computing discrete convolutions with finite impulse response filters using the FFT:
<ul>
<li>Overlap–add method</li>
<li>Overlap–save method</li>
</ul>
</li>
</ul>
</li>
<li>Sigma approximation</li>
<li>Dirichlet kernel — convolving any function with the Dirichlet kernel yields its trigonometric interpolant</li>
<li>Gibbs phenomenon</li>
</ul>
<ul>
<li>Relations between Fourier transforms and Fourier series</li>
</ul>
<ul>
<li>Bluestein's FFT algorithm</li>
<li>Bruun's FFT algorithm</li>
<li>Cooley–Tukey FFT algorithm</li>
<li>Split-radix FFT algorithm — variant of Cooley–Tukey that uses a blend of radices 2 and 4</li>
<li>Goertzel algorithm</li>
<li>Prime-factor FFT algorithm</li>
<li>Rader's FFT algorithm</li>
<li>Bit-reversal permutation — particular permutation of vectors with 2 entries used in many FFTs.</li>
<li>Butterfly diagram</li>
<li>Twiddle factor — the trigonometric constant coefficients that are multiplied by the data</li>
<li>Cyclotomic fast Fourier transform — for FFT over finite fields</li>
<li>Methods for computing discrete convolutions with finite impulse response filters using the FFT:
<ul>
<li>Overlap–add method</li>
<li>Overlap–save method</li>
</ul>
</li>
</ul>
<ul>
<li>Overlap–add method</li>
<li>Overlap–save method</li>
</ul>
<h3>Other interpolants</h3>
<ul>
<li>Simple rational approximation
<ul>
<li>Polynomial and rational function modeling — comparison of polynomial and rational interpolation</li>
</ul>
</li>
<li>Wavelet
<ul>
<li>Continuous wavelet</li>
<li>Transfer matrix</li>
<li>See also: List of functional analysis topics, List of wavelet-related transforms</li>
</ul>
</li>
<li>Inverse distance weighting</li>
<li>Radial basis function (RBF) — a function of the form ƒ(<i>x</i>) = <i>φ</i>(|<i>x</i>−<i>x</i><sub>0</sub>|)
<ul>
<li>Polyharmonic spline — a commonly used radial basis function</li>
<li>Thin plate spline — a specific polyharmonic spline: <i>r</i> log <i>r</i></li>
<li>Hierarchical RBF</li>
</ul>
</li>
<li>Subdivision surface — constructed by recursively subdividing a piecewise linear interpolant
<ul>
<li>Catmull–Clark subdivision surface</li>
<li>Doo–Sabin subdivision surface</li>
<li>Loop subdivision surface</li>
</ul>
</li>
<li>Slerp (spherical linear interpolation) — interpolation between two points on a sphere
<ul>
<li>Generalized quaternion interpolation — generalizes slerp for interpolation between more than two quaternions</li>
</ul>
</li>
<li>Irrational base discrete weighted transform</li>
<li>Nevanlinna–Pick interpolation — interpolation by analytic functions in the unit disc subject to a bound
<ul>
<li>Pick matrix — the Nevanlinna–Pick interpolation has a solution if this matrix is positive semi-definite</li>
</ul>
</li>
<li>Multivariate interpolation — the function being interpolated depends on more than one variable
<ul>
<li>Barnes interpolation — method for two-dimensional functions using Gaussians common in meteorology</li>
<li>Coons surface — combination of linear interpolation and bilinear interpolation</li>
<li>Lanczos resampling — based on convolution with a sinc function</li>
<li>Natural neighbor interpolation</li>
<li>Nearest neighbor value interpolation</li>
<li>PDE surface</li>
<li>Transfinite interpolation — constructs function on planar domain given its values on the boundary</li>
<li>Trend surface analysis — based on low-order polynomials of spatial coordinates; uses scattered observations</li>
<li>Method based on polynomials are listed under <i>Polynomial interpolation</i></li>
</ul>
</li>
</ul>
<ul>
<li>Polynomial and rational function modeling — comparison of polynomial and rational interpolation</li>
</ul>
<ul>
<li>Continuous wavelet</li>
<li>Transfer matrix</li>
<li>See also: List of functional analysis topics, List of wavelet-related transforms</li>
</ul>
<ul>
<li>Polyharmonic spline — a commonly used radial basis function</li>
<li>Thin plate spline — a specific polyharmonic spline: <i>r</i> log <i>r</i></li>
<li>Hierarchical RBF</li>
</ul>
<ul>
<li>Catmull–Clark subdivision surface</li>
<li>Doo–Sabin subdivision surface</li>
<li>Loop subdivision surface</li>
</ul>
<ul>
<li>Generalized quaternion interpolation — generalizes slerp for interpolation between more than two quaternions</li>
</ul>
<ul>
<li>Pick matrix — the Nevanlinna–Pick interpolation has a solution if this matrix is positive semi-definite</li>
</ul>
<ul>
<li>Barnes interpolation — method for two-dimensional functions using Gaussians common in meteorology</li>
<li>Coons surface — combination of linear interpolation and bilinear interpolation</li>
<li>Lanczos resampling — based on convolution with a sinc function</li>
<li>Natural neighbor interpolation</li>
<li>Nearest neighbor value interpolation</li>
<li>PDE surface</li>
<li>Transfinite interpolation — constructs function on planar domain given its values on the boundary</li>
<li>Trend surface analysis — based on low-order polynomials of spatial coordinates; uses scattered observations</li>
<li>Method based on polynomials are listed under <i>Polynomial interpolation</i></li>
</ul>
<h3>Approximation theory</h3>
<p>Approximation theory</p>
<ul>
<li>Orders of approximation</li>
<li>Lebesgue's lemma</li>
<li>Curve fitting
<ul>
<li>Vector field reconstruction</li>
</ul>
</li>
<li>Modulus of continuity — measures smoothness of a function</li>
<li>Least squares (function approximation) — minimizes the error in the L-norm</li>
<li>Minimax approximation algorithm — minimizes the maximum error over an interval (the L-norm)
<ul>
<li>Equioscillation theorem — characterizes the best approximation in the L-norm</li>
</ul>
</li>
<li>Unisolvent point set — function from given function space is determined uniquely by values on such a set of points</li>
<li>Stone–Weierstrass theorem — continuous functions can be approximated uniformly by polynomials, or certain other function spaces</li>
<li>Approximation by polynomials:
<ul>
<li>Linear approximation</li>
<li>Bernstein polynomial — basis of polynomials useful for approximating a function</li>
<li>Bernstein's constant — error when approximating |<i>x</i>| by a polynomial</li>
<li>Remez algorithm — for constructing the best polynomial approximation in the L-norm</li>
<li>Bernstein's inequality (mathematical analysis) — bound on maximum of derivative of polynomial in unit disk</li>
<li>Mergelyan's theorem — generalization of Stone–Weierstrass theorem for polynomials</li>
<li>Müntz–Szász theorem — variant of Stone–Weierstrass theorem for polynomials if some coefficients have to be zero</li>
<li>Bramble–Hilbert lemma — upper bound on L error of polynomial approximation in multiple dimensions</li>
<li>Discrete Chebyshev polynomials — polynomials orthogonal with respect to a discrete measure</li>
<li>Favard's theorem — polynomials satisfying suitable 3-term recurrence relations are orthogonal polynomials</li>
</ul>
</li>
<li>Approximation by Fourier series / trigonometric polynomials:
<ul>
<li>Jackson's inequality — upper bound for best approximation by a trigonometric polynomial
<ul>
<li>Bernstein's theorem (approximation theory) — a converse to Jackson's inequality</li>
</ul>
</li>
<li>Fejér's theorem — Cesàro means of partial sums of Fourier series converge uniformly for continuous periodic functions</li>
<li>Erdős–Turán inequality — bounds distance between probability and Lebesgue measure in terms of Fourier coefficients</li>
</ul>
</li>
<li>Different approximations:
<ul>
<li>Moving least squares</li>
<li>Padé approximant
<ul>
<li>Padé table — table of Padé approximants</li>
</ul>
</li>
<li>Hartogs–Rosenthal theorem — continuous functions can be approximated uniformly by rational functions on a set of Lebesgue measure zero</li>
<li>Szász–Mirakyan operator — approximation by e <i>x</i> on a semi-infinite interval</li>
<li>Szász–Mirakjan–Kantorovich operator</li>
<li>Baskakov operator — generalize Bernstein polynomials, Szász–Mirakyan operators, and Lupas operators</li>
<li>Favard operator — approximation by sums of Gaussians</li>
</ul>
</li>
<li>Surrogate model — application: replacing a function that is hard to evaluate by a simpler function</li>
<li>Constructive function theory — field that studies connection between degree of approximation and smoothness</li>
<li>Universal differential equation — differential–algebraic equation whose solutions can approximate any continuous function</li>
<li>Fekete problem — find <i>N</i> points on a sphere that minimize some kind of energy</li>
<li>Carleman's condition — condition guaranteeing that a measure is uniquely determined by its moments</li>
<li>Krein's condition — condition that exponential sums are dense in weighted L space</li>
<li>Lethargy theorem — about distance of points in a metric space from members of a sequence of subspaces</li>
<li>Wirtinger's representation and projection theorem</li>
<li>Journals:
<ul>
<li>Constructive Approximation</li>
<li>Journal of Approximation Theory</li>
</ul>
</li>
</ul>
<ul>
<li>Vector field reconstruction</li>
</ul>
<ul>
<li>Equioscillation theorem — characterizes the best approximation in the L-norm</li>
</ul>
<ul>
<li>Linear approximation</li>
<li>Bernstein polynomial — basis of polynomials useful for approximating a function</li>
<li>Bernstein's constant — error when approximating |<i>x</i>| by a polynomial</li>
<li>Remez algorithm — for constructing the best polynomial approximation in the L-norm</li>
<li>Bernstein's inequality (mathematical analysis) — bound on maximum of derivative of polynomial in unit disk</li>
<li>Mergelyan's theorem — generalization of Stone–Weierstrass theorem for polynomials</li>
<li>Müntz–Szász theorem — variant of Stone–Weierstrass theorem for polynomials if some coefficients have to be zero</li>
<li>Bramble–Hilbert lemma — upper bound on L error of polynomial approximation in multiple dimensions</li>
<li>Discrete Chebyshev polynomials — polynomials orthogonal with respect to a discrete measure</li>
<li>Favard's theorem — polynomials satisfying suitable 3-term recurrence relations are orthogonal polynomials</li>
</ul>
<ul>
<li>Jackson's inequality — upper bound for best approximation by a trigonometric polynomial
<ul>
<li>Bernstein's theorem (approximation theory) — a converse to Jackson's inequality</li>
</ul>
</li>
<li>Fejér's theorem — Cesàro means of partial sums of Fourier series converge uniformly for continuous periodic functions</li>
<li>Erdős–Turán inequality — bounds distance between probability and Lebesgue measure in terms of Fourier coefficients</li>
</ul>
<ul>
<li>Bernstein's theorem (approximation theory) — a converse to Jackson's inequality</li>
</ul>
<ul>
<li>Moving least squares</li>
<li>Padé approximant
<ul>
<li>Padé table — table of Padé approximants</li>
</ul>
</li>
<li>Hartogs–Rosenthal theorem — continuous functions can be approximated uniformly by rational functions on a set of Lebesgue measure zero</li>
<li>Szász–Mirakyan operator — approximation by e <i>x</i> on a semi-infinite interval</li>
<li>Szász–Mirakjan–Kantorovich operator</li>
<li>Baskakov operator — generalize Bernstein polynomials, Szász–Mirakyan operators, and Lupas operators</li>
<li>Favard operator — approximation by sums of Gaussians</li>
</ul>
<ul>
<li>Padé table — table of Padé approximants</li>
</ul>
<ul>
<li>Constructive Approximation</li>
<li>Journal of Approximation Theory</li>
</ul>
<h3>Miscellaneous</h3>
<ul>
<li>Extrapolation
<ul>
<li>Linear predictive analysis — linear extrapolation</li>
</ul>
</li>
<li>Unisolvent functions — functions for which the interpolation problem has a unique solution</li>
<li>Regression analysis
<ul>
<li>Isotonic regression</li>
</ul>
</li>
<li>Curve-fitting compaction</li>
<li>Interpolation (computer graphics)</li>
</ul>
<ul>
<li>Linear predictive analysis — linear extrapolation</li>
</ul>
<ul>
<li>Isotonic regression</li>
</ul>
<h2>Finding roots of nonlinear equations</h2>
<p>Root-finding algorithm — algorithms for solving the equation <i>f</i>(<i>x</i>) = 0</p>
<ul>
<li>General methods:
<ul>
<li>Bisection method — simple and robust; linear convergence
<ul>
<li>Lehmer–Schur algorithm — variant for complex functions</li>
</ul>
</li>
<li>Fixed-point iteration</li>
<li>Newton's method — based on linear approximation around the current iterate; quadratic convergence
<ul>
<li>Kantorovich theorem — gives a region around solution such that Newton's method converges</li>
<li>Newton fractal — indicates which initial condition converges to which root under Newton iteration</li>
<li>Quasi-Newton method — uses an approximation of the Jacobian:
<ul>
<li>Broyden's method — uses a rank-one update for the Jacobian</li>
<li>Symmetric rank-one — a symmetric (but not necessarily positive definite) rank-one update of the Jacobian</li>
<li>Davidon–Fletcher–Powell formula — update of the Jacobian in which the matrix remains positive definite</li>
<li>Broyden–Fletcher–Goldfarb–Shanno algorithm — rank-two update of the Jacobian in which the matrix remains positive definite</li>
<li>Limited-memory BFGS method — truncated, matrix-free variant of BFGS method suitable for large problems</li>
</ul>
</li>
<li>Steffensen's method — uses divided differences instead of the derivative</li>
</ul>
</li>
<li>Secant method — based on linear interpolation at last two iterates</li>
<li>False position method — secant method with ideas from the bisection method</li>
<li>Muller's method — based on quadratic interpolation at last three iterates</li>
<li>Sidi's generalized secant method — higher-order variants of secant method</li>
<li>Inverse quadratic interpolation — similar to Muller's method, but interpolates the inverse</li>
<li>Brent's method — combines bisection method, secant method and inverse quadratic interpolation</li>
<li>Ridders' method — fits a linear function times an exponential to last two iterates and their midpoint</li>
<li>Halley's method — uses <i>f</i>, <i>f</i>' and <i>f</i>''; achieves the cubic convergence</li>
<li>Householder's method — uses first <i>d</i> derivatives to achieve order <i>d</i> + 1; generalizes Newton's and Halley's method</li>
</ul>
</li>
<li>Methods for polynomials:
<ul>
<li>Aberth method</li>
<li>Bairstow's method</li>
<li>Durand–Kerner method</li>
<li>Graeffe's method</li>
<li>Jenkins–Traub algorithm — fast, reliable, and widely used</li>
<li>Laguerre's method</li>
<li>Splitting circle method</li>
</ul>
</li>
<li>Analysis:
<ul>
<li>Wilkinson's polynomial</li>
</ul>
</li>
<li>Numerical continuation — tracking a root as one parameters in the equation changes
<ul>
<li>Piecewise linear continuation</li>
</ul>
</li>
</ul>
<ul>
<li>Bisection method — simple and robust; linear convergence
<ul>
<li>Lehmer–Schur algorithm — variant for complex functions</li>
</ul>
</li>
<li>Fixed-point iteration</li>
<li>Newton's method — based on linear approximation around the current iterate; quadratic convergence
<ul>
<li>Kantorovich theorem — gives a region around solution such that Newton's method converges</li>
<li>Newton fractal — indicates which initial condition converges to which root under Newton iteration</li>
<li>Quasi-Newton method — uses an approximation of the Jacobian:
<ul>
<li>Broyden's method — uses a rank-one update for the Jacobian</li>
<li>Symmetric rank-one — a symmetric (but not necessarily positive definite) rank-one update of the Jacobian</li>
<li>Davidon–Fletcher–Powell formula — update of the Jacobian in which the matrix remains positive definite</li>
<li>Broyden–Fletcher–Goldfarb–Shanno algorithm — rank-two update of the Jacobian in which the matrix remains positive definite</li>
<li>Limited-memory BFGS method — truncated, matrix-free variant of BFGS method suitable for large problems</li>
</ul>
</li>
<li>Steffensen's method — uses divided differences instead of the derivative</li>
</ul>
</li>
<li>Secant method — based on linear interpolation at last two iterates</li>
<li>False position method — secant method with ideas from the bisection method</li>
<li>Muller's method — based on quadratic interpolation at last three iterates</li>
<li>Sidi's generalized secant method — higher-order variants of secant method</li>
<li>Inverse quadratic interpolation — similar to Muller's method, but interpolates the inverse</li>
<li>Brent's method — combines bisection method, secant method and inverse quadratic interpolation</li>
<li>Ridders' method — fits a linear function times an exponential to last two iterates and their midpoint</li>
<li>Halley's method — uses <i>f</i>, <i>f</i>' and <i>f</i>''; achieves the cubic convergence</li>
<li>Householder's method — uses first <i>d</i> derivatives to achieve order <i>d</i> + 1; generalizes Newton's and Halley's method</li>
</ul>
<ul>
<li>Lehmer–Schur algorithm — variant for complex functions</li>
</ul>
<ul>
<li>Kantorovich theorem — gives a region around solution such that Newton's method converges</li>
<li>Newton fractal — indicates which initial condition converges to which root under Newton iteration</li>
<li>Quasi-Newton method — uses an approximation of the Jacobian:
<ul>
<li>Broyden's method — uses a rank-one update for the Jacobian</li>
<li>Symmetric rank-one — a symmetric (but not necessarily positive definite) rank-one update of the Jacobian</li>
<li>Davidon–Fletcher–Powell formula — update of the Jacobian in which the matrix remains positive definite</li>
<li>Broyden–Fletcher–Goldfarb–Shanno algorithm — rank-two update of the Jacobian in which the matrix remains positive definite</li>
<li>Limited-memory BFGS method — truncated, matrix-free variant of BFGS method suitable for large problems</li>
</ul>
</li>
<li>Steffensen's method — uses divided differences instead of the derivative</li>
</ul>
<ul>
<li>Broyden's method — uses a rank-one update for the Jacobian</li>
<li>Symmetric rank-one — a symmetric (but not necessarily positive definite) rank-one update of the Jacobian</li>
<li>Davidon–Fletcher–Powell formula — update of the Jacobian in which the matrix remains positive definite</li>
<li>Broyden–Fletcher–Goldfarb–Shanno algorithm — rank-two update of the Jacobian in which the matrix remains positive definite</li>
<li>Limited-memory BFGS method — truncated, matrix-free variant of BFGS method suitable for large problems</li>
</ul>
<ul>
<li>Aberth method</li>
<li>Bairstow's method</li>
<li>Durand–Kerner method</li>
<li>Graeffe's method</li>
<li>Jenkins–Traub algorithm — fast, reliable, and widely used</li>
<li>Laguerre's method</li>
<li>Splitting circle method</li>
</ul>
<ul>
<li>Wilkinson's polynomial</li>
</ul>
<ul>
<li>Piecewise linear continuation</li>
</ul>
<h2>Optimization</h2>
<p>Mathematical optimization — algorithm for finding maxima or minima of a given function</p>
<h3>Basic concepts</h3>
<ul>
<li>Active set</li>
<li>Candidate solution</li>
<li>Constraint (mathematics)
<ul>
<li>Constrained optimization — studies optimization problems with constraints</li>
<li>Binary constraint — a constraint that involves exactly two variables</li>
</ul>
</li>
<li>Corner solution</li>
<li>Feasible region — contains all solutions that satisfy the constraints but may not be optimal</li>
<li>Global optimum and Local optimum</li>
<li>Maxima and minima</li>
<li>Slack variable</li>
<li>Continuous optimization</li>
<li>Discrete optimization</li>
</ul>
<ul>
<li>Constrained optimization — studies optimization problems with constraints</li>
<li>Binary constraint — a constraint that involves exactly two variables</li>
</ul>
<h3>Linear programming</h3>
<p>Linear programming (also treats <i>integer programming</i>) — objective function and constraints are linear</p>
<ul>
<li>Algorithms for linear programming:
<ul>
<li>Simplex algorithm
<ul>
<li>Bland's rule — rule to avoid cycling in the simplex method</li>
<li>Klee–Minty cube — perturbed (hyper)cube; simplex method has exponential complexity on such a domain</li>
<li>Criss-cross algorithm — similar to the simplex algorithm</li>
<li>Big M method — variation of simplex algorithm for problems with both "less than" and "greater than" constraints</li>
</ul>
</li>
<li>Interior point method
<ul>
<li>Ellipsoid method</li>
<li>Karmarkar's algorithm</li>
<li>Mehrotra predictor–corrector method</li>
</ul>
</li>
<li>Column generation</li>
<li>k-approximation of k-hitting set — algorithm for specific LP problems (to find a weighted hitting set)</li>
</ul>
</li>
<li>Linear complementarity problem</li>
<li>Decompositions:
<ul>
<li>Benders' decomposition</li>
<li>Dantzig–Wolfe decomposition</li>
<li>Theory of two-level planning</li>
<li>Variable splitting</li>
</ul>
</li>
<li>Basic solution (linear programming) — solution at vertex of feasible region</li>
<li>Fourier–Motzkin elimination</li>
<li>Hilbert basis (linear programming) — set of integer vectors in a convex cone which generate all integer vectors in the cone</li>
<li>LP-type problem</li>
<li>Linear inequality</li>
<li>Vertex enumeration problem — list all vertices of the feasible set</li>
</ul>
<ul>
<li>Simplex algorithm
<ul>
<li>Bland's rule — rule to avoid cycling in the simplex method</li>
<li>Klee–Minty cube — perturbed (hyper)cube; simplex method has exponential complexity on such a domain</li>
<li>Criss-cross algorithm — similar to the simplex algorithm</li>
<li>Big M method — variation of simplex algorithm for problems with both "less than" and "greater than" constraints</li>
</ul>
</li>
<li>Interior point method
<ul>
<li>Ellipsoid method</li>
<li>Karmarkar's algorithm</li>
<li>Mehrotra predictor–corrector method</li>
</ul>
</li>
<li>Column generation</li>
<li>k-approximation of k-hitting set — algorithm for specific LP problems (to find a weighted hitting set)</li>
</ul>
<ul>
<li>Bland's rule — rule to avoid cycling in the simplex method</li>
<li>Klee–Minty cube — perturbed (hyper)cube; simplex method has exponential complexity on such a domain</li>
<li>Criss-cross algorithm — similar to the simplex algorithm</li>
<li>Big M method — variation of simplex algorithm for problems with both "less than" and "greater than" constraints</li>
</ul>
<ul>
<li>Ellipsoid method</li>
<li>Karmarkar's algorithm</li>
<li>Mehrotra predictor–corrector method</li>
</ul>
<ul>
<li>Benders' decomposition</li>
<li>Dantzig–Wolfe decomposition</li>
<li>Theory of two-level planning</li>
<li>Variable splitting</li>
</ul>
<h3>Convex optimization</h3>
<p>Convex optimization</p>
<ul>
<li>Quadratic programming
<ul>
<li>Linear least squares (mathematics)</li>
<li>Total least squares</li>
<li>Frank–Wolfe algorithm</li>
<li>Sequential minimal optimization — breaks up large QP problems into a series of smallest possible QP problems</li>
<li>Bilinear program</li>
</ul>
</li>
<li>Basis pursuit — minimize L<sub>1</sub>-norm of vector subject to linear constraints
<ul>
<li>Basis pursuit denoising (BPDN) — regularized version of basis pursuit
<ul>
<li>In-crowd algorithm — algorithm for solving basis pursuit denoising</li>
</ul>
</li>
</ul>
</li>
<li>Linear matrix inequality</li>
<li>Conic optimization
<ul>
<li>Semidefinite programming</li>
<li>Second-order cone programming</li>
<li>Sum-of-squares optimization</li>
<li>Quadratic programming (see above)</li>
</ul>
</li>
<li>Bregman method — row-action method for strictly convex optimization problems</li>
<li>Proximal Gradient Methods — use splitting of objective function in sum of possible non-differentiable pieces</li>
<li>Subgradient method — extension of steepest descent for problems with a non-differentiable objective function</li>
<li>Biconvex optimization — generalization where objective function and constraint set can be biconvex</li>
</ul>
<ul>
<li>Linear least squares (mathematics)</li>
<li>Total least squares</li>
<li>Frank–Wolfe algorithm</li>
<li>Sequential minimal optimization — breaks up large QP problems into a series of smallest possible QP problems</li>
<li>Bilinear program</li>
</ul>
<ul>
<li>Basis pursuit denoising (BPDN) — regularized version of basis pursuit
<ul>
<li>In-crowd algorithm — algorithm for solving basis pursuit denoising</li>
</ul>
</li>
</ul>
<ul>
<li>In-crowd algorithm — algorithm for solving basis pursuit denoising</li>
</ul>
<ul>
<li>Semidefinite programming</li>
<li>Second-order cone programming</li>
<li>Sum-of-squares optimization</li>
<li>Quadratic programming (see above)</li>
</ul>
<h3>Nonlinear programming</h3>
<p>Nonlinear programming — the most general optimization problem in the usual framework</p>
<ul>
<li>Special cases of nonlinear programming:
<ul>
<li>See <i>Linear programming</i> and <i>Convex optimization</i> above</li>
<li>Geometric programming — problems involving signomials or posynomials
<ul>
<li>Signomial — similar to polynomials, but exponents need not be integers</li>
<li>Posynomial — a signomial with positive coefficients</li>
</ul>
</li>
<li>Quadratically constrained quadratic program</li>
<li>Linear-fractional programming — objective is ratio of linear functions, constraints are linear
<ul>
<li>Fractional programming — objective is ratio of nonlinear functions, constraints are linear</li>
</ul>
</li>
<li>Nonlinear complementarity problem (NCP) — find <i>x</i> such that <i>x</i> ≥ 0, <i>f</i>(<i>x</i>) ≥ 0 and <i>x</i> <i>f</i>(<i>x</i>) = 0</li>
<li>Least squares — the objective function is a sum of squares
<ul>
<li>Non-linear least squares</li>
<li>Gauss–Newton algorithm
<ul>
<li>BHHH algorithm — variant of Gauss–Newton in econometrics</li>
<li>Generalized Gauss–Newton method — for constrained nonlinear least-squares problems</li>
</ul>
</li>
<li>Levenberg–Marquardt algorithm</li>
<li>Iteratively reweighted least squares (IRLS) — solves a weigted least-squares problem at every iteration</li>
<li>Partial least squares — statistical techniques similar to principal components analysis
<ul>
<li>Non-linear iterative partial least squares (NIPLS)</li>
</ul>
</li>
</ul>
</li>
<li>Mathematical programming with equilibrium constraints — constraints include variational inequalities or complementarities</li>
<li>Univariate optimization:
<ul>
<li>Golden section search</li>
<li>Successive parabolic interpolation — based on quadratic interpolation through the last three iterates</li>
</ul>
</li>
</ul>
</li>
<li>General algorithms:
<ul>
<li>Concepts:
<ul>
<li>Descent direction</li>
<li>Guess value — the initial guess for a solution with which an algorithm starts</li>
<li>Line search
<ul>
<li>Backtracking line search</li>
<li>Wolfe conditions</li>
</ul>
</li>
</ul>
</li>
<li>Gradient method — method that uses the gradient as the search direction
<ul>
<li>Gradient descent
<ul>
<li>Stochastic gradient descent</li>
</ul>
</li>
<li>Landweber iteration — mainly used for ill-posed problems</li>
</ul>
</li>
<li>Successive linear programming (SLP) — replace problem by a linear programming problem, solve that, and repeat</li>
<li>Sequential quadratic programming (SQP) — replace problem by a quadratic programming problem, solve that, and repeat</li>
<li>Newton's method in optimization
<ul>
<li>See also under <i>Newton algorithm</i> in the section <i>Finding roots of nonlinear equations</i></li>
</ul>
</li>
<li>Nonlinear conjugate gradient method</li>
<li>Derivative-free methods
<ul>
<li>Coordinate descent — move in one of the coordinate directions
<ul>
<li>Adaptive coordinate descent — adapt coordinate directions to objective function</li>
<li>Random coordinate descent — randomized version</li>
</ul>
</li>
<li>Nelder–Mead method</li>
<li>Pattern search (optimization)</li>
<li>Powell's method — based on conjugate gradient descent</li>
<li>Rosenbrock methods — derivative-free method, similar to Nelder–Mead but with guaranteed convergence</li>
</ul>
</li>
<li>Augmented Lagrangian method — replaces contrained problems by unconstrained problems with a term added to the objective function</li>
<li>Ternary search</li>
<li>Tabu search</li>
<li>Guided Local Search — modification of search algorithms which builds up penalties during a search</li>
<li>Reactive search optimization (RSO) — the algorithm adapts its parameters automatically</li>
<li>MM algorithm — majorize-minimization, a wide framework of methods</li>
<li>Least absolute deviations
<ul>
<li>Expectation–maximization algorithm
<ul>
<li>Ordered subset expectation maximization</li>
</ul>
</li>
</ul>
</li>
<li>Adaptive projected subgradient method</li>
<li>Nearest neighbor search</li>
<li>Space mapping — uses "coarse" (ideal or low-fidelity) and "fine" (practical or high-fidelity) models</li>
</ul>
</li>
</ul>
<ul>
<li>See <i>Linear programming</i> and <i>Convex optimization</i> above</li>
<li>Geometric programming — problems involving signomials or posynomials
<ul>
<li>Signomial — similar to polynomials, but exponents need not be integers</li>
<li>Posynomial — a signomial with positive coefficients</li>
</ul>
</li>
<li>Quadratically constrained quadratic program</li>
<li>Linear-fractional programming — objective is ratio of linear functions, constraints are linear
<ul>
<li>Fractional programming — objective is ratio of nonlinear functions, constraints are linear</li>
</ul>
</li>
<li>Nonlinear complementarity problem (NCP) — find <i>x</i> such that <i>x</i> ≥ 0, <i>f</i>(<i>x</i>) ≥ 0 and <i>x</i> <i>f</i>(<i>x</i>) = 0</li>
<li>Least squares — the objective function is a sum of squares
<ul>
<li>Non-linear least squares</li>
<li>Gauss–Newton algorithm
<ul>
<li>BHHH algorithm — variant of Gauss–Newton in econometrics</li>
<li>Generalized Gauss–Newton method — for constrained nonlinear least-squares problems</li>
</ul>
</li>
<li>Levenberg–Marquardt algorithm</li>
<li>Iteratively reweighted least squares (IRLS) — solves a weigted least-squares problem at every iteration</li>
<li>Partial least squares — statistical techniques similar to principal components analysis
<ul>
<li>Non-linear iterative partial least squares (NIPLS)</li>
</ul>
</li>
</ul>
</li>
<li>Mathematical programming with equilibrium constraints — constraints include variational inequalities or complementarities</li>
<li>Univariate optimization:
<ul>
<li>Golden section search</li>
<li>Successive parabolic interpolation — based on quadratic interpolation through the last three iterates</li>
</ul>
</li>
</ul>
<ul>
<li>Signomial — similar to polynomials, but exponents need not be integers</li>
<li>Posynomial — a signomial with positive coefficients</li>
</ul>
<ul>
<li>Fractional programming — objective is ratio of nonlinear functions, constraints are linear</li>
</ul>
<ul>
<li>Non-linear least squares</li>
<li>Gauss–Newton algorithm
<ul>
<li>BHHH algorithm — variant of Gauss–Newton in econometrics</li>
<li>Generalized Gauss–Newton method — for constrained nonlinear least-squares problems</li>
</ul>
</li>
<li>Levenberg–Marquardt algorithm</li>
<li>Iteratively reweighted least squares (IRLS) — solves a weigted least-squares problem at every iteration</li>
<li>Partial least squares — statistical techniques similar to principal components analysis
<ul>
<li>Non-linear iterative partial least squares (NIPLS)</li>
</ul>
</li>
</ul>
<ul>
<li>BHHH algorithm — variant of Gauss–Newton in econometrics</li>
<li>Generalized Gauss–Newton method — for constrained nonlinear least-squares problems</li>
</ul>
<ul>
<li>Non-linear iterative partial least squares (NIPLS)</li>
</ul>
<ul>
<li>Golden section search</li>
<li>Successive parabolic interpolation — based on quadratic interpolation through the last three iterates</li>
</ul>
<ul>
<li>Concepts:
<ul>
<li>Descent direction</li>
<li>Guess value — the initial guess for a solution with which an algorithm starts</li>
<li>Line search
<ul>
<li>Backtracking line search</li>
<li>Wolfe conditions</li>
</ul>
</li>
</ul>
</li>
<li>Gradient method — method that uses the gradient as the search direction
<ul>
<li>Gradient descent
<ul>
<li>Stochastic gradient descent</li>
</ul>
</li>
<li>Landweber iteration — mainly used for ill-posed problems</li>
</ul>
</li>
<li>Successive linear programming (SLP) — replace problem by a linear programming problem, solve that, and repeat</li>
<li>Sequential quadratic programming (SQP) — replace problem by a quadratic programming problem, solve that, and repeat</li>
<li>Newton's method in optimization
<ul>
<li>See also under <i>Newton algorithm</i> in the section <i>Finding roots of nonlinear equations</i></li>
</ul>
</li>
<li>Nonlinear conjugate gradient method</li>
<li>Derivative-free methods
<ul>
<li>Coordinate descent — move in one of the coordinate directions
<ul>
<li>Adaptive coordinate descent — adapt coordinate directions to objective function</li>
<li>Random coordinate descent — randomized version</li>
</ul>
</li>
<li>Nelder–Mead method</li>
<li>Pattern search (optimization)</li>
<li>Powell's method — based on conjugate gradient descent</li>
<li>Rosenbrock methods — derivative-free method, similar to Nelder–Mead but with guaranteed convergence</li>
</ul>
</li>
<li>Augmented Lagrangian method — replaces contrained problems by unconstrained problems with a term added to the objective function</li>
<li>Ternary search</li>
<li>Tabu search</li>
<li>Guided Local Search — modification of search algorithms which builds up penalties during a search</li>
<li>Reactive search optimization (RSO) — the algorithm adapts its parameters automatically</li>
<li>MM algorithm — majorize-minimization, a wide framework of methods</li>
<li>Least absolute deviations
<ul>
<li>Expectation–maximization algorithm
<ul>
<li>Ordered subset expectation maximization</li>
</ul>
</li>
</ul>
</li>
<li>Adaptive projected subgradient method</li>
<li>Nearest neighbor search</li>
<li>Space mapping — uses "coarse" (ideal or low-fidelity) and "fine" (practical or high-fidelity) models</li>
</ul>
<ul>
<li>Descent direction</li>
<li>Guess value — the initial guess for a solution with which an algorithm starts</li>
<li>Line search
<ul>
<li>Backtracking line search</li>
<li>Wolfe conditions</li>
</ul>
</li>
</ul>
<ul>
<li>Backtracking line search</li>
<li>Wolfe conditions</li>
</ul>
<ul>
<li>Gradient descent
<ul>
<li>Stochastic gradient descent</li>
</ul>
</li>
<li>Landweber iteration — mainly used for ill-posed problems</li>
</ul>
<ul>
<li>Stochastic gradient descent</li>
</ul>
<ul>
<li>See also under <i>Newton algorithm</i> in the section <i>Finding roots of nonlinear equations</i></li>
</ul>
<ul>
<li>Coordinate descent — move in one of the coordinate directions
<ul>
<li>Adaptive coordinate descent — adapt coordinate directions to objective function</li>
<li>Random coordinate descent — randomized version</li>
</ul>
</li>
<li>Nelder–Mead method</li>
<li>Pattern search (optimization)</li>
<li>Powell's method — based on conjugate gradient descent</li>
<li>Rosenbrock methods — derivative-free method, similar to Nelder–Mead but with guaranteed convergence</li>
</ul>
<ul>
<li>Adaptive coordinate descent — adapt coordinate directions to objective function</li>
<li>Random coordinate descent — randomized version</li>
</ul>
<ul>
<li>Expectation–maximization algorithm
<ul>
<li>Ordered subset expectation maximization</li>
</ul>
</li>
</ul>
<ul>
<li>Ordered subset expectation maximization</li>
</ul>
<h3>Optimal control and infinite-dimensional optimization</h3>
<p>Optimal control</p>
<ul>
<li>Pontryagin's minimum principle — infinite-dimensional version of Lagrange multipliers
<ul>
<li>Costate equations — equation for the "Lagrange multipliers" in Pontryagin's minimum principle</li>
<li>Hamiltonian (control theory) — minimum principle says that this function should be minimized</li>
</ul>
</li>
<li>Types of problems:
<ul>
<li>Linear-quadratic regulator — system dynamics is a linear differential equation, objective is quadratic</li>
<li>Linear-quadratic-Gaussian control (LQG) — system dynamics is a linear SDE with additive noise, objective is quadratic
<ul>
<li>Optimal projection equations — method for reducing dimension of LQG control problem</li>
</ul>
</li>
</ul>
</li>
<li>Algebraic Riccati equation — matrix equation occurring in many optimal control problems</li>
<li>Bang–bang control — control that switches abruptly between two states</li>
<li>Covector mapping principle</li>
<li>Differential dynamic programming — uses locally-quadratic models of the dynamics and cost functions</li>
<li>DNSS point — initial state for certain optimal control problems with multiple optimal solutions</li>
<li>Legendre–Clebsch condition — second-order condition for solution of optimal control problem</li>
<li>Pseudospectral optimal control
<ul>
<li>Bellman pseudospectral method — based on Bellman's principle of optimality</li>
<li>Chebyshev pseudospectral method — uses Chebyshev polynomials (of the first kind)</li>
<li>Flat pseudospectral method — combines Ross–Fahroo pseudospectral method with differential flatness</li>
<li>Gauss pseudospectral method — uses collocation at the Legendre–Gauss points</li>
<li>Legendre pseudospectral method — uses Legendre polynomials</li>
<li>Pseudospectral knotting method — generalization of pseudospectral methods in optimal control</li>
<li>Ross–Fahroo pseudospectral method — class of pseudospectral method including Chebyshev, Legendre and knotting</li>
</ul>
</li>
<li>Ross–Fahroo lemma — condition to make discretization and duality operations commute</li>
<li>Ross' π lemma — there is fundamental time constant within which a control solution must be computed for controllability and stability</li>
<li>Sethi model — optimal control problem modelling advertising</li>
</ul>
<ul>
<li>Costate equations — equation for the "Lagrange multipliers" in Pontryagin's minimum principle</li>
<li>Hamiltonian (control theory) — minimum principle says that this function should be minimized</li>
</ul>
<ul>
<li>Linear-quadratic regulator — system dynamics is a linear differential equation, objective is quadratic</li>
<li>Linear-quadratic-Gaussian control (LQG) — system dynamics is a linear SDE with additive noise, objective is quadratic
<ul>
<li>Optimal projection equations — method for reducing dimension of LQG control problem</li>
</ul>
</li>
</ul>
<ul>
<li>Optimal projection equations — method for reducing dimension of LQG control problem</li>
</ul>
<ul>
<li>Bellman pseudospectral method — based on Bellman's principle of optimality</li>
<li>Chebyshev pseudospectral method — uses Chebyshev polynomials (of the first kind)</li>
<li>Flat pseudospectral method — combines Ross–Fahroo pseudospectral method with differential flatness</li>
<li>Gauss pseudospectral method — uses collocation at the Legendre–Gauss points</li>
<li>Legendre pseudospectral method — uses Legendre polynomials</li>
<li>Pseudospectral knotting method — generalization of pseudospectral methods in optimal control</li>
<li>Ross–Fahroo pseudospectral method — class of pseudospectral method including Chebyshev, Legendre and knotting</li>
</ul>
<p>Infinite-dimensional optimization</p>
<ul>
<li>Semi-infinite programming — infinite number of variables and finite number of constraints, or other way around</li>
<li>Shape optimization, Topology optimization — optimization over a set of regions
<ul>
<li>Topological derivative — derivative with respect to changing in the shape</li>
</ul>
</li>
<li>Generalized semi-infinite programming — finite number of variables, infinite number of constraints</li>
</ul>
<ul>
<li>Topological derivative — derivative with respect to changing in the shape</li>
</ul>
<h3>Uncertainty and randomness</h3>
<ul>
<li>Approaches to deal with uncertainty:
<ul>
<li>Markov decision process</li>
<li>Partially observable Markov decision process</li>
<li>Probabilistic-based design optimization</li>
<li>Robust optimization
<ul>
<li>Wald's maximin model</li>
</ul>
</li>
<li>Scenario optimization — constraints are uncertain</li>
<li>Stochastic approximation</li>
<li>Stochastic optimization</li>
<li>Stochastic programming</li>
<li>Stochastic gradient descent</li>
</ul>
</li>
<li>Random optimization algorithms:
<ul>
<li>Random search — choose a point randomly in ball around current iterate</li>
<li>Simulated annealing
<ul>
<li>Adaptive simulated annealing — variant in which the algorithm parameters are adjusted during the computation.</li>
<li>Great Deluge algorithm</li>
<li>Mean field annealing — deterministic variant of simulated annealing</li>
</ul>
</li>
<li>Bayesian optimization — treats objective function as a random function and places a prior over it</li>
<li>Evolutionary algorithm
<ul>
<li>Differential evolution</li>
<li>Evolutionary programming</li>
<li>Genetic algorithm, Genetic programming
<ul>
<li>Genetic algorithms in economics</li>
</ul>
</li>
<li>MCACEA (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) — uses an evolutionary algorithm for every agent</li>
<li>Simultaneous perturbation stochastic approximation (SPSA)</li>
</ul>
</li>
<li>Luus–Jaakola</li>
<li>Particle swarm optimization</li>
<li>Stochastic tunneling</li>
<li>Harmony search — mimicks the improvisation process of musicians</li>
<li>see also the section <i>Monte Carlo method</i></li>
</ul>
</li>
</ul>
<ul>
<li>Markov decision process</li>
<li>Partially observable Markov decision process</li>
<li>Probabilistic-based design optimization</li>
<li>Robust optimization
<ul>
<li>Wald's maximin model</li>
</ul>
</li>
<li>Scenario optimization — constraints are uncertain</li>
<li>Stochastic approximation</li>
<li>Stochastic optimization</li>
<li>Stochastic programming</li>
<li>Stochastic gradient descent</li>
</ul>
<ul>
<li>Wald's maximin model</li>
</ul>
<ul>
<li>Random search — choose a point randomly in ball around current iterate</li>
<li>Simulated annealing
<ul>
<li>Adaptive simulated annealing — variant in which the algorithm parameters are adjusted during the computation.</li>
<li>Great Deluge algorithm</li>
<li>Mean field annealing — deterministic variant of simulated annealing</li>
</ul>
</li>
<li>Bayesian optimization — treats objective function as a random function and places a prior over it</li>
<li>Evolutionary algorithm
<ul>
<li>Differential evolution</li>
<li>Evolutionary programming</li>
<li>Genetic algorithm, Genetic programming
<ul>
<li>Genetic algorithms in economics</li>
</ul>
</li>
<li>MCACEA (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) — uses an evolutionary algorithm for every agent</li>
<li>Simultaneous perturbation stochastic approximation (SPSA)</li>
</ul>
</li>
<li>Luus–Jaakola</li>
<li>Particle swarm optimization</li>
<li>Stochastic tunneling</li>
<li>Harmony search — mimicks the improvisation process of musicians</li>
<li>see also the section <i>Monte Carlo method</i></li>
</ul>
<ul>
<li>Adaptive simulated annealing — variant in which the algorithm parameters are adjusted during the computation.</li>
<li>Great Deluge algorithm</li>
<li>Mean field annealing — deterministic variant of simulated annealing</li>
</ul>
<ul>
<li>Differential evolution</li>
<li>Evolutionary programming</li>
<li>Genetic algorithm, Genetic programming
<ul>
<li>Genetic algorithms in economics</li>
</ul>
</li>
<li>MCACEA (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) — uses an evolutionary algorithm for every agent</li>
<li>Simultaneous perturbation stochastic approximation (SPSA)</li>
</ul>
<ul>
<li>Genetic algorithms in economics</li>
</ul>
<h3>Theoretical aspects</h3>
<ul>
<li>Convex analysis — function <i>f</i> such that <i>f</i>(<i>tx</i> + (1 − <i>t</i>)<i>y</i>) ≥ <i>tf</i>(<i>x</i>) + (1 − <i>t</i>)<i>f</i>(<i>y</i>) for <i>t</i> ∈ [0,1]
<ul>
<li>Pseudoconvex function — function <i>f</i> such that ∇<i>f</i> · (<i>y</i> − <i>x</i>) ≥ 0 implies <i>f</i>(<i>y</i>) ≥ <i>f</i>(<i>x</i>)</li>
<li>Quasiconvex function — function <i>f</i> such that <i>f</i>(<i>tx</i> + (1 − <i>t</i>)<i>y</i>) ≤ max(<i>f</i>(<i>x</i>), <i>f</i>(<i>y</i>)) for <i>t</i> ∈ [0,1]</li>
<li>Subderivative</li>
<li>Geodesic convexity — convexity for functions defined on a Riemannian manifold</li>
</ul>
</li>
<li>Duality (optimization)
<ul>
<li>Weak duality — dual solution gives a bound on the primal solution</li>
<li>Strong duality — primal and dual solutions are equivalent</li>
<li>Shadow price</li>
<li>Dual cone and polar cone</li>
<li>Duality gap — difference between primal and dual solution</li>
<li>Fenchel's duality theorem — relates minimization problems with maximization problems of convex conjugates</li>
<li>Perturbation function — any function which relates to primal and dual problems</li>
<li>Slater's condition — sufficient condition for strong duality to hold in a convex optimization problem</li>
<li>Total dual integrality — concept of duality for integer linear programming</li>
<li>Wolfe duality — for when objective function and constraints are differentiable</li>
</ul>
</li>
<li>Farkas' lemma</li>
<li>Karush–Kuhn–Tucker conditions (KKT) — sufficient conditions for a solution to be optimal
<ul>
<li>Fritz John conditions — variant of KKT conditions</li>
</ul>
</li>
<li>Lagrange multiplier
<ul>
<li>Lagrange multipliers on Banach spaces</li>
</ul>
</li>
<li>Semi-continuity</li>
<li>Complementarity theory — study of problems with constraints of the form 〈<i>u</i>, <i>v</i>〉 = 0
<ul>
<li>Mixed complementarity problem
<ul>
<li>Mixed linear complementarity problem</li>
<li>Lemke's algorithm — method for solving (mixed) linear complementarity problems</li>
</ul>
</li>
</ul>
</li>
<li>Danskin's theorem — used in the analysis of minimax problems</li>
<li>Maximum theorem — the maximum and maximizer are continuous as function of parameters, under some conditions</li>
<li>No free lunch in search and optimization</li>
<li>Relaxation (approximation) — approximating a given problem by an easier problem by relaxing some constraints
<ul>
<li>Lagrangian relaxation</li>
<li>Linear programming relaxation — ignoring the integrality constraints in a linear programming problem</li>
</ul>
</li>
<li>Self-concordant function</li>
<li>Reduced cost — cost for increasing a variable by a small amount</li>
<li>Hardness of approximation — computational complexity of getting an approximate solution</li>
</ul>
<ul>
<li>Pseudoconvex function — function <i>f</i> such that ∇<i>f</i> · (<i>y</i> − <i>x</i>) ≥ 0 implies <i>f</i>(<i>y</i>) ≥ <i>f</i>(<i>x</i>)</li>
<li>Quasiconvex function — function <i>f</i> such that <i>f</i>(<i>tx</i> + (1 − <i>t</i>)<i>y</i>) ≤ max(<i>f</i>(<i>x</i>), <i>f</i>(<i>y</i>)) for <i>t</i> ∈ [0,1]</li>
<li>Subderivative</li>
<li>Geodesic convexity — convexity for functions defined on a Riemannian manifold</li>
</ul>
<ul>
<li>Weak duality — dual solution gives a bound on the primal solution</li>
<li>Strong duality — primal and dual solutions are equivalent</li>
<li>Shadow price</li>
<li>Dual cone and polar cone</li>
<li>Duality gap — difference between primal and dual solution</li>
<li>Fenchel's duality theorem — relates minimization problems with maximization problems of convex conjugates</li>
<li>Perturbation function — any function which relates to primal and dual problems</li>
<li>Slater's condition — sufficient condition for strong duality to hold in a convex optimization problem</li>
<li>Total dual integrality — concept of duality for integer linear programming</li>
<li>Wolfe duality — for when objective function and constraints are differentiable</li>
</ul>
<ul>
<li>Fritz John conditions — variant of KKT conditions</li>
</ul>
<ul>
<li>Lagrange multipliers on Banach spaces</li>
</ul>
<ul>
<li>Mixed complementarity problem
<ul>
<li>Mixed linear complementarity problem</li>
<li>Lemke's algorithm — method for solving (mixed) linear complementarity problems</li>
</ul>
</li>
</ul>
<ul>
<li>Mixed linear complementarity problem</li>
<li>Lemke's algorithm — method for solving (mixed) linear complementarity problems</li>
</ul>
<ul>
<li>Lagrangian relaxation</li>
<li>Linear programming relaxation — ignoring the integrality constraints in a linear programming problem</li>
</ul>
<h3>Applications</h3>
<ul>
<li>In geometry:
<ul>
<li>Geometric median — the point minimizing the sum of distances to a given set of points</li>
<li>Chebyshev center — the centre of the smallest ball containing a given set of points</li>
</ul>
</li>
<li>In statistics:
<ul>
<li>Iterated conditional modes — maximizing joint probability of Markov random field</li>
<li>Response surface methodology — used in the design of experiments</li>
</ul>
</li>
<li>Automatic label placement</li>
<li>Compressed sensing — reconstruct a signal from knowledge that it is sparse or compressible</li>
<li>Cutting stock problem</li>
<li>Demand optimization</li>
<li>Destination dispatch — an optimization technique for dispatching elevators</li>
<li>Energy minimization</li>
<li>Entropy maximization</li>
<li>Highly optimized tolerance</li>
<li>Hyperparameter optimization</li>
<li>Inventory control problem
<ul>
<li>Newsvendor model</li>
<li>Extended newsvendor model</li>
<li>Assemble-to-order system</li>
</ul>
</li>
<li>Linear programming decoding</li>
<li>Linear search problem — find a point on a line by moving along the line</li>
<li>Low-rank approximation — find best approximation, constraint is that rank of some matrix is smaller than a given number</li>
<li>Meta-optimization — optimization of the parameters in an optimization method</li>
<li>Multidisciplinary design optimization</li>
<li>Optimal computing budget allocation — maximize the overall simulation efficiency for finding an optimal decision</li>
<li>Paper bag problem</li>
<li>Process optimization</li>
<li>Recursive economics — individuals make a series of two-period optimization decisions over time.</li>
<li>Stigler diet</li>
<li>Space allocation problem</li>
<li>Stress majorization</li>
<li>Trajectory optimization</li>
<li>Transportation theory</li>
<li>Wing-shape optimization</li>
</ul>
<ul>
<li>Geometric median — the point minimizing the sum of distances to a given set of points</li>
<li>Chebyshev center — the centre of the smallest ball containing a given set of points</li>
</ul>
<ul>
<li>Iterated conditional modes — maximizing joint probability of Markov random field</li>
<li>Response surface methodology — used in the design of experiments</li>
</ul>
<ul>
<li>Newsvendor model</li>
<li>Extended newsvendor model</li>
<li>Assemble-to-order system</li>
</ul>
<h3>Miscellaneous</h3>
<ul>
<li>Combinatorial optimization</li>
<li>Dynamic programming
<ul>
<li>Bellman equation</li>
<li>Hamilton–Jacobi–Bellman equation — continuous-time analogue of Bellman equation</li>
<li>Backward induction — solving dynamic programming problems by reasoning backwards in time</li>
<li>Optimal stopping — choosing the optimal time to take a particular action
<ul>
<li>Odds algorithm</li>
<li>Robbins' problem</li>
</ul>
</li>
</ul>
</li>
<li>Global optimization:
<ul>
<li>BRST algorithm</li>
<li>MCS algorithm</li>
</ul>
</li>
<li>Multi-objective optimization — there are multiple conflicting objectives
<ul>
<li>Benson's algorithm — for linear vector optimization problems</li>
</ul>
</li>
<li>Bilevel optimization — studies problems in which one problem is embedded in another</li>
<li>Optimal substructure</li>
<li>Dykstra's projection algorithm — finds a point in intersection of two convex sets</li>
<li>Algorithmic concepts:
<ul>
<li>Barrier function</li>
<li>Penalty method</li>
<li>Trust region</li>
</ul>
</li>
<li>Test functions for optimization:
<ul>
<li>Rosenbrock function — two-dimensional function with a banana-shaped valley</li>
<li>Himmelblau's function — two-dimensional with four local minima, defined by <img class="mwe-math-fallback-image-inline tex" alt="f(x, y) = (x^2+y-11)^2 + (x+y^2-7)^2" src="//upload.wikimedia.org/math/6/8/5/685f9d98d3d44d2bdd27a676ace57b62.png"></li>
<li>Rastrigin function — two-dimensional function with many local minima</li>
<li>Shekel function — multimodal and multidimensional</li>
</ul>
</li>
<li>Mathematical Optimization Society</li>
</ul>
<ul>
<li>Bellman equation</li>
<li>Hamilton–Jacobi–Bellman equation — continuous-time analogue of Bellman equation</li>
<li>Backward induction — solving dynamic programming problems by reasoning backwards in time</li>
<li>Optimal stopping — choosing the optimal time to take a particular action
<ul>
<li>Odds algorithm</li>
<li>Robbins' problem</li>
</ul>
</li>
</ul>
<ul>
<li>Odds algorithm</li>
<li>Robbins' problem</li>
</ul>
<ul>
<li>BRST algorithm</li>
<li>MCS algorithm</li>
</ul>
<ul>
<li>Benson's algorithm — for linear vector optimization problems</li>
</ul>
<ul>
<li>Barrier function</li>
<li>Penalty method</li>
<li>Trust region</li>
</ul>
<ul>
<li>Rosenbrock function — two-dimensional function with a banana-shaped valley</li>
<li>Himmelblau's function — two-dimensional with four local minima, defined by <img class="mwe-math-fallback-image-inline tex" alt="f(x, y) = (x^2+y-11)^2 + (x+y^2-7)^2" src="//upload.wikimedia.org/math/6/8/5/685f9d98d3d44d2bdd27a676ace57b62.png"></li>
<li>Rastrigin function — two-dimensional function with many local minima</li>
<li>Shekel function — multimodal and multidimensional</li>
</ul>
<h2>Numerical quadrature (integration)</h2>
<p>Numerical integration — the numerical evaluation of an integral</p>
<ul>
<li>Rectangle method — first-order method, based on (piecewise) constant approximation</li>
<li>Trapezoidal rule — second-order method, based on (piecewise) linear approximation</li>
<li>Simpson's rule — fourth-order method, based on (piecewise) quadratic approximation
<ul>
<li>Adaptive Simpson's method</li>
</ul>
</li>
<li>Boole's rule — sixth-order method, based on the values at five equidistant points</li>
<li>Newton–Cotes formulas — generalizes the above methods</li>
<li>Romberg's method — Richardson extrapolation applied to trapezium rule</li>
<li>Gaussian quadrature — highest possible degree with given number of points
<ul>
<li>Chebyshev–Gauss quadrature — extension of Gaussian quadrature for integrals with weight (1 − <i>x</i>) on [−1, 1]</li>
<li>Gauss–Hermite quadrature — extension of Gaussian quadrature for integrals with weight exp(−<i>x</i>) on [−∞, ∞]</li>
<li>Gauss–Jacobi quadrature — extension of Gaussian quadrature for integrals with weight (1 − <i>x</i>) (1 + <i>x</i>) on [−1, 1]</li>
<li>Gauss–Laguerre quadrature — extension of Gaussian quadrature for integrals with weight exp(−<i>x</i>) on [0, ∞]</li>
<li>Gauss–Kronrod quadrature formula — nested rule based on Gaussian quadrature</li>
<li>Gauss–Kronrod rules</li>
</ul>
</li>
<li>Tanh-sinh quadrature — variant of Gaussian quadrature which works well with singularities at the end points</li>
<li>Clenshaw–Curtis quadrature — based on expanding the integrand in terms of Chebyshev polynomials</li>
<li>Adaptive quadrature — adapting the subintervals in which the integration interval is divided depending on the integrand</li>
<li>Monte Carlo integration — takes random samples of the integrand
<ul>
<li><i>See also #Monte Carlo method</i></li>
</ul>
</li>
<li>Quantized state systems method (QSS) — based on the idea of state quantization</li>
<li>Lebedev quadrature — uses a grid on a sphere with octahedral symmetry</li>
<li>Sparse grid</li>
<li>Coopmans approximation</li>
<li>Numerical differentiation — for fractional-order integrals
<ul>
<li>Numerical smoothing and differentiation</li>
<li>Adjoint state method — approximates gradient of a function in an optimization problem</li>
</ul>
</li>
<li>Euler–Maclaurin formula</li>
</ul>
<ul>
<li>Adaptive Simpson's method</li>
</ul>
<ul>
<li>Chebyshev–Gauss quadrature — extension of Gaussian quadrature for integrals with weight (1 − <i>x</i>) on [−1, 1]</li>
<li>Gauss–Hermite quadrature — extension of Gaussian quadrature for integrals with weight exp(−<i>x</i>) on [−∞, ∞]</li>
<li>Gauss–Jacobi quadrature — extension of Gaussian quadrature for integrals with weight (1 − <i>x</i>) (1 + <i>x</i>) on [−1, 1]</li>
<li>Gauss–Laguerre quadrature — extension of Gaussian quadrature for integrals with weight exp(−<i>x</i>) on [0, ∞]</li>
<li>Gauss–Kronrod quadrature formula — nested rule based on Gaussian quadrature</li>
<li>Gauss–Kronrod rules</li>
</ul>
<ul>
<li><i>See also #Monte Carlo method</i></li>
</ul>
<ul>
<li>Numerical smoothing and differentiation</li>
<li>Adjoint state method — approximates gradient of a function in an optimization problem</li>
</ul>
<h2>Numerical methods for ordinary differential equations</h2>
<p>Numerical methods for ordinary differential equations — the numerical solution of ordinary differential equations (ODEs)</p>
<ul>
<li>Euler method — the most basic method for solving an ODE</li>
<li>Explicit and implicit methods — implicit methods need to solve an equation at every step</li>
<li>Backward Euler method — implicit variant of the Euler method</li>
<li>Trapezoidal rule — second-order implicit method</li>
<li>Runge–Kutta methods — one of the two main classes of methods for initial-value problems
<ul>
<li>Midpoint method — a second-order method with two stages</li>
<li>Heun's method — either a second-order method with two stages, or a third-order method with three stages</li>
<li>Bogacki–Shampine method — a third-order method with four stages (FSAL) and an embedded fourth-order method</li>
<li>Cash–Karp method — a fifth-order method with six stages and an embedded fourth-order method</li>
<li>Dormand–Prince method — a fifth-order method with seven stages (FSAL) and an embedded fourth-order method</li>
<li>Runge–Kutta–Fehlberg method — a fifth-order method with six stages and an embedded fourth-order method</li>
<li>Gauss–Legendre method — family of A-stable method with optimal order based on Gaussian quadrature</li>
<li>Butcher group — algebraic formalism involving rooted trees for analysing Runge–Kutta methods</li>
<li>List of Runge–Kutta methods</li>
</ul>
</li>
<li>Linear multistep method — the other main class of methods for initial-value problems
<ul>
<li>Backward differentiation formula — implicit methods of order 2 to 6; especially suitable for stiff equations</li>
<li>Numerov's method — fourth-order method for equations of the form <img class="mwe-math-fallback-image-inline tex" alt="y'' = f(t,y)" src="//upload.wikimedia.org/math/5/c/e/5ce0178f035324c4e051a319cbb53055.png"></li>
<li>Predictor–corrector method — uses one method to approximate solution and another one to increase accuracy</li>
</ul>
</li>
<li>General linear methods — a class of methods encapsulating linear multistep and Runge-Kutta methods</li>
<li>Bulirsch–Stoer algorithm — combines the midpoint method with Richardson extrapolation to attain arbitrary order</li>
<li>Exponential integrator — based on splitting ODE in a linear part, which is solved exactly, and a nonlinear part</li>
<li>Methods designed for the solution of ODEs from classical physics:
<ul>
<li>Newmark-beta method — based on the extended mean-value theorem</li>
<li>Verlet integration — a popular second-order method</li>
<li>Leapfrog integration — another name for Verlet integration</li>
<li>Beeman's algorithm — a two-step method extending the Verlet method</li>
<li>Dynamic relaxation</li>
</ul>
</li>
<li>Geometric integrator — a method that preserves some geometric structure of the equation
<ul>
<li>Symplectic integrator — a method for the solution of Hamilton's equations that preserves the symplectic structure
<ul>
<li>Variational integrator — symplectic integrators derived using the underlying variational principle</li>
<li>Semi-implicit Euler method — variant of Euler method which is symplectic when applied to separable Hamiltonians</li>
</ul>
</li>
<li>Energy drift — phenomenon that energy, which should be conserved, drifts away due to numerical errors</li>
</ul>
</li>
<li>Other methods for initial value problems (IVPs):
<ul>
<li>Bi-directional delay line</li>
<li>Partial element equivalent circuit</li>
</ul>
</li>
<li>Methods for solving two-point boundary value problems (BVPs):
<ul>
<li>Shooting method</li>
<li>Direct multiple shooting method — divides interval in several subintervals and applies the shooting method on each subinterval</li>
</ul>
</li>
<li>Methods for solving differential-algebraic equations (DAEs), i.e., ODEs with constraints:
<ul>
<li>Constraint algorithm — for solving Newton's equations with constraints</li>
<li>Pantelides algorithm — for reducing the index of a DEA</li>
</ul>
</li>
<li>Methods for solving stochastic differential equations (SDEs):
<ul>
<li>Euler–Maruyama method — generalization of the Euler method for SDEs</li>
<li>Milstein method — a method with strong order one</li>
<li>Runge–Kutta method (SDE) — generalization of the family of Runge–Kutta methods for SDEs</li>
</ul>
</li>
<li>Methods for solving integral equations:
<ul>
<li>Nyström method — replaces the integral with a quadrature rule</li>
</ul>
</li>
<li>Analysis:
<ul>
<li>Truncation error (numerical integration) — local and global truncation errors, and their relationships
<ul>
<li>Lady Windermere's Fan (mathematics) — telescopic identity relating local and global truncation errors</li>
</ul>
</li>
</ul>
</li>
<li>Stiff equation — roughly, an ODE for which unstable methods need a very short step size, but stable methods do not
<ul>
<li>L-stability — method is A-stable and stability function vanishes at infinity</li>
<li>Dynamic errors of numerical methods of ODE discretization — logarithm of stability function</li>
</ul>
</li>
<li>Adaptive stepsize — automatically changing the step size when that seems advantageous</li>
</ul>
<ul>
<li>Midpoint method — a second-order method with two stages</li>
<li>Heun's method — either a second-order method with two stages, or a third-order method with three stages</li>
<li>Bogacki–Shampine method — a third-order method with four stages (FSAL) and an embedded fourth-order method</li>
<li>Cash–Karp method — a fifth-order method with six stages and an embedded fourth-order method</li>
<li>Dormand–Prince method — a fifth-order method with seven stages (FSAL) and an embedded fourth-order method</li>
<li>Runge–Kutta–Fehlberg method — a fifth-order method with six stages and an embedded fourth-order method</li>
<li>Gauss–Legendre method — family of A-stable method with optimal order based on Gaussian quadrature</li>
<li>Butcher group — algebraic formalism involving rooted trees for analysing Runge–Kutta methods</li>
<li>List of Runge–Kutta methods</li>
</ul>
<ul>
<li>Backward differentiation formula — implicit methods of order 2 to 6; especially suitable for stiff equations</li>
<li>Numerov's method — fourth-order method for equations of the form <img class="mwe-math-fallback-image-inline tex" alt="y'' = f(t,y)" src="//upload.wikimedia.org/math/5/c/e/5ce0178f035324c4e051a319cbb53055.png"></li>
<li>Predictor–corrector method — uses one method to approximate solution and another one to increase accuracy</li>
</ul>
<ul>
<li>Newmark-beta method — based on the extended mean-value theorem</li>
<li>Verlet integration — a popular second-order method</li>
<li>Leapfrog integration — another name for Verlet integration</li>
<li>Beeman's algorithm — a two-step method extending the Verlet method</li>
<li>Dynamic relaxation</li>
</ul>
<ul>
<li>Symplectic integrator — a method for the solution of Hamilton's equations that preserves the symplectic structure
<ul>
<li>Variational integrator — symplectic integrators derived using the underlying variational principle</li>
<li>Semi-implicit Euler method — variant of Euler method which is symplectic when applied to separable Hamiltonians</li>
</ul>
</li>
<li>Energy drift — phenomenon that energy, which should be conserved, drifts away due to numerical errors</li>
</ul>
<ul>
<li>Variational integrator — symplectic integrators derived using the underlying variational principle</li>
<li>Semi-implicit Euler method — variant of Euler method which is symplectic when applied to separable Hamiltonians</li>
</ul>
<ul>
<li>Bi-directional delay line</li>
<li>Partial element equivalent circuit</li>
</ul>
<ul>
<li>Shooting method</li>
<li>Direct multiple shooting method — divides interval in several subintervals and applies the shooting method on each subinterval</li>
</ul>
<ul>
<li>Constraint algorithm — for solving Newton's equations with constraints</li>
<li>Pantelides algorithm — for reducing the index of a DEA</li>
</ul>
<ul>
<li>Euler–Maruyama method — generalization of the Euler method for SDEs</li>
<li>Milstein method — a method with strong order one</li>
<li>Runge–Kutta method (SDE) — generalization of the family of Runge–Kutta methods for SDEs</li>
</ul>
<ul>
<li>Nyström method — replaces the integral with a quadrature rule</li>
</ul>
<ul>
<li>Truncation error (numerical integration) — local and global truncation errors, and their relationships
<ul>
<li>Lady Windermere's Fan (mathematics) — telescopic identity relating local and global truncation errors</li>
</ul>
</li>
</ul>
<ul>
<li>Lady Windermere's Fan (mathematics) — telescopic identity relating local and global truncation errors</li>
</ul>
<ul>
<li>L-stability — method is A-stable and stability function vanishes at infinity</li>
<li>Dynamic errors of numerical methods of ODE discretization — logarithm of stability function</li>
</ul>
<h2>Numerical methods for partial differential equations</h2>
<p>Numerical partial differential equations — the numerical solution of partial differential equations (PDEs)</p>
<h3>Finite difference methods</h3>
<p>Finite difference method — based on approximating differential operators with difference operators</p>
<ul>
<li>Finite difference — the discrete analogue of a differential operator
<ul>
<li>Finite difference coefficient — table of coefficients of finite-difference approximations to derivatives</li>
<li>Discrete Laplace operator — finite-difference approximation of the Laplace operator
<ul>
<li>Eigenvalues and eigenvectors of the second derivative — includes eigenvalues of discrete Laplace operator</li>
<li>Kronecker sum of discrete Laplacians — used for Laplace operator in multiple dimensions</li>
</ul>
</li>
<li>Discrete Poisson equation — discrete analogue of the Poisson equation using the discrete Laplace operator</li>
</ul>
</li>
<li>Stencil (numerical analysis) — the geometric arrangements of grid points affected by a basic step of the algorithm
<ul>
<li>Compact stencil — stencil which only uses a few grid points, usually only the immediate and diagonal neighbours
<ul>
<li>Higher-order compact finite difference scheme</li>
</ul>
</li>
<li>Non-compact stencil — any stencil that is not compact</li>
<li>Five-point stencil — two-dimensional stencil consisting of a point and its four immediate neighbours on a rectangular grid</li>
</ul>
</li>
<li>Finite difference methods for heat equation and related PDEs:
<ul>
<li>FTCS scheme (forward-time central-space) — first-order explicit</li>
<li>Crank–Nicolson method — second-order implicit</li>
</ul>
</li>
<li>Finite difference methods for hyperbolic PDEs like the wave equation:
<ul>
<li>Lax–Friedrichs method — first-order explicit</li>
<li>Lax–Wendroff method — second-order explicit</li>
<li>MacCormack method — second-order explicit</li>
<li>Upwind scheme
<ul>
<li>Upwind differencing scheme for convection — first-order scheme for convection–diffusion problems</li>
</ul>
</li>
<li>Lax–Wendroff theorem — conservative scheme for hyperbolic system of conservation laws converges to the weak solution</li>
</ul>
</li>
<li>Alternating direction implicit method (ADI) — update using the flow in <i>x</i>-direction and then using flow in <i>y</i>-direction</li>
<li>Nonstandard finite difference scheme</li>
<li>Specific applications:
<ul>
<li>Finite difference methods for option pricing</li>
<li>Finite-difference time-domain method — a finite-difference method for electrodynamics</li>
</ul>
</li>
</ul>
<ul>
<li>Finite difference coefficient — table of coefficients of finite-difference approximations to derivatives</li>
<li>Discrete Laplace operator — finite-difference approximation of the Laplace operator
<ul>
<li>Eigenvalues and eigenvectors of the second derivative — includes eigenvalues of discrete Laplace operator</li>
<li>Kronecker sum of discrete Laplacians — used for Laplace operator in multiple dimensions</li>
</ul>
</li>
<li>Discrete Poisson equation — discrete analogue of the Poisson equation using the discrete Laplace operator</li>
</ul>
<ul>
<li>Eigenvalues and eigenvectors of the second derivative — includes eigenvalues of discrete Laplace operator</li>
<li>Kronecker sum of discrete Laplacians — used for Laplace operator in multiple dimensions</li>
</ul>
<ul>
<li>Compact stencil — stencil which only uses a few grid points, usually only the immediate and diagonal neighbours
<ul>
<li>Higher-order compact finite difference scheme</li>
</ul>
</li>
<li>Non-compact stencil — any stencil that is not compact</li>
<li>Five-point stencil — two-dimensional stencil consisting of a point and its four immediate neighbours on a rectangular grid</li>
</ul>
<ul>
<li>Higher-order compact finite difference scheme</li>
</ul>
<ul>
<li>FTCS scheme (forward-time central-space) — first-order explicit</li>
<li>Crank–Nicolson method — second-order implicit</li>
</ul>
<ul>
<li>Lax–Friedrichs method — first-order explicit</li>
<li>Lax–Wendroff method — second-order explicit</li>
<li>MacCormack method — second-order explicit</li>
<li>Upwind scheme
<ul>
<li>Upwind differencing scheme for convection — first-order scheme for convection–diffusion problems</li>
</ul>
</li>
<li>Lax–Wendroff theorem — conservative scheme for hyperbolic system of conservation laws converges to the weak solution</li>
</ul>
<ul>
<li>Upwind differencing scheme for convection — first-order scheme for convection–diffusion problems</li>
</ul>
<ul>
<li>Finite difference methods for option pricing</li>
<li>Finite-difference time-domain method — a finite-difference method for electrodynamics</li>
</ul>
<h3>Finite element methods</h3>
<p>Finite element method — based on a discretization of the space of solutions</p>
<ul>
<li>Finite element method in structural mechanics — a physical approach to finite element methods</li>
<li>Galerkin method — a finite element method in which the residual is orthogonal to the finite element space
<ul>
<li>Discontinuous Galerkin method — a Galerkin method in which the approximate solution is not continuous</li>
</ul>
</li>
<li>Rayleigh–Ritz method — a finite element method based on variational principles</li>
<li>Spectral element method — high-order finite element methods</li>
<li>hp-FEM — variant in which both the size and the order of the elements are automatically adapted</li>
<li>Examples of finite elemets:
<ul>
<li>Bilinear quadrilateral element — also known as the Q4 element</li>
<li>Constant strain triangle element (CST) — also known as the T3 element</li>
<li>Barsoum elements</li>
</ul>
</li>
<li>Direct stiffness method — a particular implementation of the finite element method, often used in structural analysis</li>
<li>Trefftz method</li>
<li>Finite element updating</li>
<li>Extended finite element method — puts functions tailored to the problem in the approximation space</li>
<li>Functionally graded elements — elements for describing functionally graded materials</li>
<li>Superelement — particular grouping of finite elements, employed as a single element</li>
<li>Interval finite element method — combination of finite elements with interval arithmetic</li>
<li>Discrete exterior calculus — discrete form of the exterior calculus of differential geometry</li>
<li>Modal analysis using FEM — solution of eigenvalue problems to find natural vibrations</li>
<li>Céa's lemma — solution in the finite-element space is an almost best approximation in that space of the true solution</li>
<li>Patch test (finite elements) — simple test for the quality of a finite element</li>
<li>MAFELAP (MAthematics of Finite ELements and APplications) — international conference held at Brunel University</li>
<li>NAFEMS — not-for-profit organisation that sets and maintains standards in computer-aided engineering analysis</li>
<li>Multiphase topology optimisation — technique based on finite elements for determining optimal composition of a mixture</li>
<li>Interval finite element</li>
<li>Applied element method — for simulation of cracks and structural collapse</li>
<li>Wood–Armer method — structural analysis method based on finite elements used to design reinforcement for concrete slabs</li>
<li>Isogeometric analysis — integrates finite elements into conventional NURBS-based CAD design tools</li>
<li>Stiffness matrix — finite-dimensional analogue of differential operator</li>
<li>Combination with meshfree methods:
<ul>
<li>Weakened weak form — form of a PDE that is weaker than the standard weak form</li>
<li>G space — functional space used in formulating the weakened weak form</li>
<li>Smoothed finite element method</li>
</ul>
</li>
<li>List of finite element software packages</li>
</ul>
<ul>
<li>Discontinuous Galerkin method — a Galerkin method in which the approximate solution is not continuous</li>
</ul>
<ul>
<li>Bilinear quadrilateral element — also known as the Q4 element</li>
<li>Constant strain triangle element (CST) — also known as the T3 element</li>
<li>Barsoum elements</li>
</ul>
<ul>
<li>Weakened weak form — form of a PDE that is weaker than the standard weak form</li>
<li>G space — functional space used in formulating the weakened weak form</li>
<li>Smoothed finite element method</li>
</ul>
<h3>Other methods</h3>
<ul>
<li>Spectral method — based on the Fourier transformation
<ul>
<li>Pseudo-spectral method</li>
</ul>
</li>
<li>Method of lines — reduces the PDE to a large system of ordinary differential equations</li>
<li>Boundary element method (BEM) — based on transforming the PDE to an integral equation on the boundary of the domain
<ul>
<li>Interval boundary element method — a version using interval arithmetics</li>
</ul>
</li>
<li>Analytic element method — similar to the boundary element method, but the integral equation is evaluated analytically</li>
<li>Finite volume method — based on dividing the domain in many small domains; popular in computational fluid dynamics
<ul>
<li>Godunov's scheme — first-order conservative scheme for fluid flow, based on piecewise constant approximation</li>
<li>MUSCL scheme — second-order variant of Godunov's scheme</li>
<li>AUSM — advection upstream splitting method</li>
<li>Flux limiter — limits spatial derivatives (fluxes) in order to avoid spurious oscillations</li>
<li>Riemann solver — a solver for Riemann problems (a conservation law with piecewise constant data)</li>
<li>Properties of discretization schemes — finite volume methods can be conservative, bounded, etc.</li>
</ul>
</li>
<li>Discrete element method — a method in which the elements can move freely relative to each other
<ul>
<li>Extended discrete element method — adds properties such as strain to each particle</li>
<li>Movable cellular automaton — combination of cellular automata with discrete elements</li>
</ul>
</li>
<li>Meshfree methods — does not use a mesh, but uses a particle view of the field
<ul>
<li>Discrete least squares meshless method — based on minimization of weighted summation of the squared residual</li>
<li>Diffuse element method</li>
<li>Finite pointset method — represent continuum by a point cloud</li>
<li>Moving Particle Semi-implicit Method</li>
<li>Method of fundamental solutions (MFS) — represents solution as linear combination of fundamental solutions</li>
<li>Variants of MFS with source points on the physical boundary:
<ul>
<li>Boundary knot method (BKM)</li>
<li>Boundary particle method (BPM)</li>
<li>Regularized meshless method (RMM)</li>
<li>Singular boundary method (SBM)</li>
</ul>
</li>
</ul>
</li>
<li>Methods designed for problems from electromagnetics:
<ul>
<li>Finite-difference time-domain method — a finite-difference method</li>
<li>Rigorous coupled-wave analysis — semi-analytical Fourier-space method based on Floquet's theorem</li>
<li>Transmission-line matrix method (TLM) — based on analogy between electromagnetic field and mesh of transmission lines</li>
<li>Uniform theory of diffraction — specifically designed for scattering problems</li>
</ul>
</li>
<li>Particle-in-cell — used especially in fluid dynamics
<ul>
<li>Multiphase particle-in-cell method — considers solid particles as both numerical particles and fluid</li>
</ul>
</li>
<li>High-resolution scheme</li>
<li>Shock capturing method</li>
<li>Vorticity confinement — for vortex-dominated flows in fluid dynamics, similar to shock capturing</li>
<li>Split-step method</li>
<li>Fast marching method</li>
<li>Orthogonal collocation</li>
<li>Lattice Boltzmann methods — for the solution of the Navier-Stokes equations</li>
<li>Roe solver — for the solution of the Euler equation</li>
<li>Relaxation (iterative method) — a method for solving elliptic PDEs by converting them to evolution equations</li>
<li>Broad classes of methods:
<ul>
<li>Mimetic methods — methods that respect in some sense the structure of the original problem</li>
<li>Multiphysics — models consisting of various submodels with different physics</li>
<li>Immersed boundary method — for simulating elastic structures immersed within fluids</li>
</ul>
</li>
<li>Multisymplectic integrator — extension of symplectic integrators, which are for ODEs</li>
<li>Stretched grid method — for problems solution that can be related to an elastic grid behavior.</li>
</ul>
<ul>
<li>Pseudo-spectral method</li>
</ul>
<ul>
<li>Interval boundary element method — a version using interval arithmetics</li>
</ul>
<ul>
<li>Godunov's scheme — first-order conservative scheme for fluid flow, based on piecewise constant approximation</li>
<li>MUSCL scheme — second-order variant of Godunov's scheme</li>
<li>AUSM — advection upstream splitting method</li>
<li>Flux limiter — limits spatial derivatives (fluxes) in order to avoid spurious oscillations</li>
<li>Riemann solver — a solver for Riemann problems (a conservation law with piecewise constant data)</li>
<li>Properties of discretization schemes — finite volume methods can be conservative, bounded, etc.</li>
</ul>
<ul>
<li>Extended discrete element method — adds properties such as strain to each particle</li>
<li>Movable cellular automaton — combination of cellular automata with discrete elements</li>
</ul>
<ul>
<li>Discrete least squares meshless method — based on minimization of weighted summation of the squared residual</li>
<li>Diffuse element method</li>
<li>Finite pointset method — represent continuum by a point cloud</li>
<li>Moving Particle Semi-implicit Method</li>
<li>Method of fundamental solutions (MFS) — represents solution as linear combination of fundamental solutions</li>
<li>Variants of MFS with source points on the physical boundary:
<ul>
<li>Boundary knot method (BKM)</li>
<li>Boundary particle method (BPM)</li>
<li>Regularized meshless method (RMM)</li>
<li>Singular boundary method (SBM)</li>
</ul>
</li>
</ul>
<ul>
<li>Boundary knot method (BKM)</li>
<li>Boundary particle method (BPM)</li>
<li>Regularized meshless method (RMM)</li>
<li>Singular boundary method (SBM)</li>
</ul>
<ul>
<li>Finite-difference time-domain method — a finite-difference method</li>
<li>Rigorous coupled-wave analysis — semi-analytical Fourier-space method based on Floquet's theorem</li>
<li>Transmission-line matrix method (TLM) — based on analogy between electromagnetic field and mesh of transmission lines</li>
<li>Uniform theory of diffraction — specifically designed for scattering problems</li>
</ul>
<ul>
<li>Multiphase particle-in-cell method — considers solid particles as both numerical particles and fluid</li>
</ul>
<ul>
<li>Mimetic methods — methods that respect in some sense the structure of the original problem</li>
<li>Multiphysics — models consisting of various submodels with different physics</li>
<li>Immersed boundary method — for simulating elastic structures immersed within fluids</li>
</ul>
<h3>Techniques for improving these methods</h3>
<ul>
<li>Multigrid method — uses a hierarchy of nested meshes to speed up the methods</li>
<li>Domain decomposition methods — divides the domain in a few subdomains and solves the PDE on these subdomains
<ul>
<li>Additive Schwarz method</li>
<li>Abstract additive Schwarz method — abstract version of additive Schwarz without reference to geometric information</li>
<li>Balancing domain decomposition method (BDD) — preconditioner for symmetric positive definite matrices</li>
<li>Balancing domain decomposition by constraints (BDDC) — further development of BDD</li>
<li>Finite element tearing and interconnect (FETI)</li>
<li>FETI-DP — further development of FETI</li>
<li>Fictitious domain method — preconditioner constructed with a structured mesh on a fictitious domain of simple shape</li>
<li>Mortar methods — meshes on subdomain do not mesh</li>
<li>Neumann–Dirichlet method — combines Neumann problem on one subdomain with Dirichlet problem on other subdomain</li>
<li>Neumann–Neumann methods — domain decomposition methods that use Neumann problems on the subdomains</li>
<li>Poincaré–Steklov operator — maps tangential electric field onto the equivalent electric current</li>
<li>Schur complement method — early and basic method on subdomains that do not overlap</li>
<li>Schwarz alternating method — early and basic method on subdomains that overlap</li>
</ul>
</li>
<li>Coarse space — variant of the problem which uses a discretization with fewer degrees of freedom</li>
<li>Adaptive mesh refinement — uses the computed solution to refine the mesh only where necessary</li>
<li>Fast multipole method — hierarchical method for evaluating particle-particle interactions</li>
<li>Perfectly matched layer — artificial absorbing layer for wave equations, used to implement absorbing boundary conditions</li>
</ul>
<ul>
<li>Additive Schwarz method</li>
<li>Abstract additive Schwarz method — abstract version of additive Schwarz without reference to geometric information</li>
<li>Balancing domain decomposition method (BDD) — preconditioner for symmetric positive definite matrices</li>
<li>Balancing domain decomposition by constraints (BDDC) — further development of BDD</li>
<li>Finite element tearing and interconnect (FETI)</li>
<li>FETI-DP — further development of FETI</li>
<li>Fictitious domain method — preconditioner constructed with a structured mesh on a fictitious domain of simple shape</li>
<li>Mortar methods — meshes on subdomain do not mesh</li>
<li>Neumann–Dirichlet method — combines Neumann problem on one subdomain with Dirichlet problem on other subdomain</li>
<li>Neumann–Neumann methods — domain decomposition methods that use Neumann problems on the subdomains</li>
<li>Poincaré–Steklov operator — maps tangential electric field onto the equivalent electric current</li>
<li>Schur complement method — early and basic method on subdomains that do not overlap</li>
<li>Schwarz alternating method — early and basic method on subdomains that overlap</li>
</ul>
<h3>Grids and meshes</h3>
<ul>
<li>Grid classification / Types of mesh:
<ul>
<li>Polygon mesh — consists of polygons in 2D or 3D</li>
<li>Triangle mesh — consists of triangles in 2D or 3D
<ul>
<li>Triangulation (geometry) — subdivision of given region in triangles, or higher-dimensional analogue</li>
<li>Nonobtuse mesh — mesh in which all angles are less than or equal to 90°</li>
<li>Point set triangulation — triangle mesh such that given set of point are all a vertex of a triangle</li>
<li>Polygon triangulation — triangle mesh inside a polygon</li>
<li>Delaunay triangulation — triangulation such that no vertex is inside the circumcentre of a triangle</li>
<li>Constrained Delaunay triangulation — generalization of the Delaunay triangulation that forces certain required segments into the triangulation</li>
<li>Pitteway triangulation — for any point, triangle containing it has nearest neighbour of the point as a vertex</li>
<li>Minimum-weight triangulation — triangulation of minimum total edge length</li>
<li>Kinetic triangulation — a triangulation that moves over time</li>
<li>Triangulated irregular network</li>
<li>Quasi-triangulation — subdivision into simplices, where vertiсes are not points but arbitrary sloped line segments</li>
</ul>
</li>
<li>Volume mesh — consists of three-dimensional shapes</li>
<li>Regular grid — consists of congruent parallelograms, or higher-dimensional analogue</li>
<li>Unstructured grid</li>
<li>Geodesic grid — isotropic grid on a sphere</li>
</ul>
</li>
<li>Mesh generation
<ul>
<li>Image-based meshing — automatic procedure of generating meshes from 3D image data</li>
<li>Marching cubes — extracts a polygon mesh from a scalar field</li>
<li>Parallel mesh generation</li>
<li>Ruppert's algorithm — creates quality Delauney triangularization from piecewise linear data</li>
</ul>
</li>
<li>Subdivisions:</li>
<li>Apollonian network — undirected graph formed by recursively subdividing a triangle</li>
<li>Barycentric subdivision — standard way of dividing arbitrary convex polygons into triangles, or the higher-dimensional analogue</li>
<li>Improving an existing mesh:
<ul>
<li>Chew's second algorithm — improves Delauney triangularization by refining poor-quality triangles</li>
<li>Laplacian smoothing — improves polynomial meshes by moving the vertices</li>
</ul>
</li>
<li>Jump-and-Walk algorithm — for finding triangle in a mesh containing a given point</li>
<li>Spatial twist continuum — dual representation of a mesh consisting of hexahedra</li>
<li>Pseudotriangle — simply connected region between any three mutually tangent convex sets</li>
<li>Simplicial complex — all vertices, line segments, triangles, tetrahedra, …, making up a mesh</li>
</ul>
<ul>
<li>Polygon mesh — consists of polygons in 2D or 3D</li>
<li>Triangle mesh — consists of triangles in 2D or 3D
<ul>
<li>Triangulation (geometry) — subdivision of given region in triangles, or higher-dimensional analogue</li>
<li>Nonobtuse mesh — mesh in which all angles are less than or equal to 90°</li>
<li>Point set triangulation — triangle mesh such that given set of point are all a vertex of a triangle</li>
<li>Polygon triangulation — triangle mesh inside a polygon</li>
<li>Delaunay triangulation — triangulation such that no vertex is inside the circumcentre of a triangle</li>
<li>Constrained Delaunay triangulation — generalization of the Delaunay triangulation that forces certain required segments into the triangulation</li>
<li>Pitteway triangulation — for any point, triangle containing it has nearest neighbour of the point as a vertex</li>
<li>Minimum-weight triangulation — triangulation of minimum total edge length</li>
<li>Kinetic triangulation — a triangulation that moves over time</li>
<li>Triangulated irregular network</li>
<li>Quasi-triangulation — subdivision into simplices, where vertiсes are not points but arbitrary sloped line segments</li>
</ul>
</li>
<li>Volume mesh — consists of three-dimensional shapes</li>
<li>Regular grid — consists of congruent parallelograms, or higher-dimensional analogue</li>
<li>Unstructured grid</li>
<li>Geodesic grid — isotropic grid on a sphere</li>
</ul>
<ul>
<li>Triangulation (geometry) — subdivision of given region in triangles, or higher-dimensional analogue</li>
<li>Nonobtuse mesh — mesh in which all angles are less than or equal to 90°</li>
<li>Point set triangulation — triangle mesh such that given set of point are all a vertex of a triangle</li>
<li>Polygon triangulation — triangle mesh inside a polygon</li>
<li>Delaunay triangulation — triangulation such that no vertex is inside the circumcentre of a triangle</li>
<li>Constrained Delaunay triangulation — generalization of the Delaunay triangulation that forces certain required segments into the triangulation</li>
<li>Pitteway triangulation — for any point, triangle containing it has nearest neighbour of the point as a vertex</li>
<li>Minimum-weight triangulation — triangulation of minimum total edge length</li>
<li>Kinetic triangulation — a triangulation that moves over time</li>
<li>Triangulated irregular network</li>
<li>Quasi-triangulation — subdivision into simplices, where vertiсes are not points but arbitrary sloped line segments</li>
</ul>
<ul>
<li>Image-based meshing — automatic procedure of generating meshes from 3D image data</li>
<li>Marching cubes — extracts a polygon mesh from a scalar field</li>
<li>Parallel mesh generation</li>
<li>Ruppert's algorithm — creates quality Delauney triangularization from piecewise linear data</li>
</ul>
<ul>
<li>Chew's second algorithm — improves Delauney triangularization by refining poor-quality triangles</li>
<li>Laplacian smoothing — improves polynomial meshes by moving the vertices</li>
</ul>
<h3>Analysis</h3>
<ul>
<li>Lax equivalence theorem — a consistent method is convergent if and only if it is stable</li>
<li>Courant–Friedrichs–Lewy condition — stability condition for hyperbolic PDEs</li>
<li>Von Neumann stability analysis — all Fourier components of the error should be stable</li>
<li>Numerical diffusion — diffusion introduced by the numerical method, above to that which is naturally present
<ul>
<li>False diffusion</li>
</ul>
</li>
<li>Numerical resistivity — the same, with resistivity instead of diffusion</li>
<li>Weak formulation — a functional-analytic reformulation of the PDE necessary for some methods</li>
<li>Total variation diminishing — property of schemes that do not introduce spurious oscillations</li>
<li>Godunov's theorem — linear monotone schemes can only be of first order</li>
<li>Motz's problem — benchmark problem for singularity problems</li>
</ul>
<ul>
<li>False diffusion</li>
</ul>
<h2>Monte Carlo method</h2>
<ul>
<li>Variants of the Monte Carlo method:
<ul>
<li>Direct simulation Monte Carlo</li>
<li>Quasi-Monte Carlo method</li>
<li>Markov chain Monte Carlo
<ul>
<li>Metropolis–Hastings algorithm
<ul>
<li>Multiple-try Metropolis — modification which allows larger step sizes</li>
<li>Wang and Landau algorithm — extension of Metropolis Monte Carlo</li>
<li>Equation of State Calculations by Fast Computing Machines — 1953 article proposing the Metropolis Monte Carlo algorithm</li>
<li>Multicanonical ensemble — sampling technique that uses Metropolis–Hastings to compute integrals</li>
</ul>
</li>
<li>Gibbs sampling</li>
<li>Coupling from the past</li>
<li>Reversible-jump Markov chain Monte Carlo</li>
</ul>
</li>
<li>Dynamic Monte Carlo method
<ul>
<li>Kinetic Monte Carlo</li>
<li>Gillespie algorithm</li>
</ul>
</li>
<li>Particle filter
<ul>
<li>Auxiliary particle filter</li>
</ul>
</li>
<li>Reverse Monte Carlo</li>
<li>Demon algorithm</li>
</ul>
</li>
<li>Pseudo-random number sampling
<ul>
<li>Inverse transform sampling — general and straightforward method but computationally expensive</li>
<li>Rejection sampling — sample from a simpler distribution but reject some of the samples
<ul>
<li>Ziggurat algorithm — uses a pre-computed table covering the probability distribution with rectangular segments</li>
</ul>
</li>
<li>For sampling from a normal distribution:
<ul>
<li>Box–Muller transform</li>
<li>Marsaglia polar method</li>
</ul>
</li>
<li>Convolution random number generator — generates a random variable as a sum of other random variables</li>
<li>Indexed search</li>
</ul>
</li>
<li>Variance reduction techniques:
<ul>
<li>Antithetic variates</li>
<li>Control variates</li>
<li>Importance sampling</li>
<li>Stratified sampling</li>
<li>VEGAS algorithm</li>
</ul>
</li>
<li>Low-discrepancy sequence
<ul>
<li>Constructions of low-discrepancy sequences</li>
</ul>
</li>
<li>Event generator</li>
<li>Parallel tempering</li>
<li>Umbrella sampling — improves sampling in physical systems with significant energy barriers</li>
<li>Hybrid Monte Carlo</li>
<li>Ensemble Kalman filter — recursive filter suitable for problems with a large number of variables</li>
<li>Transition path sampling</li>
<li>Applications:
<ul>
<li>Ensemble forecasting — produce multiple numerical predictions from slightly initial conditions or parameters</li>
<li>Bond fluctuation model — for simulating the conformation and dynamics of polymer systems</li>
<li>Iterated filtering</li>
<li>Metropolis light transport</li>
<li>Monte Carlo localization — estimates the position and orientation of a robot</li>
<li>Monte Carlo methods for electron transport</li>
<li>Monte Carlo method for photon transport</li>
<li>Monte Carlo methods in finance
<ul>
<li>Monte Carlo methods for option pricing</li>
<li>Quasi-Monte Carlo methods in finance</li>
</ul>
</li>
<li>Monte Carlo molecular modeling
<ul>
<li>Path integral molecular dynamics — incorporates Feynman path integrals</li>
</ul>
</li>
<li>Quantum Monte Carlo
<ul>
<li>Diffusion Monte Carlo — uses a Green function to solve the Schrödinger equation</li>
<li>Gaussian quantum Monte Carlo</li>
<li>Path integral Monte Carlo</li>
<li>Reptation Monte Carlo</li>
<li>Variational Monte Carlo</li>
</ul>
</li>
<li>Methods for simulating the Ising model:
<ul>
<li>Swendsen–Wang algorithm — entire sample is divided into equal-spin clusters</li>
<li>Wolff algorithm — improvement of the Swendsen–Wang algorithm</li>
<li>Metropolis–Hastings algorithm</li>
</ul>
</li>
<li>Auxiliary field Monte Carlo — computes averages of operators in many-body quantum mechanical problems</li>
<li>Cross-entropy method — for multi-extremal optimization and importance sampling</li>
</ul>
</li>
<li>Also see the list of statistics topics</li>
</ul>
<ul>
<li>Direct simulation Monte Carlo</li>
<li>Quasi-Monte Carlo method</li>
<li>Markov chain Monte Carlo
<ul>
<li>Metropolis–Hastings algorithm
<ul>
<li>Multiple-try Metropolis — modification which allows larger step sizes</li>
<li>Wang and Landau algorithm — extension of Metropolis Monte Carlo</li>
<li>Equation of State Calculations by Fast Computing Machines — 1953 article proposing the Metropolis Monte Carlo algorithm</li>
<li>Multicanonical ensemble — sampling technique that uses Metropolis–Hastings to compute integrals</li>
</ul>
</li>
<li>Gibbs sampling</li>
<li>Coupling from the past</li>
<li>Reversible-jump Markov chain Monte Carlo</li>
</ul>
</li>
<li>Dynamic Monte Carlo method
<ul>
<li>Kinetic Monte Carlo</li>
<li>Gillespie algorithm</li>
</ul>
</li>
<li>Particle filter
<ul>
<li>Auxiliary particle filter</li>
</ul>
</li>
<li>Reverse Monte Carlo</li>
<li>Demon algorithm</li>
</ul>
<ul>
<li>Metropolis–Hastings algorithm
<ul>
<li>Multiple-try Metropolis — modification which allows larger step sizes</li>
<li>Wang and Landau algorithm — extension of Metropolis Monte Carlo</li>
<li>Equation of State Calculations by Fast Computing Machines — 1953 article proposing the Metropolis Monte Carlo algorithm</li>
<li>Multicanonical ensemble — sampling technique that uses Metropolis–Hastings to compute integrals</li>
</ul>
</li>
<li>Gibbs sampling</li>
<li>Coupling from the past</li>
<li>Reversible-jump Markov chain Monte Carlo</li>
</ul>
<ul>
<li>Multiple-try Metropolis — modification which allows larger step sizes</li>
<li>Wang and Landau algorithm — extension of Metropolis Monte Carlo</li>
<li>Equation of State Calculations by Fast Computing Machines — 1953 article proposing the Metropolis Monte Carlo algorithm</li>
<li>Multicanonical ensemble — sampling technique that uses Metropolis–Hastings to compute integrals</li>
</ul>
<ul>
<li>Kinetic Monte Carlo</li>
<li>Gillespie algorithm</li>
</ul>
<ul>
<li>Auxiliary particle filter</li>
</ul>
<ul>
<li>Inverse transform sampling — general and straightforward method but computationally expensive</li>
<li>Rejection sampling — sample from a simpler distribution but reject some of the samples
<ul>
<li>Ziggurat algorithm — uses a pre-computed table covering the probability distribution with rectangular segments</li>
</ul>
</li>
<li>For sampling from a normal distribution:
<ul>
<li>Box–Muller transform</li>
<li>Marsaglia polar method</li>
</ul>
</li>
<li>Convolution random number generator — generates a random variable as a sum of other random variables</li>
<li>Indexed search</li>
</ul>
<ul>
<li>Ziggurat algorithm — uses a pre-computed table covering the probability distribution with rectangular segments</li>
</ul>
<ul>
<li>Box–Muller transform</li>
<li>Marsaglia polar method</li>
</ul>
<ul>
<li>Antithetic variates</li>
<li>Control variates</li>
<li>Importance sampling</li>
<li>Stratified sampling</li>
<li>VEGAS algorithm</li>
</ul>
<ul>
<li>Constructions of low-discrepancy sequences</li>
</ul>
<ul>
<li>Ensemble forecasting — produce multiple numerical predictions from slightly initial conditions or parameters</li>
<li>Bond fluctuation model — for simulating the conformation and dynamics of polymer systems</li>
<li>Iterated filtering</li>
<li>Metropolis light transport</li>
<li>Monte Carlo localization — estimates the position and orientation of a robot</li>
<li>Monte Carlo methods for electron transport</li>
<li>Monte Carlo method for photon transport</li>
<li>Monte Carlo methods in finance
<ul>
<li>Monte Carlo methods for option pricing</li>
<li>Quasi-Monte Carlo methods in finance</li>
</ul>
</li>
<li>Monte Carlo molecular modeling
<ul>
<li>Path integral molecular dynamics — incorporates Feynman path integrals</li>
</ul>
</li>
<li>Quantum Monte Carlo
<ul>
<li>Diffusion Monte Carlo — uses a Green function to solve the Schrödinger equation</li>
<li>Gaussian quantum Monte Carlo</li>
<li>Path integral Monte Carlo</li>
<li>Reptation Monte Carlo</li>
<li>Variational Monte Carlo</li>
</ul>
</li>
<li>Methods for simulating the Ising model:
<ul>
<li>Swendsen–Wang algorithm — entire sample is divided into equal-spin clusters</li>
<li>Wolff algorithm — improvement of the Swendsen–Wang algorithm</li>
<li>Metropolis–Hastings algorithm</li>
</ul>
</li>
<li>Auxiliary field Monte Carlo — computes averages of operators in many-body quantum mechanical problems</li>
<li>Cross-entropy method — for multi-extremal optimization and importance sampling</li>
</ul>
<ul>
<li>Monte Carlo methods for option pricing</li>
<li>Quasi-Monte Carlo methods in finance</li>
</ul>
<ul>
<li>Path integral molecular dynamics — incorporates Feynman path integrals</li>
</ul>
<ul>
<li>Diffusion Monte Carlo — uses a Green function to solve the Schrödinger equation</li>
<li>Gaussian quantum Monte Carlo</li>
<li>Path integral Monte Carlo</li>
<li>Reptation Monte Carlo</li>
<li>Variational Monte Carlo</li>
</ul>
<ul>
<li>Swendsen–Wang algorithm — entire sample is divided into equal-spin clusters</li>
<li>Wolff algorithm — improvement of the Swendsen–Wang algorithm</li>
<li>Metropolis–Hastings algorithm</li>
</ul>
<h2>Applications</h2>
<ul>
<li>Computational physics
<ul>
<li>Computational electromagnetics</li>
<li>Computational fluid dynamics (CFD)
<ul>
<li>Numerical methods in fluid mechanics</li>
<li>Large eddy simulation</li>
<li>Smoothed-particle hydrodynamics</li>
<li>Aeroacoustic analogy — used in numerical aeroacoustics to reduce sound sources to simple emitter types</li>
<li>Stochastic Eulerian Lagrangian method — uses Eulerian description for fluids and Lagrangian for structures</li>
<li>Explicit algebraic stress model</li>
</ul>
</li>
<li>Computational magnetohydrodynamics (CMHD) — studies electrically conducting fluids</li>
<li>Climate model</li>
<li>Numerical weather prediction
<ul>
<li>Geodesic grid</li>
</ul>
</li>
<li>Celestial mechanics
<ul>
<li>Numerical model of the Solar System</li>
</ul>
</li>
<li>Quantum jump method — used for simulating open quantum systems, operates on wave function</li>
<li>Dynamic Design Analysis Method (DDAM) — for evaluating effect of underwater explosions on equipment</li>
</ul>
</li>
<li>Computational chemistry
<ul>
<li>Cell lists</li>
<li>Coupled cluster</li>
<li>Density functional theory</li>
<li>DIIS — direct inversion in (or of) the iterative subspace</li>
</ul>
</li>
<li>Computational sociology</li>
<li>Computational statistics</li>
</ul>
<ul>
<li>Computational electromagnetics</li>
<li>Computational fluid dynamics (CFD)
<ul>
<li>Numerical methods in fluid mechanics</li>
<li>Large eddy simulation</li>
<li>Smoothed-particle hydrodynamics</li>
<li>Aeroacoustic analogy — used in numerical aeroacoustics to reduce sound sources to simple emitter types</li>
<li>Stochastic Eulerian Lagrangian method — uses Eulerian description for fluids and Lagrangian for structures</li>
<li>Explicit algebraic stress model</li>
</ul>
</li>
<li>Computational magnetohydrodynamics (CMHD) — studies electrically conducting fluids</li>
<li>Climate model</li>
<li>Numerical weather prediction
<ul>
<li>Geodesic grid</li>
</ul>
</li>
<li>Celestial mechanics
<ul>
<li>Numerical model of the Solar System</li>
</ul>
</li>
<li>Quantum jump method — used for simulating open quantum systems, operates on wave function</li>
<li>Dynamic Design Analysis Method (DDAM) — for evaluating effect of underwater explosions on equipment</li>
</ul>
<ul>
<li>Numerical methods in fluid mechanics</li>
<li>Large eddy simulation</li>
<li>Smoothed-particle hydrodynamics</li>
<li>Aeroacoustic analogy — used in numerical aeroacoustics to reduce sound sources to simple emitter types</li>
<li>Stochastic Eulerian Lagrangian method — uses Eulerian description for fluids and Lagrangian for structures</li>
<li>Explicit algebraic stress model</li>
</ul>
<ul>
<li>Geodesic grid</li>
</ul>
<ul>
<li>Numerical model of the Solar System</li>
</ul>
<ul>
<li>Cell lists</li>
<li>Coupled cluster</li>
<li>Density functional theory</li>
<li>DIIS — direct inversion in (or of) the iterative subspace</li>
</ul>
<h2>Software</h2>
<p>For a large list of software, see the list of numerical analysis software.</p>
</body>
</html>