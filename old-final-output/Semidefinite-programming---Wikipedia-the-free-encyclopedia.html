<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Semidefinite-programming---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Semidefinite programming</h1>
<p><b>Semidefinite programming</b> (<b>SDP</b>) is a subfield of convex optimization concerned with the optimization of a linear objective function (that is, a function to be maximized or minimized) over the intersection of the cone of positive semidefinite matrices with an affine space, i.e., a spectrahedron.</p>
<p>Semidefinite programming is a relatively new field of optimization which is of growing interest for several reasons. Many practical problems in operations research and combinatorial optimization can be modeled or approximated as semidefinite programming problems. In automatic control theory, SDP's are used in the context of linear matrix inequalities. SDPs are in fact a special case of cone programming and can be efficiently solved by interior point methods. All linear programs can be expressed as SDPs, and via hierarchies of SDPs the solutions of polynomial optimization problems can be approximated. Semidefinite programming has been used in the optimization of complex systems. In recent years, some quantum query complexity problems have been formulated in term of semidefinite programs.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Motivation and definition
<ul>
<li>1.1 Initial motivation</li>
<li>1.2 Equivalent formulations</li>
</ul>
</li>
<li>2 Duality theory
<ul>
<li>2.1 Definitions</li>
<li>2.2 Weak duality</li>
<li>2.3 Strong duality</li>
</ul>
</li>
<li>3 Examples
<ul>
<li>3.1 Example 1</li>
<li>3.2 Example 2</li>
<li>3.3 Example 3 (Goemans-Williamson MAX CUT approximation algorithm)</li>
</ul>
</li>
<li>4 Algorithms
<ul>
<li>4.1 Interior point methods</li>
<li>4.2 Bundle method</li>
<li>4.3 Other</li>
</ul>
</li>
<li>5 Software</li>
<li>6 Applications</li>
<li>7 References</li>
<li>8 External links</li>
</ul>
<ul>
<li>1.1 Initial motivation</li>
<li>1.2 Equivalent formulations</li>
</ul>
<ul>
<li>2.1 Definitions</li>
<li>2.2 Weak duality</li>
<li>2.3 Strong duality</li>
</ul>
<ul>
<li>3.1 Example 1</li>
<li>3.2 Example 2</li>
<li>3.3 Example 3 (Goemans-Williamson MAX CUT approximation algorithm)</li>
</ul>
<ul>
<li>4.1 Interior point methods</li>
<li>4.2 Bundle method</li>
<li>4.3 Other</li>
</ul>
<p></p>
<h2>Motivation and definition</h2>
<h3>Initial motivation</h3>
<p>A linear programming problem is one in which we wish to maximize or minimize a linear objective function of real variables over a polytope. In semidefinite programming, we instead use real-valued vectors and are allowed to take the dot product of vectors; nonnegativity constraints on real variables in LP are replaced by semidefiniteness constraints on matrix variables in SDP. Specifically, a general semidefinite programming problem can be defined as any mathematical programming problem of the form</p>
<h3>Equivalent formulations</h3>
<p>An <img class="mwe-math-fallback-image-inline tex" alt="n \times n" src="//upload.wikimedia.org/math/6/0/7/607acaa73c762411b20745149a11e90b.png"> matrix <img class="mwe-math-fallback-image-inline tex" alt="M" src="//upload.wikimedia.org/math/6/9/6/69691c7bdcc3ce6d5d8a1361f22d04ac.png"> is said to be positive semidefinite if it is the gramian matrix of some vectors (i.e. if there exist vectors <img class="mwe-math-fallback-image-inline tex" alt="x^1, \ldots, x^n" src="//upload.wikimedia.org/math/4/0/f/40f5724a8aea802b64d9ababa61fbec1.png"> such that <img class="mwe-math-fallback-image-inline tex" alt="m_{i,j}=x^i \cdot x^j" src="//upload.wikimedia.org/math/d/4/9/d49bdc7cc5aee6049e326ee409b6a600.png"> for all <img class="mwe-math-fallback-image-inline tex" alt="i,j" src="//upload.wikimedia.org/math/e/e/8/ee813f0ede8664a8049b1b6720f03b60.png">). If this is the case, we denote this as <img class="mwe-math-fallback-image-inline tex" alt="M \succeq 0" src="//upload.wikimedia.org/math/4/6/9/4699fd6ecbf65db5a6ec0ab77959c82a.png">. Note that there are several other equivalent definitions of being positive semidefinite, for example, positive semidefinite matrices have only non-negative eigenvalues and have a positive definite square root.</p>
<p>Denote by <img class="mwe-math-fallback-image-inline tex" alt="\mathbb{S}^n" src="//upload.wikimedia.org/math/6/a/f/6af6cea7f3e0e5b952eb7db71fda65a8.png"> the space of all <img class="mwe-math-fallback-image-inline tex" alt="n \times n" src="//upload.wikimedia.org/math/6/0/7/607acaa73c762411b20745149a11e90b.png"> real symmetric matrices. The space is equipped with the inner product (where <img class="mwe-math-fallback-image-inline tex" alt="{\rm tr}" src="//upload.wikimedia.org/math/2/a/a/2aaacd3b83e40ccfcbcdfb4982026598.png"> denotes the trace) <img class="mwe-math-fallback-image-inline tex" alt="
  \langle A,B\rangle_{\mathbb{S}^n} = {\rm tr}(A^T B) = \sum_{i=1,j=1}^n
  A_{ij}B_{ij}.
" src="//upload.wikimedia.org/math/3/6/5/36502f8844159cb36dc832ae72173eaa.png"></p>
<p>We can rewrite the mathematical program given in the previous section equivalently as</p>
<p>where entry <img class="mwe-math-fallback-image-inline tex" alt="i,j" src="//upload.wikimedia.org/math/e/e/8/ee813f0ede8664a8049b1b6720f03b60.png"> in <img class="mwe-math-fallback-image-inline tex" alt="C" src="//upload.wikimedia.org/math/0/d/6/0d61f8370cad1d412f80b84d143e1257.png"> is given by <img class="mwe-math-fallback-image-inline tex" alt="c_{i,j}" src="//upload.wikimedia.org/math/9/2/e/92e822addd4c1d7ce4e6ca59c1504af4.png"> from the previous section and <img class="mwe-math-fallback-image-inline tex" alt="A_k" src="//upload.wikimedia.org/math/d/9/3/d93f57d24bbe3378bf1116d752877d4f.png"> is an <img class="mwe-math-fallback-image-inline tex" alt="n \times n" src="//upload.wikimedia.org/math/6/0/7/607acaa73c762411b20745149a11e90b.png"> matrix having <img class="mwe-math-fallback-image-inline tex" alt="i,j" src="//upload.wikimedia.org/math/e/e/8/ee813f0ede8664a8049b1b6720f03b60.png">th entry <img class="mwe-math-fallback-image-inline tex" alt="a_{i,j,k}" src="//upload.wikimedia.org/math/c/d/f/cdf7fc3eb2fe92d657a6e5873e4b3fe0.png"> from the previous section.</p>
<p>Note that if we add slack variables appropriately, this SDP can be converted to one of the form</p>
<p>For convenience, an SDP may be specified in a slightly different, but equivalent form. For example, linear expressions involving nonnegative scalar variables may be added to the program specification. This remains an SDP because each variable can be incorporated into the matrix <img class="mwe-math-fallback-image-inline tex" alt="X" src="//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png"> as a diagonal entry (<img class="mwe-math-fallback-image-inline tex" alt="X_{ii}" src="//upload.wikimedia.org/math/2/7/0/27062a1a56818ac8fb0141f4e1bc3292.png"> for some <img class="mwe-math-fallback-image-inline tex" alt="i" src="//upload.wikimedia.org/math/8/6/5/865c0c0b4ab0e063e5caa3387c1a8741.png">). To ensure that <img class="mwe-math-fallback-image-inline tex" alt="X_{ii} \geq 0" src="//upload.wikimedia.org/math/3/5/2/352e325ac2e1e3571943f314b56cfa2f.png">, constraints <img class="mwe-math-fallback-image-inline tex" alt="X_{ij} = 0" src="//upload.wikimedia.org/math/b/3/9/b3911a388d24085c221f7fd9af9739a3.png"> can be added for all <img class="mwe-math-fallback-image-inline tex" alt="j \neq i" src="//upload.wikimedia.org/math/c/c/f/ccf5f455f5de788b5d5eeaf339059de1.png">. As another example, note that for any positive semidefinite matrix <img class="mwe-math-fallback-image-inline tex" alt="X" src="//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png">, there exists a set of vectors <img class="mwe-math-fallback-image-inline tex" alt="\{ v_i \}" src="//upload.wikimedia.org/math/0/3/e/03e8c609dbb101a8d92839b7f35e122c.png"> such that the <img class="mwe-math-fallback-image-inline tex" alt="i" src="//upload.wikimedia.org/math/8/6/5/865c0c0b4ab0e063e5caa3387c1a8741.png">, <img class="mwe-math-fallback-image-inline tex" alt="j" src="//upload.wikimedia.org/math/3/6/3/363b122c528f54df4a0446b6bab05515.png"> entry of <img class="mwe-math-fallback-image-inline tex" alt="X" src="//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png"> is <img class="mwe-math-fallback-image-inline tex" alt="X_{ij} = (v_i, v_j)" src="//upload.wikimedia.org/math/e/c/2/ec25ce17c06970008e2d731a928d033e.png"> the scalar product of <img class="mwe-math-fallback-image-inline tex" alt="v_i" src="//upload.wikimedia.org/math/f/0/e/f0e66f55342ef85ba8be3415dd92d8e2.png"> and <img class="mwe-math-fallback-image-inline tex" alt="v_j" src="//upload.wikimedia.org/math/2/7/5/275dc431f9270317c68595220d6b8730.png">. Therefore, SDPs are often formulated in terms of linear expressions on scalar products of vectors. Given the solution to the SDP in the standard form, the vectors <img class="mwe-math-fallback-image-inline tex" alt="\{ v_i \}" src="//upload.wikimedia.org/math/0/3/e/03e8c609dbb101a8d92839b7f35e122c.png"> can be recovered in <img class="mwe-math-fallback-image-inline tex" alt="O(n^3)" src="//upload.wikimedia.org/math/6/8/0/6809c59370e21b3e6e8fd117442fd377.png"> time (e.g., by using an incomplete Cholesky decomposition of X).</p>
<h2>Duality theory</h2>
<h3>Definitions</h3>
<p>Analogously to linear programming, given a general SDP of the form</p>
<p>(the primal problem or P-SDP), we define the <i>dual</i> semidefinite program (D-SDP) as</p>
<p>where for any two matrices <img class="mwe-math-fallback-image-inline tex" alt="P" src="//upload.wikimedia.org/math/4/4/c/44c29edb103a2872f519ad0c9a0fdaaa.png"> and <img class="mwe-math-fallback-image-inline tex" alt="Q" src="//upload.wikimedia.org/math/f/0/9/f09564c9ca56850d4cd6b3319e541aee.png">, <img class="mwe-math-fallback-image-inline tex" alt="P \succeq Q" src="//upload.wikimedia.org/math/3/3/b/33bbee7dea13ee0d4a311a3ee2c7d4eb.png"> means <img class="mwe-math-fallback-image-inline tex" alt="P-Q \succeq 0" src="//upload.wikimedia.org/math/9/c/e/9ce23d30aa47d387ebb335c488b3d368.png">.</p>
<h3>Weak duality</h3>
<p>The weak duality theorem states that the value of the primal SDP is at least the value of the dual SDP. Therefore, any feasible solution to the dual SDP lower-bounds the primal SDP value, and conversely, any feasible solution to the primal SDP upper-bounds the dual SDP value. This is because</p>
<p>where the last inequality is because both matrices are positive semidefinite, and the result of this function is sometimes referred to as duality gap.</p>
<h3>Strong duality</h3>
<p>Under a condition known as Slater's condition, the value of the primal and dual SDPs are equal. This is known as strong duality. Unlike for linear programs, however, not every SDP satisfies strong duality; in general, the value of the dual SDP may lie strictly below the value of the primal.</p>
<p>(i) Suppose the primal problem (P-SDP) is bounded below and strictly feasible (i.e., there exists <img class="mwe-math-fallback-image-inline tex" alt="X_0\in\mathbb{S}^n, X_0\succ 0" src="//upload.wikimedia.org/math/d/a/7/da7289749ddef76fe96004c07c14e3d9.png"> such that <img class="mwe-math-fallback-image-inline tex" alt="\langle
A_i,X_0\rangle_{\mathbb{S}^n} = b_i" src="//upload.wikimedia.org/math/a/b/9/ab943489cb9b3ca72270800575b52d2e.png">, <img class="mwe-math-fallback-image-inline tex" alt="i=1,\ldots,m" src="//upload.wikimedia.org/math/e/9/f/e9fc2bada9d8a674f790a0c58e5312b7.png">). Then there is an optimal solution <img class="mwe-math-fallback-image-inline tex" alt="y^*" src="//upload.wikimedia.org/math/8/9/b/89b3a69e7c4dd4ee4e7aaf09191ac8e1.png"> to (D-SDP) and</p>
<p>(ii) Suppose the dual problem (D-SDP) is bounded above and strictly feasible (i.e., <img class="mwe-math-fallback-image-inline tex" alt="\sum_{i=1}^m (y_0)_i A_i
\prec C" src="//upload.wikimedia.org/math/5/7/8/578486f24b6366edcfcc854573887c8a.png"> for some <img class="mwe-math-fallback-image-inline tex" alt="y_0\in\R^m" src="//upload.wikimedia.org/math/f/a/0/fa0ce99d887c915be2090f917e9016bb.png">). Then there is an optimal solution <img class="mwe-math-fallback-image-inline tex" alt="X^*" src="//upload.wikimedia.org/math/f/2/d/f2d6c5f9b59bae5bc8a1ff91f8ae7ded.png"> to (P-SDP) and the equality from (i) holds.</p>
<h2>Examples</h2>
<h3>Example 1</h3>
<p>Consider three random variables <img class="mwe-math-fallback-image-inline tex" alt="A" src="//upload.wikimedia.org/math/7/f/c/7fc56270e7a70fa81a5935b72eacbe29.png">, <img class="mwe-math-fallback-image-inline tex" alt="B" src="//upload.wikimedia.org/math/9/d/5/9d5ed678fe57bcca610140957afab571.png">, and <img class="mwe-math-fallback-image-inline tex" alt="C" src="//upload.wikimedia.org/math/0/d/6/0d61f8370cad1d412f80b84d143e1257.png">. By definition, their correlation coefficients <img class="mwe-math-fallback-image-inline tex" alt="\rho_{AB}, \ \rho_{AC}, \rho_{BC} " src="//upload.wikimedia.org/math/d/e/f/def6df310c1afd3d9915f400e8ed7c83.png"> are valid if and only if</p>
<p>Suppose that we know from some prior knowledge (empirical results of an experiment, for example) that <img class="mwe-math-fallback-image-inline tex" alt="-0.2 \leq \rho_{AB} \leq -0.1" src="//upload.wikimedia.org/math/7/d/9/7d998af280a3e97c9343dba52e52cc93.png"> and <img class="mwe-math-fallback-image-inline tex" alt="0.4 \leq \rho_{BC} \leq 0.5" src="//upload.wikimedia.org/math/1/e/0/1e04db601379e2d5b3a026dd587587e4.png">. The problem of determining the smallest and largest values that <img class="mwe-math-fallback-image-inline tex" alt="\rho_{AC} \ " src="//upload.wikimedia.org/math/a/a/b/aaba1aef36163cf425751afc5cd04aaf.png"> can take is given by:</p>
<p>we set <img class="mwe-math-fallback-image-inline tex" alt="\rho_{AB} = x_{12}, \ \rho_{AC} = x_{13}, \ \rho_{BC} = x_{23} " src="//upload.wikimedia.org/math/1/1/9/1191b244ef613ec7d85f8fcc7b6c3d37.png"> to obtain the answer. This can be formulated by an SDP. We handle the inequality constraints by augmenting the variable matrix and introducing slack variables, for example</p>
<p><img class="mwe-math-fallback-image-inline tex" alt="\mathrm{tr}\left(\left(\begin{array}{cccccc}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right)\cdot\left(\begin{array}{cccccc}
1 &amp; x_{12} &amp; x_{13} &amp; 0 &amp; 0 &amp; 0\\
x_{12} &amp; 1 &amp; x_{23} &amp; 0 &amp; 0 &amp; 0\\
x_{13} &amp; x_{23} &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; s_{1} &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; s_{2} &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; s_{3}\end{array}\right)\right)=x_{12} + s_{1}=-0.1" src="//upload.wikimedia.org/math/2/4/d/24d1326cfd0440119334e712dc3e8f89.png"></p>
<p>Solving this SDP gives the minimum and maximum values of <img class="mwe-math-fallback-image-inline tex" alt="\rho_{AC} = x_{13} \ " src="//upload.wikimedia.org/math/c/8/0/c80bdb0f4479ca99a97b1e2bca106799.png"> as <img class="mwe-math-fallback-image-inline tex" alt="-0.978" src="//upload.wikimedia.org/math/a/6/7/a6770d397eeb8ea8b95ddd8c8d1052dc.png"> and <img class="mwe-math-fallback-image-inline tex" alt=" 0.872 " src="//upload.wikimedia.org/math/e/6/b/e6beb5d7465a1f43e959e0731e865cfd.png"> respectively.</p>
<h3>Example 2</h3>
<p>Consider the problem</p>
<p>where we assume that <img class="mwe-math-fallback-image-inline tex" alt="d^Tx&gt;0" src="//upload.wikimedia.org/math/a/3/4/a3497df2adb01d8db618032d44c81800.png"> whenever <img class="mwe-math-fallback-image-inline tex" alt="Ax+b\geq 0" src="//upload.wikimedia.org/math/6/2/4/624041fa0c0db52ea379c4ee713a89f8.png">.</p>
<p>Introducing an auxiliary variable <img class="mwe-math-fallback-image-inline tex" alt="t" src="//upload.wikimedia.org/math/e/3/5/e358efa489f58062f10dd7316b65649e.png"> the problem can be reformulated:</p>
<p>In this formulation, the objective is a linear function of the variables <img class="mwe-math-fallback-image-inline tex" alt="x,t" src="//upload.wikimedia.org/math/2/6/d/26d5cf1de3b457150077701d163bcaca.png">.</p>
<p>The first restriction can be written as</p>
<p>where the matrix <img class="mwe-math-fallback-image-inline tex" alt="\textbf{diag}(Ax+b)" src="//upload.wikimedia.org/math/1/f/4/1f470b7f2d765e018b4bd83a55a9c5da.png"> is the square matrix with values in the diagonal equal to the elements of the vector <img class="mwe-math-fallback-image-inline tex" alt="Ax+b" src="//upload.wikimedia.org/math/7/c/8/7c8a2320ba08ff74e783b46a8513dfa1.png">.</p>
<p>The second restriction can be written as</p>
<p>Defining <img class="mwe-math-fallback-image-inline tex" alt="D" src="//upload.wikimedia.org/math/f/6/2/f623e75af30e62bbd73d6df5b50bb7b5.png"> as follows</p>
<p>We can use the theory of Schur Complements to see that</p>
<p>(Boyd and Vandenberghe, 1996)</p>
<p>The semidefinite program associated with this problem is</p>
<h3>Example 3 (Goemans-Williamson MAX CUT approximation algorithm)</h3>
<p>Semidefinite programs are important tools for developing approximation algorithms for NP-hard maximization problems. The first approximation algorithm based on an SDP is due to Goemans and Williamson (JACM, 1995). They studied the MAX CUT problem: Given a graph <i>G</i> = (<i>V</i>, <i>E</i>), output a partition of the vertices <i>V</i> so as to maximize the number of edges crossing from one side to the other. This problem can be expressed as an integer quadratic program:</p>
<p>Unless P = NP, we cannot solve this maximization problem efficiently. However, Goemans and Williamson observed a general three-step procedure for attacking this sort of problem:</p>
<ol>
<li><i>Relax</i> the integer quadratic program into an SDP.</li>
<li>Solve the SDP (to within an arbitrarily small additive error <img class="mwe-math-fallback-image-inline tex" alt="\epsilon" src="//upload.wikimedia.org/math/c/5/0/c50b9e82e318d4c163e4b1b060f7daf5.png">).</li>
<li><i>Round</i> the SDP solution to obtain an approximate solution to the original integer quadratic program.</li>
</ol>
<p>For MAX CUT, the most natural relaxation is</p>
<p>This is an SDP because the objective function and constraints are all linear functions of vector inner products. Solving the SDP gives a set of unit vectors in <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{R^n}" src="//upload.wikimedia.org/math/5/0/b/50b644a47d4cb9eecb09cf589f3044e8.png">; since the vectors are not required to be collinear, the value of this relaxed program can only be higher than the value of the original quadratic integer program. Finally, a rounding procedure is needed to obtain a partition. Goemans and Williamson simply choose a uniformly random hyperplane through the origin and divide the vertices according to which side of the hyperplane the corresponding vectors lie. Straightforward analysis shows that this procedure achieves an expected <i>approximation ratio</i> (performance guarantee) of 0.87856 - ε. (The expected value of the cut is the sum over edges of the probability that the edge is cut, which is proportional to the angle <img class="mwe-math-fallback-image-inline tex" alt="\cos^{-1}\langle v_{i}, v_{j}\rangle" src="//upload.wikimedia.org/math/1/7/0/170ffad77d715fab45c1cd3f8f57c84b.png"> between the vectors at the endpoints of the edge over <img class="mwe-math-fallback-image-inline tex" alt="\pi" src="//upload.wikimedia.org/math/5/2/2/522359592d78569a9eac16498aa7a087.png">. Comparing this probability to <img class="mwe-math-fallback-image-inline tex" alt="(1-\langle v_{i}, v_{j}\rangle)/{2}" src="//upload.wikimedia.org/math/6/7/a/67a181e397632a962fe0e288bff02cc4.png">, in expectation the ratio is always at least 0.87856.) Assuming the Unique Games Conjecture, it can be shown that this approximation ratio is essentially optimal.</p>
<p>Since the original paper of Goemans and Williamson, SDPs have been applied to develop numerous approximation algorithms. Recently, Prasad Raghavendra has developed a general framework for constraint satisfaction problems based on the Unique Games Conjecture.</p>
<h2>Algorithms</h2>
<p>There are several types of algorithms for solving SDPs. These algorithms output the value of the SDP up to an additive error <img class="mwe-math-fallback-image-inline tex" alt="\epsilon" src="//upload.wikimedia.org/math/c/5/0/c50b9e82e318d4c163e4b1b060f7daf5.png"> in time that is polynomial in the program description size and <img class="mwe-math-fallback-image-inline tex" alt="\log (1/\epsilon)" src="//upload.wikimedia.org/math/4/a/c/4acbab6a3981a472d2f6f3e914966354.png">.</p>
<h3>Interior point methods</h3>
<p>Most codes are based on interior point methods (CSDP, SeDuMi, SDPT3, DSDP, SDPA). Robust and efficient for general linear SDP problems. Restricted by the fact that the algorithms are second-order methods and need to store and factorize a large (and often dense) matrix.</p>
<h3>Bundle method</h3>
<p>The code ConicBundle formulates the SDP problem as a nonsmooth optimization problem and solves it by the Spectral Bundle method of nonsmooth optimization. This approach is very efficient for a special class of linear SDP problems.</p>
<h3>Other</h3>
<p>Algorithms based on augmented Lagrangian method (PENSDP) are similar in behavior to the interior point methods and can be specialized to some very large scale problems. Other algorithms use low-rank information and reformulation of the SDP as a nonlinear programming problem (SPDLR).</p>
<h2>Software</h2>
<p>The following codes are available for SDP:</p>
<ul>
<li>SDPA, C++</li>
<li>CSDP, C</li>
<li>SDPT3, Matlab. Free.</li>
<li>SeDuMi, Matlab. Free.</li>
<li>DSDP</li>
<li>PENSDP, C, Fortran, Matlab. Commercial.</li>
<li>SDPLR, C, Matlab</li>
<li>ConicBundle, C/C++</li>
<li>CVXOPT, python. Free.</li>
</ul>
<p>SeDuMi runs on MATLAB and uses the Self-Dual method for solving general convex optimization problems.</p>
<h2>Applications</h2>
<p>Semidefinite programming has been applied to find approximate solutions to combinatorial optimization problems, such as the solution of the max cut problem with an approximation ratio of 0.87856. SDPs are also used in geometry to determine tensegrity graphs, and arise in control theory as LMIs.</p>
</body>
</html>