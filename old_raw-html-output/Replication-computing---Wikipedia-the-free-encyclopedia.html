<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Replication (computing)</span></h1>
<p><b>Replication</b> in <a href="/wiki/Computing" title="Computing">computing</a> involves sharing information so as to ensure consistency between redundant resources, such as <a href="/wiki/Software" title="Software">software</a> or <a href="/wiki/Computer_hardware" title="Computer hardware">hardware</a> components, to improve reliability, <a href="/wiki/Fault-tolerance" title="Fault-tolerance" class="mw-redirect">fault-tolerance</a>, or accessibility.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Terminology"><span class="tocnumber">1</span> <span class="toctext">Terminology</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Replication_models_in_distributed_systems"><span class="tocnumber">1.1</span> <span class="toctext">Replication models in distributed systems</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-3"><a href="#Database_replication"><span class="tocnumber">2</span> <span class="toctext">Database replication</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Disk_storage_replication"><span class="tocnumber">3</span> <span class="toctext">Disk storage replication</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Implementations"><span class="tocnumber">3.1</span> <span class="toctext">Implementations</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#File-based_replication"><span class="tocnumber">4</span> <span class="toctext">File-based replication</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Capture_with_a_kernel_driver"><span class="tocnumber">4.1</span> <span class="toctext">Capture with a kernel driver</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#Filesystem_journal_replication"><span class="tocnumber">4.1.1</span> <span class="toctext">Filesystem journal replication</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="#Batch_replication"><span class="tocnumber">4.2</span> <span class="toctext">Batch replication</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Distributed_shared_memory_replication"><span class="tocnumber">5</span> <span class="toctext">Distributed shared memory replication</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Primary-backup_and_multi-primary_replication"><span class="tocnumber">6</span> <span class="toctext">Primary-backup and multi-primary replication</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Replication_models_in_distributed_systems"><span class="tocnumber">1.1</span> <span class="toctext">Replication models in distributed systems</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Implementations"><span class="tocnumber">3.1</span> <span class="toctext">Implementations</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Capture_with_a_kernel_driver"><span class="tocnumber">4.1</span> <span class="toctext">Capture with a kernel driver</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#Filesystem_journal_replication"><span class="tocnumber">4.1.1</span> <span class="toctext">Filesystem journal replication</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="#Batch_replication"><span class="tocnumber">4.2</span> <span class="toctext">Batch replication</span></a></li>
</ul>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#Filesystem_journal_replication"><span class="tocnumber">4.1.1</span> <span class="toctext">Filesystem journal replication</span></a></li>
</ul>
<p></p>
<h2><span class="mw-headline" id="Terminology"><span id="MASTER-ELECTION"></span>Terminology</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=1" title="Edit section: Terminology">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>One speaks of:</p>
<ul>
<li><i>data replication</i> if the same data is stored on multiple <a href="/wiki/Data_storage_device" title="Data storage device">storage devices</a>,<sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup></li>
<li><i>computation replication</i> if the same computing task is executed many times.</li>
</ul>
<p>A computational task is typically <i>replicated in space</i>, i.e. executed on separate devices, or it could be <i>replicated in time</i>, if it is executed repeatedly on a single device. Replication in space or in time is often linked to scheduling algorithms <sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup></p>
<p>The access to a replicated entity is typically uniform with access to a single, non-replicated entity. The replication itself should be <a href="/wiki/Transparency_(human-computer_interaction)" title="Transparency (human-computer interaction)" class="mw-redirect">transparent</a> to an external user. Also, in a failure scenario, a <a href="/wiki/Failover" title="Failover">failover</a> of replicas is hidden as much as possible. The latter refers to data replication with respect to <a href="/wiki/Quality_of_service" title="Quality of service">Quality of Service (QoS)</a> aspects.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup></p>
<p>Computer scientists talk about active and passive replication in systems that replicate data or services:</p>
<ul>
<li><i>active replication</i> is performed by processing the same request at every replica.</li>
<li><i>passive replication</i> involves processing each single request on a single replica and then transferring its resultant state to the other replicas.</li>
</ul>
<p>If at any time one master replica is designated to process all the requests, then we are talking about the <i>primary-backup</i> scheme (<i><a href="/wiki/Master-slave_(computers)" title="Master-slave (computers)" class="mw-redirect">master-slave</a></i> scheme) predominant in <a href="/wiki/High-availability_cluster" title="High-availability cluster">high-availability clusters</a>. On the other side, if any replica processes a request and then distributes a new state, then this is a <i>multi-primary</i> scheme (called <i><a href="/wiki/Multi-master_replication" title="Multi-master replication">multi-master</a></i> in the database field). In the multi-primary scheme, some form of <a href="/wiki/Distributed_concurrency_control" title="Distributed concurrency control">distributed concurrency control</a> must be used, such as <a href="/wiki/Distributed_lock_manager" title="Distributed lock manager">distributed lock manager</a>.</p>
<p><a href="/wiki/Load_balancing_(computing)" title="Load balancing (computing)">Load balancing</a> differs from task replication, since it distributes a load of different (not the same) computations across machines, and allows a single computation to be dropped in case of failure. Load balancing, however, sometimes uses data replication (especially multi-master replication) internally, to distribute its data among machines.</p>
<p><a href="/wiki/Backup" title="Backup">Backup</a> differs from replication in that it saves a copy of data unchanged for a long period of time.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2012)">citation needed</span></a></i>]</sup> Replicas, on the other hand, undergo frequent updates and quickly lose any historical state. Replication is one of the oldest and most important topics in the overall area of <a href="/wiki/Distributed_computing" title="Distributed computing">distributed systems</a>.</p>
<p>Whether one replicates data or computation, the objective is to have some group of processes that handle incoming events. If we replicate data, these processes are passive and operate only to maintain the stored data, reply to read requests, and apply updates. When we replicate computation, the usual goal is to provide fault-tolerance. For example, a replicated service might be used to control a telephone switch, with the objective of ensuring that even if the primary controller fails, the backup can take over its functions. But the underlying needs are the same in both cases: by ensuring that the replicas see the same events in equivalent orders, they stay in consistent states and hence any replica can respond to queries.</p>
<h3><span class="mw-headline" id="Replication_models_in_distributed_systems">Replication models in distributed systems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=2" title="Edit section: Replication models in distributed systems">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A number of widely cited models exist for data replication, each having its own properties and performance:</p>
<ol>
<li><i><a href="/w/index.php?title=Transactional_replication&amp;action=edit&amp;redlink=1" class="new" title="Transactional replication (page does not exist)">Transactional replication</a></i>. This is the model for replicating <a href="/wiki/Transactional_data" title="Transactional data" class="mw-redirect">transactional data</a>, for example a database or some other form of transactional storage structure. The <a href="/w/index.php?title=One-copy_serializability&amp;action=edit&amp;redlink=1" class="new" title="One-copy serializability (page does not exist)">one-copy serializability</a> model is employed in this case, which defines legal outcomes of a transaction on replicated data in accordance with the overall <a href="/wiki/ACID" title="ACID">ACID</a> properties that transactional systems seek to guarantee.</li>
<li><i><a href="/wiki/State_machine_replication" title="State machine replication">State machine replication</a>.</i> This model assumes that replicated process is a <a href="/wiki/Deterministic_finite_automaton" title="Deterministic finite automaton">deterministic finite automaton</a> and that <a href="/wiki/Atomic_broadcast" title="Atomic broadcast">atomic broadcast</a> of every event is possible. It is based on a distributed computing problem called <i><a href="/wiki/Consensus_(computer_science)" title="Consensus (computer science)">distributed consensus</a></i> and has a great deal in common with the transactional replication model. This is sometimes mistakenly used as synonym of <i>active replication</i>. State machine replication is usually implemented by a replicated log consisting of multiple subsequent rounds of the <a href="/wiki/Paxos_algorithm" title="Paxos algorithm" class="mw-redirect">Paxos algorithm</a>. This was popularized by Google's Chubby system, and is the core behind the open-source <a href="/w/index.php?title=Keyspace_(data_store)&amp;action=edit&amp;redlink=1" class="new" title="Keyspace (data store) (page does not exist)">Keyspace data store</a>.<sup id="cite_ref-keyspace_4-0" class="reference"><a href="#cite_note-keyspace-4"><span>[</span>4<span>]</span></a></sup><sup id="cite_ref-chubby_5-0" class="reference"><a href="#cite_note-chubby-5"><span>[</span>5<span>]</span></a></sup></li>
<li><i><a href="/wiki/Virtual_synchrony" title="Virtual synchrony">Virtual synchrony</a></i>. This computational model is used when a group of processes cooperate to replicate in-memory data or to coordinate actions. The model defines a distributed entity called a <i>process group</i>. A process can join a group, and is provided with a checkpoint containing the current state of the data replicated by group members. Processes can then send multicasts to the group and will see incoming multicasts in the identical order. Membership changes are handled as a special multicast that delivers a new <i>membership view</i> to the processes in the group.</li>
</ol>
<h2><span class="mw-headline" id="Database_replication"><span id="DATABASE"></span>Database replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=3" title="Edit section: Database replication">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Database" title="Database">Database</a> replication can be used on many <a href="/wiki/Database_management_system" title="Database management system" class="mw-redirect">database management systems</a>, usually with a master/slave relationship between the original and the copies. The master logs the updates, which then ripple through to the slaves. The slave outputs a message stating that it has received the update successfully, thus allowing the sending (and potentially re-sending until successfully applied) of subsequent updates.</p>
<p><a href="/wiki/Multi-master_replication" title="Multi-master replication">Multi-master replication</a>, where updates can be submitted to any database node, and then ripple through to other servers, is often desired, but introduces substantially increased costs and complexity which may make it impractical in some situations. The most common challenge that exists in multi-master replication is transactional conflict prevention or resolution. Most synchronous or eager replication solutions do conflict prevention, while asynchronous solutions have to do conflict resolution. For instance, if a record is changed on two nodes simultaneously, an eager replication system would detect the conflict before confirming the commit and abort one of the transactions. A <a href="/wiki/Lazy_replication" title="Lazy replication" class="mw-redirect">lazy replication</a> system would allow both transactions to commit and run a conflict resolution during resynchronization. The resolution of such a conflict may be based on a timestamp of the transaction, on the hierarchy of the origin nodes or on much more complex logic, which decides consistently on all nodes.</p>
<p>Database replication becomes difficult when it scales up. Usually, the scale up goes with two dimensions, horizontal and vertical: horizontal scale-up has more data replicas, vertical scale-up has data replicas located further away in distance. Problems raised by horizontal scale-up can be alleviated by a multi-layer multi-view access protocol. Vertical scale-up causes fewer problems in that internet reliability and performance are improving.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup></p>
<p>When data is replicated between database servers, so that the information remains consistent throughout the database system and users cannot tell or even know which server in the DBMS they are using, the system is said to exhibit replication transparency.</p>
<h2><span class="mw-headline" id="Disk_storage_replication">Disk storage replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=4" title="Edit section: Disk storage replication">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Active (real-time) storage replication is usually implemented by distributing updates of a <a href="/wiki/Block_device" title="Block device" class="mw-redirect">block device</a> to several physical <a href="/wiki/Hard_disk" title="Hard disk" class="mw-redirect">hard disks</a>. This way, any <a href="/wiki/File_system" title="File system">file system</a> supported by the <a href="/wiki/Operating_system" title="Operating system">operating system</a> can be replicated without modification, as the file system code works on a level above the block device driver layer. It is implemented either in hardware (in a <a href="/wiki/Disk_array_controller" title="Disk array controller">disk array controller</a>) or in software (in a <a href="/wiki/Device_driver" title="Device driver">device driver</a>).</p>
<p>The most basic method is <a href="/wiki/Disk_mirroring" title="Disk mirroring">disk mirroring</a>, typical for locally-connected disks. The storage industry narrows the definitions, so <i>mirroring</i> is a local (short-distance) operation. A <i>replication</i> is extendable across a <a href="/wiki/Computer_network" title="Computer network">computer network</a>, so the disks can be located in physically distant locations, and the master-slave database replication model is usually applied. The purpose of replication is to prevent damage from failures or <a href="/wiki/Disaster_Recovery" title="Disaster Recovery" class="mw-redirect">disasters</a> that may occur in one location, or in case such events do occur, improve the ability to recover.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7"><span>[</span>7<span>]</span></a></sup> For replication, latency is the key factor because it determines either how far apart the sites can be or the type of replication that can be employed.</p>
<p>The main characteristic of such cross-site replication is how write operations are handled:</p>
<ul>
<li><a href="/wiki/Synchronization" title="Synchronization">Synchronous</a> replication - guarantees "zero data loss" by the means of <a href="/wiki/Atomic_operation" title="Atomic operation" class="mw-redirect">atomic</a> write operation, i.e. write either completes on both sides or not at all. Write is not considered complete until acknowledgement by both local and remote storage. Most applications wait for a write transaction to complete before proceeding with further work, hence overall performance decreases considerably. Inherently, performance drops proportionally to distance, as <a href="/wiki/Latency_(engineering)" title="Latency (engineering)">latency</a> is caused by <a href="/wiki/Speed_of_light" title="Speed of light">speed of light</a>. For 10 km distance, the fastest possible roundtrip takes 67 μs, whereas nowadays a whole local cached write completes in about 10-20 μs.
<ul>
<li>An often-overlooked aspect of synchronous replication is the fact that failure of <i>remote</i> replica, or even just the <i>interconnection</i>, stops by definition any and all writes (freezing the local storage system). This is the behaviour that guarantees zero data loss. However, many commercial systems at such potentially dangerous point do not freeze, but just proceed with local writes, losing the desired zero <a href="/wiki/Recovery_point_objective" title="Recovery point objective">recovery point objective</a>.</li>
<li>The main difference between synchronous and asynchronous volume replication is that synchronous replication needs to wait for the destination server in any write operation.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup></li>
</ul>
</li>
<li><a href="/wiki/Asynchronous_I/O" title="Asynchronous I/O">Asynchronous</a> replication - write is considered complete as soon as local storage acknowledges it. Remote storage is updated, but probably with a small <a href="/wiki/Lag" title="Lag">lag</a>. Performance is greatly increased, but in case of losing a local storage, the remote storage is not guaranteed to have the current copy of data and most recent data may be lost.</li>
<li>Semi-synchronous replication - this usually means<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2009)">citation needed</span></a></i>]</sup> that a write is considered complete as soon as local storage acknowledges it and a remote server acknowledges that it has received the write either into memory or to a dedicated log file. The actual remote write is not performed immediately but is performed asynchronously, resulting in better performance than synchronous replication but offering no guarantee of durability.
<ul>
<li>Point-in-time replication - introduces periodic <a href="/wiki/Snapshot_(computer_storage)" title="Snapshot (computer storage)">snapshots</a> that are replicated instead of primary storage. If the replicated snapshots are pointer-based, then during replication only the changed data is moved not the entire volume. Using this method, replication can occur over smaller, less expensive bandwidth links such as iSCSI or T1 instead of fiber optic lines.</li>
</ul>
</li>
</ul>
<ul>
<li>An often-overlooked aspect of synchronous replication is the fact that failure of <i>remote</i> replica, or even just the <i>interconnection</i>, stops by definition any and all writes (freezing the local storage system). This is the behaviour that guarantees zero data loss. However, many commercial systems at such potentially dangerous point do not freeze, but just proceed with local writes, losing the desired zero <a href="/wiki/Recovery_point_objective" title="Recovery point objective">recovery point objective</a>.</li>
<li>The main difference between synchronous and asynchronous volume replication is that synchronous replication needs to wait for the destination server in any write operation.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup></li>
</ul>
<ul>
<li>Point-in-time replication - introduces periodic <a href="/wiki/Snapshot_(computer_storage)" title="Snapshot (computer storage)">snapshots</a> that are replicated instead of primary storage. If the replicated snapshots are pointer-based, then during replication only the changed data is moved not the entire volume. Using this method, replication can occur over smaller, less expensive bandwidth links such as iSCSI or T1 instead of fiber optic lines.</li>
</ul>
<p>To address the limits imposed by latency, techniques of <a href="/wiki/WAN_optimization" title="WAN optimization">WAN optimization</a> can be applied to the link.</p>
<h3><span class="mw-headline" id="Implementations">Implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=5" title="Edit section: Implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many <a href="/wiki/Distributed_filesystem" title="Distributed filesystem" class="mw-redirect">distributed filesystems</a> use replication to ensure fault tolerance and avoid a single point of failure. See the lists of <a href="/wiki/Distributed_fault-tolerant_file_systems" title="Distributed fault-tolerant file systems" class="mw-redirect">distributed fault-tolerant file systems</a> and <a href="/wiki/Distributed_parallel_fault-tolerant_file_systems" title="Distributed parallel fault-tolerant file systems" class="mw-redirect">distributed parallel fault-tolerant file systems</a>.</p>
<p>Other storage replication software includes:</p>
<ul>
<li><a href="/wiki/CA_Technologies" title="CA Technologies">CA</a> - <a rel="nofollow" class="external text" href="http://www.arcserve.com/gb/default.aspx">ARCserve</a> Replication and High Availability <a rel="nofollow" class="external text" href="http://www.arcserve.com/gb/products/ca-arcserve-replication/ca-arcserve-replication-features-overview.aspx">RHA</a></li>
<li><a href="/wiki/Dell" title="Dell">Dell</a> - <a href="/w/index.php?title=AppAssure&amp;action=edit&amp;redlink=1" class="new" title="AppAssure (page does not exist)">AppAssure</a> Backup (replication and disaster recovery) and Compellent Remote Instant Replay</li>
<li><a href="/wiki/EMC_Corporation" title="EMC Corporation">EMC</a> - <a href="/wiki/RecoverPoint" title="RecoverPoint">EMC RecoverPoint</a>, <a href="/wiki/SRDF" title="SRDF" class="mw-redirect">EMC SRDF</a> and <a href="/wiki/VPLEX" title="VPLEX" class="mw-redirect">EMC VPLEX</a></li>
<li><a href="/w/index.php?title=EDpCloud_EnduraData&amp;action=edit&amp;redlink=1" class="new" title="EDpCloud EnduraData (page does not exist)">EnduraData</a> Real time and schedule replication</li>
<li><a href="/wiki/DataCore_Software" title="DataCore Software">DataCore</a> SANsymphony and SANmelody</li>
<li><a href="/wiki/StarWind_Software" title="StarWind Software">StarWind</a> iSCSI SAN &amp; NAS</li>
<li><a href="/w/index.php?title=StorMagic&amp;action=edit&amp;redlink=1" class="new" title="StorMagic (page does not exist)">StorMagic SvSan</a> replicating iSCSI virtual Appliance for vSphere &amp; HyperV</li>
<li><a href="/wiki/FalconStor" title="FalconStor" class="mw-redirect">FalconStor</a> Replication &amp; Mirroring (sub-block heterogeneous point-in-time, async, sync)</li>
<li><a href="/wiki/FreeNAS" title="FreeNAS">FreeNAS</a> - Replication handled by ssh + zfs file system <sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup></li>
<li><a href="/wiki/Hitachi_TrueCopy" title="Hitachi TrueCopy">Hitachi TrueCopy</a></li>
<li><a href="/wiki/Hewlett-Packard" title="Hewlett-Packard">Hewlett-Packard</a> - Continuous Access (HP CA)</li>
<li><a href="/wiki/IBM" title="IBM">IBM</a> - <a href="/wiki/Peer_to_Peer_Remote_Copy" title="Peer to Peer Remote Copy">Peer to Peer Remote Copy</a> (PPRC), <a href="/wiki/Global_Mirror" title="Global Mirror" class="mw-redirect">Global Mirror</a> and <a href="/wiki/Extended_Remote_Copy" title="Extended Remote Copy">Extended Remote Copy</a> (XRC), known together as IBM Copy Services</li>
<li><a rel="nofollow" class="external text" href="http://www.linbit.com">Linbit</a> - <a href="/wiki/DRBD" title="DRBD" class="mw-redirect">DRBD</a> - open source block level replication for Linux</li>
<li><a href="/wiki/Highly_Available_STorage" title="Highly Available STorage">HAST</a> DRBD-like Open Source solution for FreeBSD.</li>
<li><a href="/wiki/MapR" title="MapR">MapR</a> volume mirroring</li>
<li><a href="/wiki/NEC" title="NEC">NEC</a> –  <a rel="nofollow" class="external text" href="http://www.nec-computers.com/support2/pib.asp?mode=default&amp;platform=replication_software_storage">DDR/RDR</a></li>
<li><a href="/wiki/NetApp" title="NetApp">NetApp</a> - <a href="/wiki/NetApp_filer#SyncMirror" title="NetApp filer">SyncMirror</a> and <a href="/wiki/NetApp_filer#SnapMirror" title="NetApp filer">SnapMirror</a></li>
<li><a href="/wiki/Veritas_Software" title="Veritas Software">Symantec Veritas Volume Replicator</a> (VVR)</li>
<li><a href="/wiki/VMware" title="VMware">VMware</a> - Site Recovery Manager (SRM) <sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup></li>
</ul>
<h2><span class="mw-headline" id="File-based_replication">File-based replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=6" title="Edit section: File-based replication">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>File-based replication is replicating files at a logical level rather than replicating at the storage block level. There are many different ways of performing this. Unlike with storage-level replication, the solutions almost exclusively rely on software.</p>
<h3><span class="mw-headline" id="Capture_with_a_kernel_driver">Capture with a kernel driver</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=7" title="Edit section: Capture with a kernel driver">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>With the use of a <a href="/wiki/Kernel_driver" title="Kernel driver" class="mw-redirect">kernel driver</a> (specifically a <a href="/wiki/Filter_driver" title="Filter driver">filter driver</a>), that intercepts calls to the filesystem functions, any activity is captured immediately as it occurs. This utilises the same type of technology that real time active virus checkers employ. At this level, logical file operations are captured like file open, write, delete, etc. The kernel driver transmits these commands to another process, generally over a network to a different machine, which will mimic the operations of the source machine. Like block-level storage replication, the file-level replication allows both synchronous and asynchronous modes. In synchronous mode, write operations on the source machine are held and not allowed to occur until the destination machine has acknowledged the successful replication. Synchronous mode is less common with file replication products although a few solutions exists.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span>[</span>11<span>]</span></a></sup></p>
<p>File level replication solution yield a few benefits. Firstly because data is captured at a file level it can make an informed decision on whether to replicate based on the location of the file and the type of file. Hence unlike block-level storage replication where a whole volume needs to be replicated, file replication products have the ability to exclude temporary files or parts of a filesystem that hold no business value. This can substantially reduce the amount of data sent from the source machine as well as decrease the storage burden on the destination machine. A further benefit to decreasing bandwidth is the data transmitted can be more granular than with block-level replication. If an application writes 100 bytes, only the 100 bytes are transmitted not a complete disk block which is generally 4096 bytes.</p>
<p>On a negative side, as this is a software only solution, it requires implementation and maintenance on the operating system level, and uses some of machine's processing power (CPU).</p>
<p>Notable implementations:</p>
<ul>
<li><a href="/wiki/CA_Technologies" title="CA Technologies">CA</a> <a rel="nofollow" class="external text" href="http://www.arcserve.com/gb/default.aspx">ARCserve</a> <a rel="nofollow" class="external text" href="http://www.arcserve.com/gb/products/ca-arcserve-replication/ca-arcserve-replication-features-overview.aspx">Replication</a></li>
<li><a href="/wiki/Hitachi_Data_Systems" title="Hitachi Data Systems">Hitachi</a> <a rel="nofollow" class="external text" href="http://www.hds.com/products/storage-software/hitachi-data-instance-manager.html">Data Instance Manager</a> (formerly <a href="/wiki/Cofio_Software" title="Cofio Software">Cofio Software</a> <a href="/wiki/AIMstor" title="AIMstor">AIMstor</a><sup id="cite_ref-12" class="reference"><a href="#cite_note-12"><span>[</span>12<span>]</span></a></sup>)</li>
<li><a href="/wiki/Double-Take_Software" title="Double-Take Software">Double-Take Software</a> <a rel="nofollow" class="external text" href="http://www.doubletake.com/uk/products/double-take-availability/Pages/default.aspx">Availability</a></li>
<li><a href="/w/index.php?title=EDpCloud_Software&amp;action=edit&amp;redlink=1" class="new" title="EDpCloud Software (page does not exist)">EDpCloud Software</a> <a rel="nofollow" class="external text" href="http://www.enduradata.com/">EDpCloud Real Time Replication</a></li>
<li><a rel="nofollow" class="external text" href="http://www.evidian.com/safekit/">Evidian SafeKit</a> Replication, High Availability and Load Balancing (<a rel="nofollow" class="external text" href="http://www.evidian.com/products/high-availability-software-for-application-clustering/file-replication-byte-level-with-failover-mirror-cluster/">synchronous byte-level file replication with failover</a>)</li>
</ul>
<h4><span class="mw-headline" id="Filesystem_journal_replication">Filesystem journal replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=8" title="Edit section: Filesystem journal replication">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In many ways working like a database journal, many filesystems have the ability to journal their activity. The journal can be sent to another machine, either periodically or in real time. It can be used there to play back events.</p>
<p>Notable implementations:</p>
<ul>
<li><a href="/wiki/Microsoft" title="Microsoft">Microsoft</a> <a href="/wiki/System_Center_Data_Protection_Manager" title="System Center Data Protection Manager">DPM</a> (periodical updates, not in real time)</li>
</ul>
<h3><span class="mw-headline" id="Batch_replication">Batch replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=9" title="Edit section: Batch replication">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>This is the process of comparing the source and destination filesystems and ensuring that the destination matches the source. The key benefit is that such solutions are generally free or inexpensive. The downside is that the process of synchronizing them is quite system-intensive, and consequently this process generally runs infrequently.</p>
<p>Notable implementations:</p>
<ul>
<li><a href="/wiki/Rsync" title="Rsync">rsync</a></li>
</ul>
<h2><span class="mw-headline" id="Distributed_shared_memory_replication">Distributed shared memory replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=10" title="Edit section: Distributed shared memory replication">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Another example of using replication appears in <a href="/wiki/Distributed_shared_memory" title="Distributed shared memory">distributed shared memory</a> systems, where it may happen that many nodes of the system share the same page of the memory - which usually means, that each node has a separate copy (replica) of this page.</p>
<h2><span class="mw-headline" id="Primary-backup_and_multi-primary_replication">Primary-backup and multi-primary replication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=11" title="Edit section: Primary-backup and multi-primary replication">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Many classical approaches to replication are based on a primary/backup model where one device or process has unilateral control over one or more other processes or devices. For example, the primary might perform some computation, streaming a log of updates to a backup (standby) process, which can then take over if the primary fails. This approach is the most common one for replicating databases, despite the risk that if a portion of the log is lost during a failure, the backup might not be in a state identical to the one the primary was in, and transactions could then be lost.</p>
<p>A weakness of primary/backup schemes is that in settings where both processes could have been active, only one is actually performing operations. We're gaining fault-tolerance but spending twice as much money to get this property. For this reason, starting in the period around 1985, the distributed systems research community began to explore alternative methods of replicating data. An outgrowth of this work was the emergence of schemes in which a group of replicas could cooperate, with each process backup up the others, and each handling some share of the workload.</p>
<p><a href="/wiki/Jim_Gray_(computer_scientist)" title="Jim Gray (computer scientist)">Jim Gray</a>, a towering figure<sup id="cite_ref-13" class="reference"><a href="#cite_note-13"><span>[</span>13<span>]</span></a></sup> within the database community, analyzed multi-primary replication schemes under the transactional model and ultimately published a widely cited paper skeptical of the approach "<a rel="nofollow" class="external text" href="http://research.microsoft.com/~gray/replicas.ps">The Dangers of Replication and a Solution</a>". In a nutshell, he argued that unless data splits in some natural way so that the database can be treated as <i>n</i> disjoint sub-databases, concurrency control conflicts will result in seriously degraded performance and the group of replicas will probably slow down as a function of <i>n</i>. Indeed, he suggests that the most common approaches are likely to result in degradation that scales as <i>O(n³)</i>. His solution, which is to partition the data, is only viable in situations where data actually has a natural partitioning key.</p>
<p>The situation is not always so bleak. For example, in the 1985-1987 period, the <a href="/wiki/Virtual_synchrony" title="Virtual synchrony">virtual synchrony</a> model was proposed and emerged as a widely adopted standard (it was used in the Isis Toolkit, Horus, Transis, Ensemble, Totem, <a href="/wiki/Spread_Toolkit" title="Spread Toolkit">Spread</a>, C-Ensemble, Phoenix and Quicksilver systems, and is the basis for the CORBA fault-tolerant computing standard; the model is also used in IBM Websphere to replicate business logic and in Microsoft's Windows Server 2008 <a href="/wiki/Microsoft_Cluster_Server" title="Microsoft Cluster Server">enterprise clustering</a> technology). Virtual synchrony permits a multi-primary approach in which a group of processes cooperate to parallelize some aspects of request processing. The scheme can only be used for some forms of in-memory data, but when feasible, provides linear speedups in the size of the group.</p>
<p>A number of modern products support similar schemes. For example, the <a href="/wiki/Spread_Toolkit" title="Spread Toolkit">Spread Toolkit</a> supports this same virtual synchrony model and can be used to implement a multi-primary replication scheme; it would also be possible to use C-Ensemble or Quicksilver in this manner. <a href="/wiki/WANdisco" title="WANdisco">WANdisco</a> permits active replication where every node on a network is an exact copy or <a href="/wiki/Replica" title="Replica">replica</a> and hence every node on the network is active at one time; this scheme is optimized for use in a <a href="/wiki/Wide_area_network" title="Wide area network">wide area network</a>.</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=12" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Change_data_capture" title="Change data capture">Change data capture</a></li>
<li><a href="/wiki/Cloud_computing" title="Cloud computing">Cloud computing</a></li>
<li><a href="/wiki/Cluster_(computing)" title="Cluster (computing)" class="mw-redirect">Cluster (computing)</a></li>
<li><a href="/wiki/Cluster_manager" title="Cluster manager">Cluster manager</a></li>
<li><a href="/wiki/Failover" title="Failover">Failover</a></li>
<li><a href="/wiki/Fault_tolerant_system" title="Fault tolerant system" class="mw-redirect">Fault tolerant system</a></li>
<li><a href="/wiki/Log_Shipping" title="Log Shipping" class="mw-redirect">Log Shipping</a></li>
<li><a href="/wiki/Optimistic_replication" title="Optimistic replication">Optimistic replication</a></li>
<li><a href="/wiki/Process_group" title="Process group">Process group</a></li>
<li><a href="/wiki/Software_transactional_memory" title="Software transactional memory">Software transactional memory</a></li>
<li><a href="/wiki/Transparency_(human-computer_interaction)" title="Transparency (human-computer interaction)" class="mw-redirect">Transparency</a></li>
<li><a href="/wiki/Virtual_synchrony" title="Virtual synchrony">Virtual synchrony</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Replication_(computing)&amp;action=edit&amp;section=13" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://searchsqlserver.techtarget.com/definition/database-replication">"What is database replication? - Definition from WhatIs.com"</a>. Searchsqlserver.techtarget.com<span class="reference-accessdate">. Retrieved 2014-01-12</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.btitle=What+is+database+replication%3F+-+Definition+from+WhatIs.com&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fsearchsqlserver.techtarget.com%2Fdefinition%2Fdatabase-replication&amp;rft.pub=Searchsqlserver.techtarget.com&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">Mansouri, Najme, GholamHosein Dastghaibyfard, and Ehsan Mansouri. "Combination of data replication and scheduling algorithm for improving data availability in Data Grids." Journal of Network and Computer Applications (2013)</span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">V. Andronikou, K. Mamouras, K. Tserpes, D. Kyriazis, T. Varvarigou, <i>Dynamic QoS-aware Data Replication in Grid Environments</i>, Elsevier Future Generation Computer Systems - The International Journal of Grid Computing and eScience, 2012</span></li>
<li id="cite_note-keyspace-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-keyspace_4-0">^</a></b></span> <span class="reference-text"><span class="citation web">Marton Trencseni, Attila Gazso (2009). <a rel="nofollow" class="external text" href="http://scalien.com/whitepapers">"Keyspace: A Consistently Replicated, Highly-Available Key-Value Store"</a><span class="reference-accessdate">. Retrieved 2010-04-18</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.aulast=Marton+Trencseni%2C+Attila+Gazso&amp;rft.au=Marton+Trencseni%2C+Attila+Gazso&amp;rft.btitle=Keyspace%3A+A+Consistently+Replicated%2C+Highly-Available+Key-Value+Store&amp;rft.date=2009&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fscalien.com%2Fwhitepapers&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-chubby-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-chubby_5-0">^</a></b></span> <span class="reference-text"><span class="citation web">Mike Burrows (2006). <a rel="nofollow" class="external text" href="http://labs.google.com/papers/chubby.html">"The Chubby Lock Service for Loosely-Coupled Distributed Systems"</a><span class="reference-accessdate">. Retrieved 2010-04-18</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.aulast=Mike+Burrows&amp;rft.au=Mike+Burrows&amp;rft.btitle=The+Chubby+Lock+Service+for+Loosely-Coupled+Distributed+Systems&amp;rft.date=2006&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Flabs.google.com%2Fpapers%2Fchubby.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><span class="citation web">Dragan Simic; Srecko Ristic; Slobodan Obradovic (April 2007). <a rel="nofollow" class="external text" href="http://facta.junis.ni.ac.rs/eae/fu2k71/4obradovic.pdf">"Measurement of the Achieved Performance Levels of the WEB Applications With Distributed Relational Database"</a> (PDF). <i>Electronics and Energetics</i> <b>20</b> (1). Facta Universitatis. p. 31–43<span class="reference-accessdate">. Retrieved 30 January 2014</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.atitle=Measurement+of+the+Achieved+Performance+Levels+of+the+WEB+Applications+With+Distributed+Relational+Database&amp;rft.au=Dragan+Simic&amp;rft.aulast=Dragan+Simic&amp;rft.au=Slobodan+Obradovic&amp;rft.au=Srecko+Ristic&amp;rft.date=April+2007&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Ffacta.junis.ni.ac.rs%2Feae%2Ffu2k71%2F4obradovic.pdf&amp;rft.issue=1&amp;rft.jtitle=Electronics+and+Energetics&amp;rft.pages=31-43&amp;rft.pub=Facta+Universitatis&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=20" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><span class="citation web">Hamilton, James. <a rel="nofollow" class="external text" href="http://perspectives.mvdirona.com/2010/05/10/InterDatacenterReplicationGeoRedundancy.aspx">"Inter-Datacenter Replication &amp; Geo-Redundancy"</a>. <i>James Hamilton's Blog</i><span class="reference-accessdate">. Retrieved 20 July 2011</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.atitle=Inter-Datacenter+Replication+%26+Geo-Redundancy&amp;rft.aufirst=James&amp;rft.au=Hamilton%2C+James&amp;rft.aulast=Hamilton&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fperspectives.mvdirona.com%2F2010%2F05%2F10%2FInterDatacenterReplicationGeoRedundancy.aspx&amp;rft.jtitle=James+Hamilton%27s+Blog&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://kb.open-e.com/What-is-the-difference-between-asynchronous-and-synchronous-volume-replication-_682.html">Open-E Knowledgebase. "What is the difference between asynchronous and synchronous volume replication?"</a> 12 August 2009.</span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://doc.freenas.org/index.php/Replication_Tasks">"Replication Tasks - FreeNAS"</a>. Doc.freenas.org. 2013-12-18<span class="reference-accessdate">. Retrieved 2014-01-12</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.btitle=Replication+Tasks+-+FreeNAS&amp;rft.date=2013-12-18&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fdoc.freenas.org%2Findex.php%2FReplication_Tasks&amp;rft.pub=Doc.freenas.org&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://pubs.vmware.com/srm-51/index.jsp?topic=%2Fcom.vmware.srm.install_config.doc%2FGUID-B3A49FFF-E3B9-45E3-AD35-093D896596A0.html">"VMware vCenter Site Recovery Manager 5.1 Documentation Library"</a>. Pubs.vmware.com<span class="reference-accessdate">. Retrieved 2014-01-12</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.btitle=VMware+vCenter+Site+Recovery+Manager+5.1+Documentation+Library&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fpubs.vmware.com%2Fsrm-51%2Findex.jsp%3Ftopic%3D%252Fcom.vmware.srm.install_config.doc%252FGUID-B3A49FFF-E3B9-45E3-AD35-093D896596A0.html&amp;rft.pub=Pubs.vmware.com&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.cofio.com/AIMstor-Replication/">AIMstor Replication</a></span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><span class="citation web">Bigelow, Bruce (4 October 2012). <a rel="nofollow" class="external text" href="http://www.xconomy.com/san-diego/2012/10/04/hitachi-data-systems-buys-cofio-software-infrastructure-developer/">"Hitachi Data Systems Buys Cofio, Software Infrastructure Developer"</a>. <i>Xconomy</i><span class="reference-accessdate">. Retrieved 12 June 2014</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReplication+%28computing%29&amp;rft.atitle=Hitachi+Data+Systems+Buys+Cofio%2C+Software+Infrastructure+Developer&amp;rft.au=Bigelow%2C+Bruce&amp;rft.aufirst=Bruce&amp;rft.aulast=Bigelow&amp;rft.date=4+October+2012&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.xconomy.com%2Fsan-diego%2F2012%2F10%2F04%2Fhitachi-data-systems-buys-cofio-software-infrastructure-developer%2F&amp;rft.jtitle=Xconomy&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><i>Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data: SIGMOD '99</i>, Philadelphia, PA, USA; June 1–3, 1999, Volume 28; p. 3.</span></li>
</ol>
