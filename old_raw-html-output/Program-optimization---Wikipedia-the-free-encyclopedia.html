<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Program optimization</span></h1>
<p>In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b>program optimization</b> or <b>software optimization</b> is the process of modifying a software system to make some aspect of it work more <a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">efficiently</a> or use fewer resources.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> In general, a <a href="/wiki/Computer_program" title="Computer program">computer program</a> may be optimized so that it executes more rapidly, or is capable of operating with less <a href="/wiki/Computer_data_storage" title="Computer data storage">memory storage</a> or other resources, or draw less power.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#General"><span class="tocnumber">1</span> <span class="toctext">General</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Levels_of_optimization"><span class="tocnumber">2</span> <span class="toctext">Levels of optimization</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Platform_dependent_and_independent_optimizations"><span class="tocnumber">2.1</span> <span class="toctext">Platform dependent and independent optimizations</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Trade-offs"><span class="tocnumber">3</span> <span class="toctext">Trade-offs</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Strength_reduction"><span class="tocnumber">4</span> <span class="toctext">Strength reduction</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Trade-offs_2"><span class="tocnumber">5</span> <span class="toctext"><span>Trade-offs</span></span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Bottlenecks"><span class="tocnumber">6</span> <span class="toctext">Bottlenecks</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#When_to_optimize"><span class="tocnumber">7</span> <span class="toctext">When to optimize</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Macros"><span class="tocnumber">8</span> <span class="toctext">Macros</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#Automated_and_manual_optimization"><span class="tocnumber">9</span> <span class="toctext">Automated and manual optimization</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Time_taken_for_optimization"><span class="tocnumber">10</span> <span class="toctext">Time taken for optimization</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Quotes"><span class="tocnumber">11</span> <span class="toctext">Quotes</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#See_also"><span class="tocnumber">12</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#References"><span class="tocnumber">13</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#External_links"><span class="tocnumber">14</span> <span class="toctext">External links</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Platform_dependent_and_independent_optimizations"><span class="tocnumber">2.1</span> <span class="toctext">Platform dependent and independent optimizations</span></a></li>
</ul>
<p></p>
<h2><span class="mw-headline" id="General">General</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=1" title="Edit section: General">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Although the word "optimization" shares the same root as "optimal", it is rare for the process of optimization to produce a truly optimal system. The optimized system will typically only be optimal in one application or for one audience. One might reduce the amount of time that a program takes to perform some task at the price of making it consume more memory. In an application where memory space is at a premium, one might deliberately choose a slower <a href="/wiki/Algorithm" title="Algorithm">algorithm</a> in order to use less memory. Often there is no "one size fits all" design which works well in all cases, so <a href="/wiki/Engineer" title="Engineer">engineers</a> make <a href="/wiki/Trade-off" title="Trade-off">trade-offs</a> to optimize the attributes of greatest interest. Additionally, the effort required to make a piece of software completely optimal — incapable of any further improvement — is almost always more than is reasonable for the benefits that would be accrued; so the process of optimization may be halted before a completely optimal solution has been reached. Fortunately, it is often the case that the greatest improvements come early in the process.</p>
<h2><span class="mw-headline" id="Levels_of_optimization">Levels of optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=2" title="Edit section: Levels of optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Optimization can occur at a number of levels. Typically the higher levels have greater impact, and are harder to change later on in a project, requiring significant changes or a complete rewrite if they need to be changed. Thus optimization can typically proceed via refinement from higher to lower, with initial gains being larger and achieved with less work, and later gains being smaller and requiring more work. However, in some cases overall performance depends on performance of very low-level portions of a program, and small changes at a late stage or early consideration of low-level details can have outsized impact. Typically some consideration is given to efficiency throughout a project – though this varies significantly – but major optimization is often considered a refinement to be done late, if ever. On longer-running projects there are typically cycles of optimization, where improving one area reveals limitations in another, and these are typically curtailed when performance is acceptable or gains become too small or costly.</p>
<p>As performance is part of the specification of a program – a program that is unusably slow is not fit for purpose: a video game with 60 Hz (frames-per-second) is acceptable, but 6 fps is unacceptably choppy – performance is a consideration from the start, to ensure that the system is able to deliver sufficient performance, and early prototypes need to have roughly acceptable performance for there to be confidence that the final system will (with optimization) achieve acceptable performance. This is sometimes omitted in the belief that optimization can always be done later, resulting in prototype systems that are far too slow – often by an order of magnitude (factor of 10×) or more – and systems that ultimately are failures because they architecturally cannot achieve their performance goals, such as the <a href="/wiki/Intel_432" title="Intel 432" class="mw-redirect">Intel 432</a> (1981); or ones that take years of work to achieve acceptable performance, such as Java (1995), which only achieved acceptable performance with <a href="/wiki/HotSpot" title="HotSpot">HotSpot</a> (1999). The degree to which performance changes between prototype and production system, and how amenable it is to optimization, can be a significant source of uncertainty and risk.</p>
<ul>
<li><b>Design level</b></li>
</ul>
<p>At the highest level, the design may be optimized to make best use of the available resources, given goals, constraints, and expected use/load. The architectural design of a system overwhelmingly affects its performance. For example, a system that is network latency-bound (where network latency is the main constraint on overall performance) would be optimized to minimize network trips, ideally making a single request (or no requests, as in a <a href="/wiki/Push_protocol" title="Push protocol" class="mw-redirect">push protocol</a>) rather than multiple roundtrips. Choice of design depends on the goals: when designing a <a href="/wiki/Compiler" title="Compiler">compiler</a>, if fast compilation is the key priority, a <a href="/wiki/One-pass_compiler" title="One-pass compiler">one-pass compiler</a> is faster than a <a href="/wiki/Multi-pass_compiler" title="Multi-pass compiler">multi-pass compiler</a> (assuming same work), but if speed of output code is the goal, a slower multi-pass compiler fulfills the goal better, even though it takes longer itself. Choice of platform and programming language occur at this level, and changing them frequently requires a complete rewrite, though a modular system may allow rewrite of only some component – for example, a Python program may rewrite performance-critical sections in C. In a distributed system, choice of architecture (client-server, peer-to-peer, etc.) occurs at the design level, and may be difficult to change, particularly if all components cannot be replaced in sync (e.g., old clients).</p>
<ul>
<li><b>Algorithms and data structures</b></li>
</ul>
<p>Given an overall design, a good choice of <a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">efficient algorithms</a> and <a href="/wiki/Data_structure" title="Data structure">data structures</a>, and efficient implementation of these algorithms and data structures comes next. After design, the choice of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> and data structures affects efficiency more than any other aspect of the program. Generally data structures are more difficult to change than algorithms, as a data structure assumption and its performance assumptions are used throughout the program, though this can be minimized by the use of abstract data types in function definitions, and keeping the concrete data structure definitions restricted to a few places.</p>
<p>For algorithms, this primarily consists of ensuring that algorithms are constant O(1), logarithmic O(log <i>n</i>), linear O(<i>n</i>), or in some cases log-linear O(<i>n</i> log <i>n</i>) in the input (both in space and time). Algorithms with quadratic complexity O(<i>n</i><sup>2</sup>) fail to scale, and even linear algorithms cause problems if repeatedly called, and are typically replaced with constant or logarithmic if possible.</p>
<p>Beyond asymptotic order of growth, the constant factors matter: an asymptotically slower algorithm may be faster or smaller (because simpler) than an asymptotically faster algorithm when they are both faced with small input, which may be the case that occurs in reality. Often a <a href="/wiki/Hybrid_algorithm" title="Hybrid algorithm">hybrid algorithm</a> will provide the best performance, due to this tradeoff changing with size.</p>
<p>A general technique to improve performance is to avoid work. A good example is the use of a <a href="/wiki/Fast_path" title="Fast path">fast path</a> for common cases, improving performance by avoiding unnecessary work. For example, using a simple text layout algorithm for Latin text, only switching to a complex layout algorithm for complex scripts, such as <a href="/wiki/Devanagari" title="Devanagari">Devanagari</a>. Another important technique is caching, particularly <a href="/wiki/Memoization" title="Memoization">memoization</a>, which avoids redundant computations. Because of the importance of caching, there are often many levels of caching in a system, which can cause problems from memory use, and correctness issues from stale caches.</p>
<ul>
<li><b>Source code level</b></li>
</ul>
<p>Beyond general algorithms and their implementation on an abstract machine, concrete source code level choices can make a significant difference. For example, on early C compilers, <code>while(1)</code> was slower than <code>for(;;)</code> for an unconditional loop, because <code>while(1)</code> evaluated 1 and then had a conditional jump which tested if it was true, while <code>for (;;)</code> had an unconditional jump . Some optimizations (such as this one) can nowadays be performed by <a href="/wiki/Optimizing_compiler" title="Optimizing compiler">optimizing compilers</a>. This depends on the source language, the target machine language, and the compiler, and can be both difficult to understand or predict and changes over time; this is a key place where understanding of compilers and machine code can improve performance. <a href="/wiki/Loop-invariant_code_motion" title="Loop-invariant code motion">Loop-invariant code motion</a> and <a href="/wiki/Return_value_optimization" title="Return value optimization">return value optimization</a> are examples of optimizations that reduce the need for auxiliary variables and can even result in faster performance by avoiding round-about optimizations.</p>
<ul>
<li><b>Build level</b></li>
</ul>
<p>Between the source and compile level, <a href="/wiki/Directive_(programming)" title="Directive (programming)">directives</a> and <a href="/wiki/Build_automation" title="Build automation">build flags</a> can be used to tune performance options in the source code and compiler respectively, such as using <a href="/wiki/Preprocessor" title="Preprocessor">preprocessor</a> defines to disable unneeded software features, optimizing for specific processor models or hardware capabilities, or predicting branching, for instance. Source-based software distribution systems such as <a href="/wiki/Berkeley_Software_Distribution" title="Berkeley Software Distribution">BSD</a>'s <a href="/wiki/Ports_collection" title="Ports collection">Ports</a> and <a href="/wiki/Gentoo_Linux" title="Gentoo Linux">Gentoo</a>'s <a href="/wiki/Portage_(software)" title="Portage (software)">Portage</a> can take advantage of this form of optimization.</p>
<ul>
<li><b>Compile level</b></li>
</ul>
<p>Use of an <a href="/wiki/Optimizing_compiler" title="Optimizing compiler">optimizing compiler</a> tends to ensure that the <a href="/wiki/Executable_program" title="Executable program" class="mw-redirect">executable program</a> is optimized at least as much as the compiler can predict.</p>
<ul>
<li><b>Assembly level</b></li>
</ul>
<p>At the lowest level, writing code using an <a href="/wiki/Assembly_language" title="Assembly language">assembly language</a>, designed for a particular hardware platform can produce the most efficient and compact code if the programmer takes advantage of the full repertoire of <a href="/wiki/Machine_instruction" title="Machine instruction" class="mw-redirect">machine instructions</a>. Many <a href="/wiki/Operating_system" title="Operating system">operating systems</a> used on <a href="/wiki/Embedded_system" title="Embedded system">embedded systems</a> have been traditionally written in assembler code for this reason. Programs (other than very small programs) are seldom written from start to finish in assembly due to the time and cost involved. Most are compiled down from a high level language to assembly and hand optimized from there. When efficiency and size are less important large parts may be written in a high-level language.</p>
<p>With more modern <a href="/wiki/Optimizing_compiler" title="Optimizing compiler">optimizing compilers</a> and the greater complexity of recent <a href="/wiki/CPU" title="CPU" class="mw-redirect">CPUs</a>, it is harder to write more efficient code than what the compiler generates, and few projects need this "ultimate" optimization step.</p>
<p>Much code written today is intended to run on as many machines as possible. As a consequence, programmers and compilers don't always take advantage of the more efficient instructions provided by newer CPUs or quirks of older models. Additionally, assembly code tuned for a particular processor without using such instructions might still be suboptimal on a different processor, expecting a different tuning of the code.</p>
<p>Typically today rather than writing in assembly language, programmers will use a <a href="/wiki/Disassembler" title="Disassembler">disassembler</a> to analyze the output of a compiler and change the high-level source code so that it can be compiled more efficiently, or understand why it is inefficient.</p>
<ul>
<li><b>Run time</b></li>
</ul>
<p><a href="/wiki/Just-in-time_compilation" title="Just-in-time compilation">Just-in-time</a> compilers can produce customized machine code based on run-time data, at the cost of compilation overhead. This technique dates to the earliest <a href="/wiki/Regular_expression" title="Regular expression">regular expression</a> engines, and has become widespread with Java HotSpot ond V8 for JavaScript. In some cases <a href="/wiki/Adaptive_optimization" title="Adaptive optimization">adaptive optimization</a> may be able to perform <a href="/wiki/Run_time_(program_lifecycle_phase)" title="Run time (program lifecycle phase)">run time</a> optimization exceeding the capability of static compilers by dynamically adjusting parameters according to the actual input or other factors.</p>
<p><a href="/wiki/Profile-guided_optimization" title="Profile-guided optimization">Profile-guided optimization</a> is an ahead-of-time (AOT) compilation optimization technique based on runtime profiles, and is similar to a static "average case" analog of the dynamic techinque of adaptive optimization.</p>
<p><a href="/wiki/Self-modifying_code" title="Self-modifying code">Self-modifying code</a> can alter itself in response to run time conditions in order to optimize code; this was more common in assembly language programs.</p>
<p>Some <a href="/wiki/CPU_design" title="CPU design" class="mw-redirect">CPU designs</a> can perform some optimizations at runtime. Some examples include <a href="/wiki/Out-of-order_execution" title="Out-of-order execution">Out-of-order execution</a>, <a href="/wiki/Speculative_execution" title="Speculative execution">Speculative execution</a>, <a href="/wiki/Instruction_pipeline" title="Instruction pipeline">Instruction pipelines</a>, and <a href="/wiki/Branch_predictor" title="Branch predictor">Branch predictors</a>. Compilers can help the program take advantage of these CPU features, for example through <a href="/wiki/Instruction_scheduling" title="Instruction scheduling">instruction scheduling</a>.</p>
<h3><span class="mw-headline" id="Platform_dependent_and_independent_optimizations">Platform dependent and independent optimizations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=3" title="Edit section: Platform dependent and independent optimizations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Code optimization can be also broadly categorized as <a href="/wiki/Computer_platform" title="Computer platform" class="mw-redirect">platform</a>-dependent and platform-independent techniques. While the latter ones are effective on most or all platforms, platform-dependent techniques use specific properties of one platform, or rely on parameters depending on the single platform or even on the single processor. Writing or producing different versions of the same code for different processors might therefore be needed. For instance, in the case of compile-level optimization, platform-independent techniques are generic techniques (such as <a href="/wiki/Loop_unwinding" title="Loop unwinding" class="mw-redirect">loop unrolling</a>, reduction in function calls, memory efficient routines, reduction in conditions, etc.), that impact most CPU architectures in a similar way. Generally, these serve to reduce the total <a href="/wiki/Instruction_path_length" title="Instruction path length">instruction path length</a> required to complete the program and/or reduce total memory usage during the process. On the other hand, platform-dependent techniques involve instruction scheduling, instruction-level parallelism, data-level parallelism, cache optimization techniques (i.e., parameters that differ among various platforms) and the optimal instruction scheduling might be different even on different processors of the same architecture.</p>
<h2><span class="mw-headline" id="Trade-offs">Trade-offs</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=4" title="Edit section: Trade-offs">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In some cases, however, optimization relies on using more elaborate algorithms, making use of "special cases" and special "tricks" and performing complex trade-offs. A "fully optimized" program might be more difficult to comprehend and hence may contain more <a href="/wiki/Software_bug" title="Software bug">faults</a> than unoptimized versions. Beyond eliminating obvious antipatterns, some code level optimizations decrease maintainability.</p>
<h2><span class="mw-headline" id="Strength_reduction">Strength reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=5" title="Edit section: Strength reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Computational tasks can be performed in several different ways with varying efficiency. A more efficient version with equivalent functionality is known as a <a href="/wiki/Strength_reduction" title="Strength reduction">strength reduction</a>. For example, consider the following <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> code snippet whose intention is to obtain the sum of all integers from 1 to N:</p>
<p>WHATSON? 3ef25dc4-5179-4e24-a8e2-35aa0c56117b</p>
<pre class="de1">
<span class="kw4">int</span> i<span class="sy0">,</span> sum <span class="sy0">=</span> <span class="nu0">0</span><span class="sy0">;</span>
<span class="kw1">for</span> <span class="br0">(</span>i <span class="sy0">=</span> <span class="nu0">1</span><span class="sy0">;</span> i <span class="sy0">&lt;=</span> N<span class="sy0">;</span> <span class="sy0">++</span>i<span class="br0">)</span> <span class="br0">{</span>
  sum <span class="sy0">+=</span> i<span class="sy0">;</span>
<span class="br0">}</span>
<span class="kw3">printf</span><span class="br0">(</span><span class="st0">"sum: %d<span class="es1">\n</span>"</span><span class="sy0">,</span> sum<span class="br0">)</span><span class="sy0">;</span>
</pre>
<p>This code can (assuming no <a href="/wiki/Arithmetic_overflow" title="Arithmetic overflow">arithmetic overflow</a>) be rewritten using a mathematical formula like:</p>
<p>WHATSON? 8628f3d8-a3cc-4f55-902b-c0b5cf116271</p>
<pre class="de1">
<span class="kw4">int</span> sum <span class="sy0">=</span> N <span class="sy0">*</span> <span class="br0">(</span><span class="nu0">1</span> <span class="sy0">+</span> N<span class="br0">)</span> <span class="sy0">/</span> <span class="nu0">2</span><span class="sy0">;</span>
<span class="kw3">printf</span><span class="br0">(</span><span class="st0">"sum: %d<span class="es1">\n</span>"</span><span class="sy0">,</span> sum<span class="br0">)</span><span class="sy0">;</span>
</pre>
<p>The optimization, sometimes performed automatically by an optimizing compiler, is to select a method (<a href="/wiki/Algorithm" title="Algorithm">algorithm</a>) that is more computationally efficient, while retaining the same functionality. See <a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">algorithmic efficiency</a> for a discussion of some of these techniques. However, a significant improvement in performance can often be achieved by removing extraneous functionality.</p>
<p>Optimization is not always an obvious or intuitive process. In the example above, the "optimized" version might actually be slower than the original version if N were sufficiently small and the particular hardware happens to be much faster at performing addition and <a href="/wiki/Loop_(computing)#Loops" title="Loop (computing)" class="mw-redirect">looping</a> operations than multiplication and division.</p>
<h2><span class="mw-headline" id="Trade-offs_2"><span id="Trade-offs">Trade-offs</span></span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=6" title="Edit section: Trade-offs">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Optimization will generally focus on improving just one or two aspects of performance: execution time, memory usage, disk space, bandwidth, power consumption or some other resource. This will usually require a trade-off — where one factor is optimized at the expense of others. For example, increasing the size of <a href="/wiki/Cache_(computing)" title="Cache (computing)">cache</a> improves runtime performance, but also increases the memory consumption. Other common trade-offs include code clarity and conciseness.</p>
<p>There are instances where the programmer performing the optimization must decide to make the software better for some operations but at the cost of making other operations less efficient. These trade-offs may sometimes be of a non-technical nature — such as when a competitor has published a <a href="/wiki/Benchmark_(computing)" title="Benchmark (computing)">benchmark</a> result that must be beaten in order to improve commercial success but comes perhaps with the burden of making normal usage of the software less efficient. Such changes are sometimes jokingly referred to as <i>pessimizations</i>.</p>
<h2><span class="mw-headline" id="Bottlenecks">Bottlenecks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=7" title="Edit section: Bottlenecks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Optimization may include finding a <a href="/wiki/Bottleneck_(engineering)" title="Bottleneck (engineering)" class="mw-redirect">bottleneck</a> in a system – a component that is the limiting factor on performance. In terms of code, this will often be a <a href="/wiki/Hot_spot_(computer_science)" title="Hot spot (computer science)" class="mw-redirect">hot spot</a> – a critical part of the code that is the primary consumer of the needed resource – though it can be another factor, such as I/O latency or network bandwidth.</p>
<p>In computer science, resource consumption often follows a form of <a href="/wiki/Power_law" title="Power law">power law</a> distribution, and the <a href="/wiki/Pareto_principle" title="Pareto principle">Pareto principle</a> can be applied to resource optimization by observing that 80% of the resources are typically used by 20% of the operations.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup> In software engineering, it is often a better approximation that 90% of the execution time of a computer program is spent executing 10% of the code (known as the 90/10 law in this context).</p>
<p>More complex algorithms and data structures perform well with many items, while simple algorithms are more suitable for small amounts of data — the setup, initialization time, and constant factors of the more complex algorithm can outweigh the benefit, and thus a <a href="/wiki/Hybrid_algorithm" title="Hybrid algorithm">hybrid algorithm</a> or <a href="/wiki/Adaptive_algorithm" title="Adaptive algorithm">adaptive algorithm</a> may be faster than any single algorithm.</p>
<p>In some cases, adding more <a href="/wiki/Main_memory" title="Main memory" class="mw-redirect">memory</a> can help to make a program run faster. For example, a filtering program will commonly read each line and filter and output that line immediately. This only uses enough memory for one line, but performance is typically poor, due to the latency of each disk read. Performance can be greatly improved by reading the entire file then writing the filtered result, though this uses much more memory. Caching the result is similarly effective, though also requiring larger memory use.</p>
<h2><span class="mw-headline" id="When_to_optimize">When to optimize</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=8" title="Edit section: When to optimize">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Optimization can reduce <a href="/wiki/Readability" title="Readability">readability</a> and add code that is used only to improve the <a href="/wiki/Computer_performance" title="Computer performance">performance</a>. This may complicate programs or systems, making them harder to maintain and debug. As a result, optimization or performance tuning is often performed at the end of the <a href="/wiki/Development_stage" title="Development stage" class="mw-redirect">development stage</a>.</p>
<p><a href="/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a> made the following two statements on optimization:</p>
<p>"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil"<sup id="cite_ref-autogenerated268_3-0" class="reference"><a href="#cite_note-autogenerated268-3"><span>[</span>3<span>]</span></a></sup></p>
<p>"In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal and I believe the same viewpoint should prevail in software engineering"<sup id="cite_ref-autogenerated268_3-1" class="reference"><a href="#cite_note-autogenerated268-3"><span>[</span>3<span>]</span></a></sup></p>
<p>"Premature optimization" is a phrase used to describe a situation where a programmer lets performance considerations affect the design of a piece of code. This can result in a design that is not as clean as it could have been or code that is incorrect, because the code is complicated by the optimization and the programmer is distracted by optimizing.</p>
<p>When deciding whether to optimize a specific part of the program, <a href="/wiki/Amdahl%27s_Law" title="Amdahl's Law" class="mw-redirect">Amdahl's Law</a> should always be considered: the impact on the overall program depends very much on how much time is actually spent in that specific part, which is not always clear from looking at the code without a <a href="/wiki/Profiling_(computer_programming)" title="Profiling (computer programming)">performance analysis</a>.</p>
<p>A better approach is therefore to design first, code from the design and then <a href="/wiki/Profiling_(computer_programming)" title="Profiling (computer programming)">profile</a>/<a href="/wiki/Benchmark_(computing)" title="Benchmark (computing)">benchmark</a> the resulting code to see which parts should be optimized. A simple and elegant design is often easier to optimize at this stage, and profiling may reveal unexpected performance problems that would not have been addressed by premature optimization.</p>
<p>In practice, it is often necessary to keep performance goals in mind when first designing software, but the programmer balances the goals of design and optimization.</p>
<h2><span class="mw-headline" id="Macros">Macros</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=9" title="Edit section: Macros">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Optimization during code development using <a href="/wiki/Macro_(computer_science)" title="Macro (computer science)">macros</a> takes on different forms in different languages.</p>
<p>In some procedural languages, such as <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> and <a href="/wiki/C%2B%2B" title="C++">C++</a>, macros are implemented using token substitution. Nowadays, <a href="/wiki/Inline_function" title="Inline function">inline functions</a> can be used as a <a href="/wiki/Type_safe" title="Type safe" class="mw-redirect">type safe</a> alternative in many cases. In both cases, the inlined function body can then undergo further compile-time optimizations by the compiler, including <a href="/wiki/Constant_folding" title="Constant folding">constant folding</a>, which may move some computations to compile time.</p>
<p>In many <a href="/wiki/Functional_programming" title="Functional programming">functional programming</a> languages macros are implemented using parse-time substitution of parse trees/abstract syntax trees, which it is claimed makes them safer to use. Since in many cases interpretation is used, that is one way to ensure that such computations are only performed at parse-time, and sometimes the only way.</p>
<p><a href="/wiki/Lisp_programming_language" title="Lisp programming language" class="mw-redirect">Lisp</a> originated this style of macro,<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2008)">citation needed</span></a></i>]</sup> and such macros are often called "Lisp-like macros." A similar effect can be achieved by using <a href="/wiki/Template_metaprogramming" title="Template metaprogramming">template metaprogramming</a> in <a href="/wiki/C%2B%2B" title="C++">C++</a>.</p>
<p>In both cases, work is moved to compile-time. The difference between <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> macros on one side, and Lisp-like macros and <a href="/wiki/C%2B%2B" title="C++">C++</a> <a href="/wiki/Template_metaprogramming" title="Template metaprogramming">template metaprogramming</a> on the other side, is that the latter tools allow performing arbitrary computations at compile-time/parse-time, while expansion of <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> macros does not perform any computation, and relies on the optimizer ability to perform it. Additionally, <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> macros do not directly support <a href="/wiki/Recursion_(computer_science)" title="Recursion (computer science)">recursion</a> or <a href="/wiki/Iteration" title="Iteration">iteration</a>, so are not <a href="/wiki/Turing_complete" title="Turing complete" class="mw-redirect">Turing complete</a>.</p>
<p>As with any optimization, however, it is often difficult to predict where such tools will have the most impact before a project is complete.</p>
<h2><span class="mw-headline" id="Automated_and_manual_optimization">Automated and manual optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=10" title="Edit section: Automated and manual optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>See also <a href="/wiki/Category:Compiler_optimizations" title="Category:Compiler optimizations">Category:Compiler optimizations</a></i></p>
<p>Optimization can be automated by compilers or performed by programmers. Gains are usually limited for local optimization, and larger for global optimizations. Usually, the most powerful optimization is to find a superior <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>.</p>
<p>Optimizing a whole system is usually undertaken by programmers because it is too complex for automated optimizers. In this situation, programmers or system administrators explicitly change code so that the overall system performs better. Although it can produce better efficiency, it is far more expensive than automated optimizations.</p>
<p>Use a <a href="/wiki/Profiler_(computer_science)" title="Profiler (computer science)" class="mw-redirect">profiler</a> (or <a href="/wiki/Profiling_(computer_programming)" title="Profiling (computer programming)">performance analyzer</a>) to find the sections of the program that are taking the most resources — the <i>bottleneck</i>. Programmers sometimes believe they have a clear idea of where the bottleneck is, but intuition is frequently wrong.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (May 2012)">citation needed</span></a></i>]</sup> Optimizing an unimportant piece of code will typically do little to help the overall performance.</p>
<p>When the bottleneck is localized, optimization usually starts with a rethinking of the algorithm used in the program. More often than not, a particular algorithm can be specifically tailored to a particular problem, yielding better performance than a generic algorithm. For example, the task of sorting a huge list of items is usually done with a <a href="/wiki/Quicksort" title="Quicksort">quicksort</a> routine, which is one of the most efficient generic algorithms. But if some characteristic of the items is exploitable (for example, they are already arranged in some particular order), a different method can be used, or even a custom-made sort routine.</p>
<p>After the programmer is reasonably sure that the best algorithm is selected, code optimization can start. Loops can be unrolled (for lower loop overhead, although this can often lead to <i>lower</i> speed if it overloads the <a href="/wiki/CPU_cache" title="CPU cache">CPU cache</a>), data types as small as possible can be used, integer arithmetic can be used instead of floating-point, and so on. (See <a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">algorithmic efficiency</a> article for these and other techniques.)</p>
<p>Performance bottlenecks can be due to language limitations rather than algorithms or data structures used in the program. Sometimes, a critical part of the program can be re-written in a different <a href="/wiki/Programming_language" title="Programming language">programming language</a> that gives more direct access to the underlying machine. For example, it is common for very <a href="/wiki/High-level_programming_language" title="High-level programming language">high-level</a> languages like <a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a> to have modules written in <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> for greater speed. Programs already written in C can have modules written in <a href="/wiki/Assembly_language" title="Assembly language">assembly</a>. Programs written in <a href="/wiki/D_programming_language" title="D programming language" class="mw-redirect">D</a> can use the <a href="/wiki/Inline_assembler" title="Inline assembler">inline assembler</a>.</p>
<p>Rewriting sections "pays off" in these circumstances because of a general "<a href="/wiki/Rule_of_thumb" title="Rule of thumb">rule of thumb</a>" known as the <a href="/wiki/90/10_law" title="90/10 law" class="mw-redirect">90/10 law</a>, which states that 90% of the time is spent in 10% of the code, and only 10% of the time in the remaining 90% of the code. So, putting intellectual effort into optimizing just a small part of the program can have a huge effect on the overall speed — if the correct part(s) can be located.</p>
<p>Manual optimization sometimes has the side effect of undermining readability. Thus code optimizations should be carefully documented (preferably using in-line comments), and their effect on future development evaluated.</p>
<p>The program that performs an automated optimization is called an <b>optimizer</b>. Most optimizers are embedded in compilers and operate during compilation. Optimizers can often tailor the generated code to specific processors.</p>
<p>Today, automated optimizations are almost exclusively limited to <a href="/wiki/Compiler_optimization" title="Compiler optimization" class="mw-redirect">compiler optimization</a>. However, because compiler optimizations are usually limited to a fixed set of rather general optimizations, there is considerable demand for optimizers which can accept descriptions of problem and language-specific optimizations, allowing an engineer to specify custom optimizations. Tools that accept descriptions of optimizations are called <a href="/wiki/Program_transformation" title="Program transformation">program transformation</a> systems and are beginning to be applied to real software systems such as C++.</p>
<p>Some high-level languages (<a href="/wiki/Eiffel_(programming_language)" title="Eiffel (programming language)">Eiffel</a>, <a href="/wiki/Esterel" title="Esterel">Esterel</a>) optimize their programs by using an <a href="/wiki/Intermediate_language" title="Intermediate language">intermediate language</a>.</p>
<p><a href="/wiki/Grid_computing" title="Grid computing">Grid computing</a> or <a href="/wiki/Distributed_computing" title="Distributed computing">distributed computing</a> aims to optimize the whole system, by moving tasks from computers with high usage to computers with idle time.</p>
<h2><span class="mw-headline" id="Time_taken_for_optimization">Time taken for optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=11" title="Edit section: Time taken for optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Sometimes, the time taken to undertake optimization therein itself may be an issue.</p>
<p>Optimizing existing code usually does not add new features, and worse, it might add new <a href="/wiki/Software_bug" title="Software bug">bugs</a> in previously working code (as any change might). Because manually optimized code might sometimes have less "readability" than unoptimized code, optimization might impact maintainability of it as well. Optimization comes at a price and it is important to be sure that the investment is worthwhile.</p>
<p>An automatic optimizer (or <a href="/wiki/Optimizing_compiler" title="Optimizing compiler">optimizing compiler</a>, a program that performs code optimization) may itself have to be optimized, either to further improve the efficiency of its target programs or else speed up its own operation. A compilation performed with optimization "turned on" usually takes longer, although this is usually only a problem when programs are quite large.</p>
<p>In particular, for <a href="/wiki/Just-in-time_compiler" title="Just-in-time compiler" class="mw-redirect">just-in-time compilers</a> the performance of the <a href="/wiki/Run_time_environment" title="Run time environment" class="mw-redirect">run time</a> compile component, executing together with its target code, is the key to improving overall execution speed.</p>
<h2><span class="mw-headline" id="Quotes">Quotes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=12" title="Edit section: Quotes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><i>"The order in which the operations shall be performed in every particular case is a very interesting and curious question, on which our space does not permit us fully to enter. In almost every computation a great variety of arrangements for the succession of the processes is possible, and various considerations must influence the selection amongst them for the purposes of a Calculating Engine. One essential object is to choose that arrangement which shall tend to reduce to a minimum the time necessary for completing the calculation."</i> — <a href="/wiki/Ada_Byron%27s_notes_on_the_analytical_engine" title="Ada Byron's notes on the analytical engine" class="mw-redirect">Ada Byron's notes on the analytical engine</a> 1842.</li>
<li><i>"More computing sins are committed in the name of efficiency (without necessarily achieving it) than for any other single reason — including blind stupidity."</i> — <a href="/wiki/W.A._Wulf" title="W.A. Wulf" class="mw-redirect">W.A. Wulf</a></li>
<li><i>"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified"</i><sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup> — <a href="/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a></li>
<li><i>"Bottlenecks occur in surprising places, so don't try to second guess and put in a speed hack until you have proven that's where the bottleneck is."</i> — <a href="/wiki/Rob_Pike" title="Rob Pike">Rob Pike</a></li>
<li><i>"The First Rule of Program Optimization: Don't do it. The Second Rule of Program Optimization (for experts only!): Don't do it yet."</i> — <a href="/wiki/Michael_A._Jackson" title="Michael A. Jackson">Michael A. Jackson</a></li>
</ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=13" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Michael_Abrash" title="Michael Abrash">Abrash Assembly Language Optimization</a></li>
<li><a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">Algorithmic efficiency</a></li>
<li><a href="/wiki/Abstract_interpretation" title="Abstract interpretation">Abstract interpretation</a></li>
<li><a href="/wiki/Cache_(computing)" title="Cache (computing)">Cache (computing)</a></li>
<li><a href="/wiki/Control_flow_graph" title="Control flow graph">Control flow graph</a></li>
<li><a href="/wiki/Lazy_evaluation" title="Lazy evaluation">Lazy evaluation</a></li>
<li><a href="/wiki/Loop_optimization" title="Loop optimization">Loop optimization</a></li>
<li><a href="/wiki/Low_level_virtual_machine" title="Low level virtual machine" class="mw-redirect">Low level virtual machine</a></li>
<li><a href="/wiki/Memoization" title="Memoization">Memoization</a></li>
<li><a href="/wiki/Memory_locality" title="Memory locality" class="mw-redirect">Memory locality</a></li>
<li><a href="/wiki/Parallel_computing" title="Parallel computing">Parallel processing</a></li>
<li><a href="/wiki/Profiling_(computer_programming)" title="Profiling (computer programming)">Performance analysis</a> (profiling)</li>
<li><a href="/wiki/Performance_tuning" title="Performance tuning">Performance tuning</a></li>
<li><a href="/wiki/Queueing_theory" title="Queueing theory">Queueing theory</a></li>
<li><a href="/wiki/Short-circuit_evaluation" title="Short-circuit evaluation">Short-circuit evaluation</a></li>
<li><a href="/wiki/Computer_simulation" title="Computer simulation">Simulation</a></li>
<li><a href="/wiki/Speculative_execution" title="Speculative execution">Speculative execution</a></li>
<li><a href="/wiki/SSA_form" title="SSA form" class="mw-redirect">SSA form</a></li>
<li><a href="/wiki/Worst-case_execution_time" title="Worst-case execution time">Worst-case execution time</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=14" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Jon_Bentley" title="Jon Bentley">Jon Bentley</a>: <i>Writing Efficient Programs</i>, <a href="/wiki/Special:BookSources/0139702512" class="internal mw-magiclink-isbn">ISBN 0-13-970251-2</a>.</li>
<li><a href="/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a>: <i><a href="/wiki/The_Art_of_Computer_Programming" title="The Art of Computer Programming">The Art of Computer Programming</a></i></li>
</ul>
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><a href="/wiki/Robert_Sedgewick_(computer_scientist)" title="Robert Sedgewick (computer scientist)">Robert Sedgewick</a>, <i>Algorithms</i>, 1984, p. 84</span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><span class="citation book">Wescott, Bob (2013). <a rel="nofollow" class="external text" href="http://www.amazon.com/Every-Computer-Performance-Book-Computers/dp/1482657759/"><i>The Every Computer Performance Book, Chapter 3: Useful laws</i></a>. <a href="/wiki/CreateSpace" title="CreateSpace" class="mw-redirect">CreateSpace</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/1482657759" title="Special:BookSources/1482657759">1482657759</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+optimization&amp;rft.aufirst=Bob&amp;rft.aulast=Wescott&amp;rft.au=Wescott%2C+Bob&amp;rft.btitle=The+Every+Computer+Performance+Book%2C+Chapter+3%3A+Useful+laws&amp;rft.date=2013&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.amazon.com%2FEvery-Computer-Performance-Book-Computers%2Fdp%2F1482657759%2F&amp;rft.isbn=1482657759&amp;rft.pub=CreateSpace&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-autogenerated268-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-autogenerated268_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-autogenerated268_3-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><span class="citation journal">Knuth, Donald (December 1974). "Structured Programming with go to Statements". <i>ACM Journal <b>Computing Surveys</b></i> <b>6</b> (4): 268. <a href="/wiki/CiteSeer#CiteSeerX" title="CiteSeer">CiteSeerX</a>: <span class="url"><a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.103.6084">10.1.1.103.6084</a></span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+optimization&amp;rft.atitle=Structured+Programming+with+go+to+Statements&amp;rft.aufirst=Donald&amp;rft.au=Knuth%2C+Donald&amp;rft.aulast=Knuth&amp;rft.date=December+1974&amp;rft.genre=article&amp;rft.issue=4&amp;rft.jtitle=ACM+Journal+%27%27%27Computing+Surveys%27%27%27&amp;rft.pages=268&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=6" class="Z3988"><span style="display:none;"> </span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><i>The Errors of Tex</i>, in <i>Software—Practice &amp; Experience</i>, Volume 19, Issue 7 (July 1989), pp. 607–685, reprinted in his book Literate Programming (p. 276)</span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://hans.gerwitz.com/2004/08/12/premature-optimization-is-the-root-of-all-evil.html">Tony Hoare, a 2004 email</a></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><a href="/wiki/Donald_Knuth" title="Donald Knuth">Knuth, Donald</a>: <a rel="nofollow" class="external text" href="http://pplab.snu.ac.kr/courses/adv_pl05/papers/p261-knuth.pdf">Structured Programming with Goto Statements</a>. <i>Computing Surveys</i> <b>6</b>:4 (1974), 261–301.</span></li>
</ol>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Program_optimization&amp;action=edit&amp;section=15" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a rel="nofollow" class="external text" href="http://www.agner.org/optimize/optimizing_cpp.pdf">Optimizing software in C++</a></li>
<li><a rel="nofollow" class="external text" href="http://web.archive.org/web/20080729033434/http://www.abarnett.demon.co.uk/tutorial.html">C optimization tutorial</a></li>
<li><a rel="nofollow" class="external text" href="http://www.research.scea.com/research/pdfs/GDC2003_Memory_Optimization_18Mar03.pdf">Memory optimization</a> by Christer Ericson</li>
<li><a rel="nofollow" class="external text" href="http://www.agner.org/optimize/">Optimization manuals for the x86 and x86-64 family microprocessors</a></li>
<li><a rel="nofollow" class="external text" href="http://www.ece.cmu.edu/~franzf/papers/gttse07.pdf">How To Write Fast Numerical Code: A Small Introduction</a></li>
<li><a rel="nofollow" class="external text" href="http://www.cs.arizona.edu/solar/">Software Optimization at Link-time And Run-time</a></li>
<li>Article "<a rel="nofollow" class="external text" href="http://doi.ieeecomputersociety.org/10.1109/2.348001">A Plea for Lean Software</a>" by <a href="/wiki/Niklaus_Wirth" title="Niklaus Wirth">Niklaus Wirth</a></li>
<li><a rel="nofollow" class="external text" href="http://c2.com/cgi/wiki?CategoryOptimization">Description from the Portland Pattern Repository</a></li>
<li><a rel="nofollow" class="external text" href="http://www.daemon.be/maarten/ipperf.html">Performance tuning of Computer Networks</a></li>
<li><a rel="nofollow" class="external text" href="http://www.thinkingparallel.com/2006/08/07/my-views-on-high-level-optimization/">An article describing high-level optimization</a></li>
<li><a rel="nofollow" class="external text" href="http://people.redhat.com/drepper/cpumemory.pdf">"What Every Programmer Should Know About Memory"</a> by Ulrich Drepper — explains the structure of modern memory subsystems and suggests how to utilize them efficiently</li>
<li><a rel="nofollow" class="external text" href="http://icl.cs.utk.edu/~mucci/latest/pubs/Notur2009-new.pdf">"Linux Multicore Performance Analysis and Optimization in a Nutshell"</a>, presentation slides by Philip Mucci</li>
<li><a rel="nofollow" class="external text" href="http://www.azillionmonkeys.com/qed/optimize.html">Programming Optimization</a> by Paul Hsieh</li>
<li><a rel="nofollow" class="external text" href="http://www.bixoft.nl/english/hyde.htm">"Why learning assembler is still a good idea" By Randall Hyde</a></li>
<li><a rel="nofollow" class="external text" href="http://www.new-npac.org/projects/cdroms/cewes-1999-06-vol1/nhse/hpccsurvey/orgs/sgi/bentley.html">Writing efficient programs ("Bentley's Rules")</a> by <a href="/wiki/Jon_Bentley" title="Jon Bentley">Jon Bentley</a></li>
<li><a rel="nofollow" class="external text" href="http://dir.salon.com/story/tech/col/rose/2004/03/19/programmers_at_work/index1.html">"Why software still stinks"</a> an article by <a href="/wiki/Scott_Rosenberg_(journalist)" title="Scott Rosenberg (journalist)">Scott Rosenberg</a></li>
<li><a rel="nofollow" class="external text" href="http://www.bitcortex.com/2009/05/25/in-defense-of-efficient-computing/">"In Defense of Efficient Computing"</a> an article by Rod Furlan</li>
<li><a rel="nofollow" class="external text" href="http://queue.acm.org/detail.cfm?id=1117403">"Performance Anti-Patterns"</a> by Bart Smaalders</li>
</ul>
