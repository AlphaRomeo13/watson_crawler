<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Secant-method---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Secant method</h1>
<p>In numerical analysis, the <b>secant method</b> is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function <i>f</i>. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predated the latter by over 3,000 years.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 The method</li>
<li>2 Derivation of the method</li>
<li>3 Convergence</li>
<li>4 Comparison with other root-finding methods</li>
<li>5 Generalizations</li>
<li>6 A computational example</li>
<li>7 Notes</li>
<li>8 See also</li>
<li>9 References</li>
<li>10 External links</li>
</ul>
<p></p>
<h2>The method</h2>
<p>The secant method is defined by the recurrence relation</p>
<p>As can be seen from the recurrence relation, the secant method requires two initial values, <i>x</i><sub>0</sub> and <i>x</i><sub>1</sub>, which should ideally be chosen to lie close to the root.</p>
<h2>Derivation of the method</h2>
<p>Starting with initial values <img class="mwe-math-fallback-image-inline tex" alt="x_0" src="//upload.wikimedia.org/math/0/b/2/0b21a666a81629962ade8afd967826ed.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png">, we construct a line through the points <img class="mwe-math-fallback-image-inline tex" alt="(~x_0,~f(x_0)~)" src="//upload.wikimedia.org/math/1/5/b/15b180cd356fe67f9103b264b077d8a3.png"> and <img class="mwe-math-fallback-image-inline tex" alt="(~x_1,~f(x_1)~)" src="//upload.wikimedia.org/math/b/4/a/b4a3abb8b8973509e5eaa8be83eb0526.png">, as demonstrated in the picture on the right. In point-slope form, this line has the equation</p>
<p>We find the root of this line – the value of <img class="mwe-math-fallback-image-inline tex" alt="x" src="//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png"> such that <img class="mwe-math-fallback-image-inline tex" alt="y=0" src="//upload.wikimedia.org/math/f/a/b/fab37d6c4a697fe660387d3ff8e889a4.png"> – by solving the following equation for <img class="mwe-math-fallback-image-inline tex" alt="x" src="//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png">:</p>
<p>The solution is</p>
<p>We then use this new value of <img class="mwe-math-fallback-image-inline tex" alt="x" src="//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png"> as <img class="mwe-math-fallback-image-inline tex" alt="x_2" src="//upload.wikimedia.org/math/8/f/4/8f43fce8dbdf3c4f8d0ac91f0de1d43d.png"> and repeat the process using <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_2" src="//upload.wikimedia.org/math/8/f/4/8f43fce8dbdf3c4f8d0ac91f0de1d43d.png"> instead of <img class="mwe-math-fallback-image-inline tex" alt="x_0" src="//upload.wikimedia.org/math/0/b/2/0b21a666a81629962ade8afd967826ed.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png">. We continue this process, solving for <img class="mwe-math-fallback-image-inline tex" alt="x_3" src="//upload.wikimedia.org/math/a/4/f/a4f66ba447cf765d4612169b07207e8d.png">, <img class="mwe-math-fallback-image-inline tex" alt="x_4" src="//upload.wikimedia.org/math/a/a/5/aa51775bcd244abf06e709f0cd80e614.png">, etc., until we reach a sufficiently high level of precision (a sufficiently small difference between <img class="mwe-math-fallback-image-inline tex" alt="x_n" src="//upload.wikimedia.org/math/6/7/b/67b68721103b5a16194f4b3e3ec222db.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_{n-1}" src="//upload.wikimedia.org/math/3/2/0/320eb171f9774f65eafb058ec460bf39.png">).</p>
<h2>Convergence</h2>
<p>The iterates <img class="mwe-math-fallback-image-inline tex" alt="x_n" src="//upload.wikimedia.org/math/6/7/b/67b68721103b5a16194f4b3e3ec222db.png"> of the secant method converge to a root of <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png">, if the initial values <img class="mwe-math-fallback-image-inline tex" alt="x_0" src="//upload.wikimedia.org/math/0/b/2/0b21a666a81629962ade8afd967826ed.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png"> are sufficiently close to the root. The order of convergence is α, where</p>
<p>is the golden ratio. In particular, the convergence is superlinear, but not quite quadratic.</p>
<p>This result only holds under some technical conditions, namely that <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png"> be twice continuously differentiable and the root in question be simple (i.e., with multiplicity 1).</p>
<p>If the initial values are not close enough to the root, then there is no guarantee that the secant method converges. There is no general definition of "close enough", but the criterion has to do with how "wiggly" the function is on the interval <img class="mwe-math-fallback-image-inline tex" alt="[~x_0,~x_1~]" src="//upload.wikimedia.org/math/8/3/1/831b01fe229d3bdbc6f122bf85357cc3.png">. For example, if <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png"> is differentiable on that interval and there is a point where <img class="mwe-math-fallback-image-inline tex" alt="f^\prime = 0" src="//upload.wikimedia.org/math/7/0/d/70d75f1a8d3a56204ac20fd3b18adc15.png"> on the interval, then the algorithm may not converge.</p>
<h2>Comparison with other root-finding methods</h2>
<p>The secant method does not require that the root remain bracketed like the bisection method does, and hence it does not always converge. The false position method (or <i>regula falsi</i>) uses the same formula as the secant method. However, it does not apply the formula on <img class="mwe-math-fallback-image-inline tex" alt="x_{n-1}" src="//upload.wikimedia.org/math/3/2/0/320eb171f9774f65eafb058ec460bf39.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_n" src="//upload.wikimedia.org/math/6/7/b/67b68721103b5a16194f4b3e3ec222db.png">, like the secant method, but on <img class="mwe-math-fallback-image-inline tex" alt="x_n" src="//upload.wikimedia.org/math/6/7/b/67b68721103b5a16194f4b3e3ec222db.png"> and on the last iterate <img class="mwe-math-fallback-image-inline tex" alt="x_k" src="//upload.wikimedia.org/math/4/2/b/42b7fa117615f02a4d47823ab2862a10.png"> such that <img class="mwe-math-fallback-image-inline tex" alt="f(x_k)" src="//upload.wikimedia.org/math/4/4/c/44c3de8d5dd7501d9ee902b84d4e71f1.png"> and <img class="mwe-math-fallback-image-inline tex" alt="f(x_n)" src="//upload.wikimedia.org/math/2/8/4/284cdcc646dec8d4bf03acb68693bf4b.png"> have a different sign. This means that the false position method always converges.</p>
<p>The recurrence formula of the secant method can be derived from the formula for Newton's method</p>
<p>by using the finite difference approximation</p>
<p>The secant method can be interpreted as a method in which the derivative is replaced by an approximation and is thus a Quasi-Newton method. If we compare Newton's method with the secant method, we see that Newton's method converges faster (order 2 against α ≈ 1.6). However, Newton's method requires the evaluation of both <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png"> and its derivative <img class="mwe-math-fallback-image-inline tex" alt="f^\prime" src="//upload.wikimedia.org/math/d/d/f/ddf5decba2390818cfa2ba29eb9e6333.png"> at every step, while the secant method only requires the evaluation of <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png">. Therefore, the secant method may occasionally be faster in practice. For instance, if we assume that evaluating <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png"> takes as much time as evaluating its derivative and we neglect all other costs, we can do two steps of the secant method (decreasing the logarithm of the error by a factor α² ≈ 2.6) for the same cost as one step of Newton's method (decreasing the logarithm of the error by a factor 2), so the secant method is faster. If however we consider parallel processing for the evaluation of the derivative, Newton's method proves its worth, being faster in time, though still spending more steps.</p>
<h2>Generalizations</h2>
<p>Broyden's method is a generalization of the secant method to more than one dimension.</p>
<p>The following graph shows the function <i>f</i> in red and the last secant line in bold blue. In the graph, the <i>x</i>-intercept of the secant line seems to be a good approximation of the root of <i>f</i>.</p>
<h2>A computational example</h2>
<p>The Secant method is applied to find a root of the function <i>f</i>(<i>x</i>)=<i>x</i>−612. Here is an implementation in the Matlab language. (From calculation, we expect that the iteration converges at x=24.7386)</p>
<p>WHATSON? 4e6bb2ae-7236-4c5a-8ec6-6b8aac5bc892</p>
<pre>
f=@(x) x^2 - 612;
x(1)=10;
x(2)=30;
for i=3:7
    x(i) = x(i-1) - (f(x(i-1)))*((x(i-1) - x(i-2))/(f(x(i-1)) - f(x(i-2))));
end
root=x(7)
</pre>
<h2>Notes</h2>
<ol>
<li><b>^</b> Papakonstantinou, J., <i>The Historical Development of the Secant Method in 1-D</i>, retrieved 2011-06-29 </li>
</ol>
<h2>See also</h2>
<ul>
<li>False position method</li>
</ul>
</body>
</html>