<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Binary-search-algorithm---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Binary search algorithm</h1>
<p>In computer science, a <b>binary search</b> or <b>half-interval search</b> algorithm finds the position of a specified input value (the search "key") within an array sorted by key value. For binary search, the array should be arranged in ascending or descending order. In each step, the algorithm compares the search key value with the key value of the middle element of the array. If the keys match, then a matching element has been found and its index, or position, is returned. Otherwise, if the search key is less than the middle element's key, then the algorithm repeats its action on the sub-array to the left of the middle element or, if the search key is greater, on the sub-array to the right. If the remaining array to be searched is empty, then the key cannot be found in the array and a special "not found" indication is returned.</p>
<p>A binary search halves the number of items to check with each iteration, so locating an item (or determining its absence) takes logarithmic time. A binary search is a dichotomic divide and conquer search algorithm.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Overview</li>
<li>2 Examples
<ul>
<li>2.1 Number guessing game</li>
<li>2.2 Word lists</li>
<li>2.3 Applications to complexity theory</li>
</ul>
</li>
<li>3 Algorithm
<ul>
<li>3.1 Recursive</li>
<li>3.2 Iterative</li>
<li>3.3 Deferred detection of equality</li>
</ul>
</li>
<li>4 Performance
<ul>
<li>4.1 Average performance</li>
</ul>
</li>
<li>5 Variations
<ul>
<li>5.1 Exclusive or inclusive bounds</li>
<li>5.2 Midpoint and width</li>
<li>5.3 Search domain</li>
<li>5.4 Noisy search</li>
<li>5.5 Exponential search</li>
<li>5.6 Interpolated search</li>
</ul>
</li>
<li>6 Implementation issues
<ul>
<li>6.1 Arithmetic</li>
</ul>
</li>
<li>7 Language support</li>
<li>8 See also</li>
<li>9 References
<ul>
<li>9.1 Other sources</li>
</ul>
</li>
<li>10 External links</li>
</ul>
<ul>
<li>2.1 Number guessing game</li>
<li>2.2 Word lists</li>
<li>2.3 Applications to complexity theory</li>
</ul>
<ul>
<li>3.1 Recursive</li>
<li>3.2 Iterative</li>
<li>3.3 Deferred detection of equality</li>
</ul>
<ul>
<li>4.1 Average performance</li>
</ul>
<ul>
<li>5.1 Exclusive or inclusive bounds</li>
<li>5.2 Midpoint and width</li>
<li>5.3 Search domain</li>
<li>5.4 Noisy search</li>
<li>5.5 Exponential search</li>
<li>5.6 Interpolated search</li>
</ul>
<ul>
<li>6.1 Arithmetic</li>
</ul>
<ul>
<li>9.1 Other sources</li>
</ul>
<p></p>
<h2>Overview</h2>
<p>Searching a sorted collection is a common task. A dictionary is a sorted list of word definitions. Given a word, one can find its definition. A telephone book is a sorted list of people's names, addresses, and telephone numbers. Knowing someone's name allows one to quickly find their telephone number and address.</p>
<p>If the list to be searched contains more than a few items (a dozen, say) a binary search will require far fewer comparisons than a linear search, but it imposes the requirement that the list be sorted. Similarly, a hash search can be faster than a binary search but imposes still greater requirements. If the contents of the array are modified between searches, maintaining these requirements may even take more time than the searches. And if it is known that some items will be searched for <i>much</i> more often than others, <i>and</i> it can be arranged so that these items are at the start of the list, then a linear search may be the best.</p>
<p>More generally, algorithm allows searching over argument of any monotonic function for a point, at which function reaches the arbitrary value (enclosed between minimum and maximum at the given range).</p>
<h2>Examples</h2>
<p>Example: The list to be searched: L = 1 3 4 6 8 9 11. The value to be found: X = 4.</p>
<p>WHATSON? c1169cf9-55be-4827-bc41-52ec9ae5fb0f</p>
<pre>
   Compare X to 6. X is smaller. Repeat with L = 1 3 4.
   Compare X to 3. X is bigger. Repeat with L = 4.
   Compare X to 4. They are equal. We're done, we found X.
</pre>
<p>This is called Binary Search: each iteration of (1)-(4) the length of the list we are looking in gets cut in half; therefore, the total number of iterations cannot be greater than logN.</p>
<h3>Number guessing game</h3>
<p>This rather simple game begins something like "I'm thinking of an integer between forty and sixty inclusive, and to your guesses I'll respond 'Higher', 'Lower', or 'Yes!' as might be the case." Supposing that <i>N</i> is the number of possible values (here, twenty-one, as "inclusive" was stated), then at most <img class="mwe-math-fallback-image-inline tex" alt="\lfloor\log_2 N\rfloor" src="//upload.wikimedia.org/math/b/c/9/bc91d70a4b6e6973a510741d4a460fd9.png"> questions are required to determine the number, since each question halves the search space. Note that one less question (iteration) is required than for the general algorithm, since the number is already constrained to be within a particular range.</p>
<p>Even if the number to guess can be arbitrarily large, in which case there is no upper bound <i>N</i>, the number can be found in at most <img class="mwe-math-fallback-image-inline tex" alt="2\lfloor \log_2 k \rfloor+1" src="//upload.wikimedia.org/math/c/5/9/c595e8659de776ec00b2493993abae0c.png"> steps (where <i>k</i> is the (unknown) selected number) by first finding an upper bound with one-sided binary search. For example, if the number were 11, the following sequence of guesses could be used to find it: 1 (Higher), 2 (Higher), 4 (Higher), 8 (Higher), 16 (Lower), 12 (Lower), 10 (Higher). Now we know that the number must be 11 because it is higher than 10 and lower than 12.</p>
<p>One could also extend the method to include negative numbers; for example the following guesses could be used to find −13: 0, −1, −2, −4, −8, −16, −12, −14. Now we know that the number must be −13 because it is lower than −12 and higher than −14.</p>
<h3>Word lists</h3>
<p>People typically use a mixture of the binary search and interpolative search algorithms when searching a telephone book, after the initial guess we exploit the fact that the entries are sorted and can rapidly find the required entry. For example when searching for Smith, if Rogers and Thomas have been found, one can flip to a page about halfway between the previous guesses. If this shows Samson, it can be concluded that Smith is somewhere between the Samson and Thomas pages so these can be divided.</p>
<h3>Applications to complexity theory</h3>
<p>Even if we do not know a fixed range the number <i>k</i> falls in, we can still determine its value by asking <img class="mwe-math-fallback-image-inline tex" alt="2\lceil\log_2k\rceil" src="//upload.wikimedia.org/math/1/c/e/1ce4822393603b990eb9f15a52191e92.png"> simple yes/no questions of the form "Is <i>k</i> greater than <i>x</i>?" for some number <i>x</i>. As a simple consequence of this, if you can answer the question "Is this integer property <i>k</i> greater than a given value?" in some amount of time then you can find the value of that property in the same amount of time with an added factor of <img class="mwe-math-fallback-image-inline tex" alt="\log_2 k" src="//upload.wikimedia.org/math/a/7/b/a7bf52f2342c237e54c0e37eb748db03.png">. This is called a <i>reduction</i>, and it is because of this kind of reduction that most complexity theorists concentrate on decision problems, algorithms that produce a simple yes/no answer.</p>
<p>For example, suppose we could answer "Does this <i>n</i> x <i>n</i> matrix have permanent larger than <i>k</i>?" in O(<i>n</i>) time. Then, by using binary search, we could find the (ceiling of the) permanent itself in O(<i>n</i> log <i>p</i>) time, where <i>p</i> is the value of the permanent. Notice that <i>p</i> is not the size of the input, but the <i>value</i> of the output; given a matrix whose maximum item (in absolute value) is <i>m</i>, <i>p</i> is bounded by <img class="mwe-math-fallback-image-inline tex" alt="m n n!" src="//upload.wikimedia.org/math/d/1/d/d1d6e57856332bda48471e91b9b4a4bd.png">. Hence log <i>p</i> = O(<i>n</i> log <i>n</i> + log <i>m</i>). A binary search could find the permanent in O(<i>n</i> log <i>n</i> + <i>n</i> log <i>m</i>).</p>
<h2>Algorithm</h2>
<h3>Recursive</h3>
<p>A straightforward implementation of binary search is recursive. The initial call uses the indices of the entire array to be searched. The procedure then calculates an index midway between the two indices, determines which of the two subarrays to search, and then does a recursive call to search that subarray. Each of the calls is tail recursive, so a compiler need not make a new stack frame for each call. The variables <code>imin</code> and <code>imax</code> are the lowest and highest inclusive indices that are searched.</p>
<p>WHATSON? 6254820d-1216-4432-a43a-d37772547a49</p>
<pre>
int binary_search(int A[], int key, int imin, int imax)
{
  // test if array is empty
  if (imax &lt; imin)
    // set is empty, so return value showing not found
    return KEY_NOT_FOUND;
  else
    {
      // calculate midpoint to cut set in half
      int imid = midpoint(imin, imax);
 
      // three-way comparison
      if (A[imid] &gt; key)
        // key is in lower subset
        return binary_search(A, key, imin, imid-1);
      else if (A[imid] &lt; key)
        // key is in upper subset
        return binary_search(A, key, imid+1, imax);
      else
        // key has been found
        return imid;
    }
}
</pre>
<p>It is invoked with initial <code>imin</code> and <code>imax</code> values of <code>0</code> and <code>N-1</code> for a zero based array of length N.</p>
<p>The number type "int" shown in the code has an influence on how the midpoint calculation can be implemented correctly. With unlimited numbers, the midpoint can be calculated as <code>"(imin + imax) / 2"</code>. In practical programming, however, the calculation is often performed with numbers of a limited range, and then the intermediate result <code>"(imin + imax)"</code> might overflow. With limited numbers, the midpoint can be calculated correctly as <code>"imin + ((imax - imin) / 2)"</code>.</p>
<h3>Iterative</h3>
<p>The binary search algorithm can also be expressed iteratively with two index limits that progressively narrow the search range.</p>
<p>WHATSON? cf64137a-1887-4c78-bdae-c0612f0a97f8</p>
<pre>
int binary_search(int A[], int key, int imin, int imax)
{
  // continue searching while [imin,imax] is not empty
  while (imax &gt;= imin)
    {
      // calculate the midpoint for roughly equal partition
      int imid = midpoint(imin, imax);
      if(A[imid] == key)
        // key found at index imid
        return imid; 
      // determine which subarray to search
      else if (A[imid] &lt; key)
        // change min index to search upper subarray
        imin = imid + 1;
      else         
        // change max index to search lower subarray
        imax = imid - 1;
    }
  // key was not found
  return KEY_NOT_FOUND;
}
</pre>
<h3>Deferred detection of equality</h3>
<p>The above iterative and recursive versions take three paths based on the key comparison: one path for less than, one path for greater than, and one path for equality. (There are two conditional branches.) The path for equality is taken only when the record is finally matched, so it is rarely taken. That branch path can be moved outside the search loop in the deferred test for equality version of the algorithm. The following algorithm uses only one conditional branch per iteration.</p>
<p>WHATSON? bde488e3-4519-4c6b-9d01-f245ce546c81</p>
<pre>
// inclusive indices
//   0 &lt;= imin when using truncate toward zero divide
//     imid = (imin+imax)/2;
//   imin unrestricted when using truncate toward minus infinity divide
//     imid = (imin+imax)&gt;&gt;1; or
//     imid = (int)floor((imin+imax)/2.0);
int binary_search(int A[], int key, int imin, int imax)
{
  // continually narrow search until just one element remains
  while (imin &lt; imax)
    {
      int imid = midpoint(imin, imax);
 
      // code must guarantee the interval is reduced at each iteration
      assert(imid &lt; imax);
      // note: 0 &lt;= imin &lt; imax implies imid will always be less than imax
 
      // reduce the search
      if (A[imid] &lt; key)
        imin = imid + 1;
      else
        imax = imid;
    }
  // At exit of while:
  //   if A[] is empty, then imax &lt; imin
  //   otherwise imax == imin
 
  // deferred test for equality
  if ((imax == imin) &amp;&amp; (A[imin] == key))
    return imin;
  else
    return KEY_NOT_FOUND;
}
</pre>
<p>The deferred detection approach foregoes the possibility of early termination on discovery of a match, so the search will take about log<sub>2</sub>(<i>N</i>) iterations. On average, a <i>successful</i> early termination search will not save many iterations. For large arrays that are a power of 2, the savings is about two iterations. Half the time, a match is found with one iteration left to go; one quarter the time with two iterations left, one eighth with three iterations, and so forth. The infinite series sum is 2.</p>
<p>The deferred detection algorithm has the advantage that if the keys are not unique, it returns the smallest index (the starting index) of the region where elements have the search key. The early termination version would return the first match it found, and that match might be anywhere in region of equal keys.</p>
<h2>Performance</h2>
<p>With each test that fails to find a match at the probed position, the search is continued with one or other of the two sub-intervals, each at most half the size. More precisely, if the number of items, <i>N</i>, is odd then both sub-intervals will contain (<i>N</i>−1)/2 elements, while if <i>N</i> is even then the two sub-intervals contain <i>N</i>/2−1 and <i>N</i>/2 elements.</p>
<p>If the original number of items is <i>N</i> then after the first iteration there will be at most <i>N</i>/2 items remaining, then at most <i>N</i>/4 items, at most <i>N</i>/8 items, and so on. In the worst case, when the value is not in the list, the algorithm must continue iterating until the span has been made empty; this will have taken at most ⌊log<sub>2</sub>(<i>N</i>)+1⌋ iterations, where the ⌊ ⌋ notation denotes the floor function that rounds its argument down to an integer. This worst case analysis is tight: for any <i>N</i> there exists a query that takes exactly ⌊log<sub>2</sub>(<i>N</i>)+1⌋ iterations. When compared to linear search, whose worst-case behaviour is <i>N</i> iterations, we see that binary search is substantially faster as <i>N</i> grows large. For example, to search a list of one million items takes as many as one million iterations with linear search, but never more than twenty iterations with binary search. However, a binary search can only be performed if the list is in sorted order.</p>
<h3>Average performance</h3>
<p>log<sub>2</sub>(<i>N</i>)−1 is the expected number of probes in an average successful search, and the worst case is log<sub>2</sub>(<i>N</i>), just one more probe. If the list is empty, no probes at all are made. Thus binary search is a logarithmic algorithm and executes in O(log <i>N</i>) time. In most cases it is considerably faster than a linear search. It can be implemented using iteration, or recursion. In some languages it is more elegantly expressed recursively; however, in some C-based languages tail recursion is not eliminated and the recursive version requires more stack space.</p>
<p>Binary search can interact poorly with the memory hierarchy (i.e. caching), because of its random-access nature. For in-memory searching, if the span to be searched is small, a linear search may have superior performance simply because it exhibits better locality of reference. For external searching, care must be taken or each of the first several probes will lead to a disk seek. A common method is to abandon binary searching for linear searching as soon as the size of the remaining span falls below a small value such as 8 or 16 or even more in recent computers. The exact value depends entirely on the machine running the algorithm.</p>
<p>Notice that for multiple searches <i>with a fixed value for N</i>, then (with the appropriate regard for integer division), the first iteration always selects the middle element at <i>N</i>/2, and the second always selects either <i>N</i>/4 or 3<i>N</i>/4, and so on. Thus if the array's key values are in some sort of slow storage (on a disc file, in virtual memory, not in the cpu's on-chip memory), keeping those three keys in a local array for a special preliminary search will avoid accessing widely separated memory. Escalating to seven or fifteen such values will allow further levels at not much cost in storage. On the other hand, if the searches are frequent and not separated by much other activity, the computer's various storage control features will more or less automatically promote frequently accessed elements into faster storage.</p>
<p>When multiple binary searches are to be performed for the same key in related lists, fractional cascading can be used to speed up successive searches after the first one.</p>
<p>Even though in theory binary search is almost always faster than linear search, in practice even on small arrays (around 64 items or less) it might be infeasible to ever use binary search. On large unsorted arrays, it only makes sense to binary search if the number of searches is large enough, because the initial time to sort the array is comparable to log(n) linear searches </p>
<h2>Variations</h2>
<p>There are many, and they are easily confused.</p>
<h3>Exclusive or inclusive bounds</h3>
<p>The most significant differences are between the "exclusive" and "inclusive" forms of the bounds. In the "exclusive" bound form the span to be searched is <i>(L+1)</i> to <i>(R−1)</i>, and this may seem clumsy when the span to be searched could be described in the "inclusive" form, as <i>L</i> to <i>R</i>. Although the details differ the two forms are equivalent as can be seen by transforming one version into the other. The inclusive bound form can be attained by replacing all appearances of "L" by "(L−1)" and "R" by "(R+1)" then rearranging. Thus, the initialisation of <i>L := 0</i> becomes <i>(L−1) := 0</i> or <i>L := 1</i>, and <i>R := N+1</i> becomes <i>(R+1) := N+1</i> or <i>R := N</i>. So far so good, but note now that the changes to <i>L</i> and <i>R</i> are no longer simply transferring the value of <i>p</i> to <i>L</i> or <i>R</i> as appropriate but now must be <i>(R+1) := p</i> or <i>R := p−1</i>, and <i>(L−1) := p</i> or <i>L := p+1</i>.</p>
<p>Thus, the gain of a simpler initialisation, done once, is lost by a more complex calculation, and which is done for every iteration. If that is not enough, the test for an empty span is more complex also, as compared to the simplicity of checking that the value of <i>p</i> is zero. Nevertheless, the inclusive bound form is found in many publications, such as Donald Knuth. <i>The Art of Computer Programming</i>, Volume 3: <i>Sorting and Searching</i>, Third Edition.</p>
<p>Another common variation uses inclusive bounds for the left bound, but exclusive bounds for the right bound. This is derived from the fact that the bounds in a language with zero-based arrays can be simply initialized to 0 and the size of the array, respectively. This mirrors the way array slices are represented in some programming languages.</p>
<h3>Midpoint and width</h3>
<p>A different variation involves abandoning the <i>L</i> and <i>R</i> pointers and using a current position <i>p</i> and a width <i>w</i>. At each iteration, the position <i>p</i> is adjusted and the width <i>w</i> is halved. Knuth states, "It is possible to do this, but only if extreme care is paid to the details."</p>
<h3>Search domain</h3>
<p>There is no particular requirement that the array being searched has the bounds 1 to <i>N</i>. It is possible to search a specified range, elements <i>first</i> to <i>last</i> instead of 1 to <i>N</i>. All that is necessary is that the initialization of the bounds be <i>L := first−1</i> and <i>R := last+1</i>, then all proceeds as before.</p>
<p>The elements of the list are not necessarily all unique. If one searches for a value that occurs multiple times in the list, the index returned will be of the first-encountered equal element, and this will not necessarily be that of the first, last, or middle element of the run of equal-key elements but will depend on the positions of the values. Modifying the list even in seemingly unrelated ways such as adding elements elsewhere in the list may change the result.</p>
<p>If the location of the first and/or last equal element needs to be determined, this can be done efficiently with a variant of the binary search algorithms which perform only one inequality test per iteration. See deferred detection of equality.</p>
<h3>Noisy search</h3>
<p>Several algorithms closely related to or extending binary search exist. For instance, <b>noisy binary search</b> solves the same class of projects as regular binary search, with the added complexity that any given test can return a false value at random. (Usually, the number of such erroneous results are bounded in some way, either in the form of an average error rate, or in the total number of errors allowed per element in the search space.) Optimal algorithms for several classes of noisy binary search problems have been known since the late seventies, and more recently, optimal algorithms for noisy binary search in quantum computers (where several elements can be tested at the same time) have been discovered.</p>
<h3>Exponential search</h3>
<p>An <b>exponential search</b> (also called a <b>one-sided search</b>) searches from a starting point within the array and either expects that the element <img class="mwe-math-fallback-image-inline tex" alt="p" src="//upload.wikimedia.org/math/8/3/8/83878c91171338902e0fe0fb97a8c47a.png"> being sought is nearby or the upper (lower) bound on the array is unknown. Starting with a step size of 1 and doubling with each step, the method looks for a number &gt;= (&lt;=) <img class="mwe-math-fallback-image-inline tex" alt="p" src="//upload.wikimedia.org/math/8/3/8/83878c91171338902e0fe0fb97a8c47a.png">. Once the upper (lower) bound is found, then the method proceeds with a binary search. The complexity of the search is <img class="mwe-math-fallback-image-inline tex" alt="2 \lceil\log_2 n\rceil" src="//upload.wikimedia.org/math/2/f/9/2f9d15f69a6d78feeba392f023694567.png"> if the sought element is in the <i>n</i>th array position. This depends only on <img class="mwe-math-fallback-image-inline tex" alt="n" src="//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png"> and not on the size of the array.</p>
<h3>Interpolated search</h3>
<p>An interpolated search tries to guess the location of the element <img class="mwe-math-fallback-image-inline tex" alt="p" src="//upload.wikimedia.org/math/8/3/8/83878c91171338902e0fe0fb97a8c47a.png"> you're searching for, typically by calculating a midpoint based on the lowest and highest value and assuming a fairly even distribution of values. When <img class="mwe-math-fallback-image-inline tex" alt="p" src="//upload.wikimedia.org/math/8/3/8/83878c91171338902e0fe0fb97a8c47a.png"> has been determined an exponential search is performed.</p>
<h2>Implementation issues</h2>
<p>Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky… — Donald Knuth</p>
<p>When Jon Bentley assigned it as a problem in a course for professional programmers, he found that an astounding ninety percent failed to code a binary search correctly after several hours of working on it, and another study shows that accurate code for it is only found in five out of twenty textbooks. Furthermore, Bentley's own implementation of binary search, published in his 1986 book <i>Programming Pearls</i>, contains an error that remained undetected for over twenty years.</p>
<h3>Arithmetic</h3>
<p>In a practical implementation, the variables used to represent the indices will often be of finite size, hence only capable of representing a finite range of values. For example, 32-bit unsigned integers can only hold values from 0 to 4294967295. 32-bit signed integers can only hold values from -2147483648 to 2147483647. If the binary search algorithm is to operate on large arrays, this has three implications:</p>
<ul>
<li>The values <code>first − 1</code> and <code>last + 1</code> must both be representable within the finite bounds of the chosen integer type . Therefore, continuing the 32-bit unsigned example, the largest value that <code>last</code> may take is +429496729<b>4</b>, not +429496729<b>5</b>. A problem exists even for the "inclusive" form of the method, as if <code>x &gt; A(4294967295).Key</code>, then on the final iteration the algorithm will attempt to store 4294967296 into <code>L</code> and fail. Equivalent issues apply to the lower limit, where <code>first − 1</code> could become negative as when the first element of the array is at index zero.</li>
<li>If the midpoint of the span is calculated as <code>p := (L + R)/2</code>, then the value <code>(L + R)</code> will exceed the number range if <code>last</code> is greater than (for unsigned) 4294967295/2 or (for signed) 2147483647/2 and the search wanders toward the upper end of the search space. This can be avoided by performing the calculation as <code>p := (R - L)/2 + L</code>. For example, this bug existed in Java SDK at <code>Arrays.binarySearch()</code> from 1.2 to 5.0 and was fixed in 6.0.</li>
<li>KEY_NOT_FOUND must be a valid value of the return type, but this value can never be an index of the array</li>
</ul>
<h2>Language support</h2>
<p>Many standard libraries provide a way to do a binary search:</p>
<ul>
<li>C provides algorithm function <tt>bsearch</tt> in its standard library.</li>
<li>C++'s STL provides algorithm functions <code>binary_search</code>, <code>lower_bound</code> and <code>upper_bound</code>.</li>
<li>Java offers a set of overloaded <code>binarySearch()</code> static methods in the classes <code>Arrays</code> and <code>Collections</code> in the standard <code>java.util</code> package for performing binary searches on Java arrays and on <code>List</code>s, respectively. They must be arrays of primitives, or the arrays or Lists must be of a type that implements the <code>Comparable</code> interface, or you must specify a custom <code>Comparator</code> object.</li>
<li>Microsoft's .NET Framework 2.0 offers static generic versions of the binary search algorithm in its collection base classes. An example would be <code>System.Array</code>'s method <code>BinarySearch&lt;T&gt;(T[] array, T value)</code>.</li>
<li>Python provides the <code>bisect</code> module.</li>
<li>COBOL can perform binary search on internal tables using the <code>SEARCH ALL</code> statement.</li>
<li>Perl can perform a generic binary search using the CPAN module Search::Binary.</li>
<li>Go's <code>sort</code> standard library package contains functions <code>Search</code>, <code>SearchInts</code>, <code>SearchFloat64s</code>, and <code>SearchStrings</code>, which implement general binary search, as well as specific implementations for searching slices of integers, floating-point numbers, and strings, respectively.</li>
<li>For Objective-C, the Cocoa framework provides the NSArray -indexOfObject:inSortedRange:options:usingComparator: method in Mac OS X 10.6+. Apple's Core Foundation C framework also contains a CFArrayBSearchValues() function.</li>
</ul>
<h2>See also</h2>
<ul>
<li>Interpolation search, similar method with better average complexity</li>
<li>Index (information technology), very fast 'lookup' using an index to directly select an entry</li>
<li>Branch table, alternative indexed 'lookup' method for decision making</li>
<li>Self-balancing binary search tree</li>
<li>Run-time analysis, illustrating binary search method on machines of differing speeds</li>
<li>Bisection method, the same idea used to solve equations in the real numbers</li>
</ul>
</body>
</html>