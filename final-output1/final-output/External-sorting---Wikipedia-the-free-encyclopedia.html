<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>External-sorting---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>External sorting</h1>
<p><b>External sorting</b> is a term for a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory (usually a hard drive). External sorting typically uses a hybrid sort-merge strategy. In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file. In the merge phase, the sorted subfiles are combined into a single larger file.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 External merge sort
<ul>
<li>1.1 Additional passes</li>
<li>1.2 Tuning performance</li>
</ul>
</li>
<li>2 Other algorithms</li>
<li>3 References</li>
<li>4 External links</li>
</ul>
<ul>
<li>1.1 Additional passes</li>
<li>1.2 Tuning performance</li>
</ul>
<p></p>
<h2>External merge sort</h2>
<p>One example of external sorting is the external merge sort algorithm, which sorts chunks that each fit in RAM, then merges the sorted chunks together. For example, for sorting 900 megabytes of data using only 100 megabytes of RAM:</p>
<h1>External sorting</h1>
<p><b>External sorting</b> is a term for a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory (usually a hard drive). External sorting typically uses a hybrid sort-merge strategy. In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file. In the merge phase, the sorted subfiles are combined into a single larger file.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 External merge sort
<ul>
<li>1.1 Additional passes</li>
<li>1.2 Tuning performance</li>
</ul>
</li>
<li>2 Other algorithms</li>
<li>3 References</li>
<li>4 External links</li>
</ul>
<ul>
<li>1.1 Additional passes</li>
<li>1.2 Tuning performance</li>
</ul>
<p></p>
<h2>External merge sort</h2>
<p>One example of external sorting is the external merge sort algorithm, which sorts chunks that each fit in RAM, then merges the sorted chunks together. For example, for sorting 900 megabytes of data using only 100 megabytes of RAM:</p>
<ol>
<li>Read 100 MB of the data in main memory and sort by some conventional method, like quicksort.</li>
<li>Write the sorted data to disk.</li>
<li>Repeat steps 1 and 2 until all of the data is in sorted 100 MB chunks (there are 900MB / 100MB = 9 chunks), which now need to be merged into one single output file.</li>
<li>Read the first 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into input buffers in main memory and allocate the remaining 10 MB for an output buffer. (In practice, it might provide better performance to make the output buffer larger and the input buffers slightly smaller.)</li>
<li>Perform a 9-way merge and store the result in the output buffer. Whenever the output buffer fills, write it to the final sorted file and empty it. Whenever any of the 9 input buffers empties, fill it with the next 10 MB of its associated 100 MB sorted chunk until no more data from the chunk is available. This is the key step that makes external merge sort work externally -- because the merge algorithm only makes one pass sequentially through each of the chunks, each chunk does not have to be loaded completely; rather, sequential parts of the chunk can be loaded as needed.</li>
</ol>
<h3>Additional passes</h3>
<ol>
<li>Read 100 MB of the data in main memory and sort by some conventional method, like quicksort.</li>
<li>Write the sorted data to disk.</li>
<li>Repeat steps 1 and 2 until all of the data is in sorted 100 MB chunks (there are 900MB / 100MB = 9 chunks), which now need to be merged into one single output file.</li>
<li>Read the first 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into input buffers in main memory and allocate the remaining 10 MB for an output buffer. (In practice, it might provide better performance to make the output buffer larger and the input buffers slightly smaller.)</li>
<li>Perform a 9-way merge and store the result in the output buffer. Whenever the output buffer fills, write it to the final sorted file and empty it. Whenever any of the 9 input buffers empties, fill it with the next 10 MB of its associated 100 MB sorted chunk until no more data from the chunk is available. This is the key step that makes external merge sort work externally -- because the merge algorithm only makes one pass sequentially through each of the chunks, each chunk does not have to be loaded completely; rather, sequential parts of the chunk can be loaded as needed.</li>
</ol>
<h3>Additional passes</h3>
<p>Above example shows a two-pass sort: a sort pass followed by a merge pass. Note that we had one merge pass that merged all the chunks at once, rather than in regular merge sort, where we merge two chunks at each step, and take <img class="mwe-math-fallback-png-inline tex" alt="\log n" src="//upload.wikimedia.org/math/0/d/2/0d2e858bd7f89eed5461e5637d6e0a50.png"> merge passes total. The reason for this is that every merge pass requires reading and writing <i>every value</i> in the array from and to disk once. Disk access is usually slow, and so reads and writes should be avoided as much as possible.</p>
<p>However, there is a trade-off with using fewer merge passes. As the number of chunks increases, the amount of data we can read from each chunk at a time during the merge process decreases. For sorting, say, 50 GB in 100 MB of RAM, using a single merge pass isn't efficient: the disk seeks required to fill the input buffers with data from each of the 500 chunks (we read 100MB / 501 ~ 200KB from each chunk at a time) take up most of the sort time. Using two merge passes solves the problem. Then the sorting process might look like this:</p>
<ol>
<li>Run the initial chunk-sorting pass as before.</li>
<li>Run a first merge pass combining 25 chunks at a time, resulting in 20 larger sorted chunks.</li>
<li>Run a second merge pass to merge the 20 larger sorted chunks.</li>
</ol>
<p>Like in-memory sorts, efficient external sorts require O(<i>n</i> log <i>n</i>) time: exponential increases in data size require linear increases in the number of passes. If one makes liberal use of the gigabytes of RAM provided by modern computers, the logarithmic factor grows very slowly: under reasonable assumptions, one could sort at least 500 GB of data using 1 GB of main memory before a third pass became advantageous, and could sort many times that before a fourth pass became useful.</p>
<p>Doubling the memory dedicated to sorting both allows the same amount of data to be sorted using half as many chunks <i>and</i> allows the merge phase to do half as many buffer-filling reads during the merging phase, potentially reducing the number of seeks required by about three-quarters. So, dedicating more RAM to sorting can be an effective way to increase speed if it allows reducing the number of passes, or if disk seek time accounts for a substantial part of sorting time.</p>
<h3>Tuning performance</h3>
<p>The Sort Benchmark, created by computer scientist Jim Gray, compares external sorting algorithms implemented using finely tuned hardware and software. Winning implementations use several techniques:</p>
<p>Above example shows a two-pass sort: a sort pass followed by a merge pass. Note that we had one merge pass that merged all the chunks at once, rather than in regular merge sort, where we merge two chunks at each step, and take <img class="mwe-math-fallback-png-inline tex" alt="\log n" src="//upload.wikimedia.org/math/0/d/2/0d2e858bd7f89eed5461e5637d6e0a50.png"> merge passes total. The reason for this is that every merge pass requires reading and writing <i>every value</i> in the array from and to disk once. Disk access is usually slow, and so reads and writes should be avoided as much as possible.</p>
<p>However, there is a trade-off with using fewer merge passes. As the number of chunks increases, the amount of data we can read from each chunk at a time during the merge process decreases. For sorting, say, 50 GB in 100 MB of RAM, using a single merge pass isn't efficient: the disk seeks required to fill the input buffers with data from each of the 500 chunks (we read 100MB / 501 ~ 200KB from each chunk at a time) take up most of the sort time. Using two merge passes solves the problem. Then the sorting process might look like this:</p>
<ol>
<li>Run the initial chunk-sorting pass as before.</li>
<li>Run a first merge pass combining 25 chunks at a time, resulting in 20 larger sorted chunks.</li>
<li>Run a second merge pass to merge the 20 larger sorted chunks.</li>
</ol>
<p>Like in-memory sorts, efficient external sorts require O(<i>n</i> log <i>n</i>) time: exponential increases in data size require linear increases in the number of passes. If one makes liberal use of the gigabytes of RAM provided by modern computers, the logarithmic factor grows very slowly: under reasonable assumptions, one could sort at least 500 GB of data using 1 GB of main memory before a third pass became advantageous, and could sort many times that before a fourth pass became useful.</p>
<p>Doubling the memory dedicated to sorting both allows the same amount of data to be sorted using half as many chunks <i>and</i> allows the merge phase to do half as many buffer-filling reads during the merging phase, potentially reducing the number of seeks required by about three-quarters. So, dedicating more RAM to sorting can be an effective way to increase speed if it allows reducing the number of passes, or if disk seek time accounts for a substantial part of sorting time.</p>
<h3>Tuning performance</h3>
<p>The Sort Benchmark, created by computer scientist Jim Gray, compares external sorting algorithms implemented using finely tuned hardware and software. Winning implementations use several techniques:</p>
<ul>
<li><b>Using parallelism</b>
<ul>
<li>Multiple disk drives can be used in parallel in order to improve sequential read and write speed. This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.</li>
<li>Sorting software can use multiple threads, to speed up the process on modern multicore computers.</li>
<li>Software can use asynchronous I/O so that one run of data can be sorted or merged while other runs are being read from or written to disk.</li>
<li>Multiple machines connected by fast network links can each sort part of a huge dataset in parallel.</li>
</ul>
</li>
<li><b>Increasing hardware speed</b>
<ul>
<li>Using more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.</li>
<li>Fast external memory, like 15K RPM disks or solid-state drives, can speed sorts (but adds substantial costs proportional to the data size).</li>
<li><i>Many</i> other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. "Balancing" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.</li>
<li>Cost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.</li>
</ul>
</li>
<li><b>Increasing software speed</b>
<ul>
<li>Some Sort Benchmark entrants use a variation on radix sort for the first phase of sorting: they separate data into one of many "bins" based on the beginning of its value. Sort Benchmark data is random and especially well-suited to this optimization.</li>
<li>Compacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.</li>
<li>Because the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.</li>
</ul>
</li>
</ul>
<ul>
<li>Multiple disk drives can be used in parallel in order to improve sequential read and write speed. This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.</li>
<li>Sorting software can use multiple threads, to speed up the process on modern multicore computers.</li>
<li>Software can use asynchronous I/O so that one run of data can be sorted or merged while other runs are being read from or written to disk.</li>
<li>Multiple machines connected by fast network links can each sort part of a huge dataset in parallel.</li>
</ul>
<ul>
<li>Using more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.</li>
<li>Fast external memory, like 15K RPM disks or solid-state drives, can speed sorts (but adds substantial costs proportional to the data size).</li>
<li><i>Many</i> other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. "Balancing" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.</li>
<li>Cost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.</li>
</ul>
<ul>
<li>Some Sort Benchmark entrants use a variation on radix sort for the first phase of sorting: they separate data into one of many "bins" based on the beginning of its value. Sort Benchmark data is random and especially well-suited to this optimization.</li>
<li>Compacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.</li>
<li>Because the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.</li>
</ul>
<ul>
<li><b>Using parallelism</b>
<ul>
<li>Multiple disk drives can be used in parallel in order to improve sequential read and write speed. This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.</li>
<li>Sorting software can use multiple threads, to speed up the process on modern multicore computers.</li>
<li>Software can use asynchronous I/O so that one run of data can be sorted or merged while other runs are being read from or written to disk.</li>
<li>Multiple machines connected by fast network links can each sort part of a huge dataset in parallel.</li>
</ul>
</li>
<li><b>Increasing hardware speed</b>
<ul>
<li>Using more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.</li>
<li>Fast external memory, like 15K RPM disks or solid-state drives, can speed sorts (but adds substantial costs proportional to the data size).</li>
<li><i>Many</i> other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. "Balancing" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.</li>
<li>Cost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.</li>
</ul>
</li>
<li><b>Increasing software speed</b>
<ul>
<li>Some Sort Benchmark entrants use a variation on radix sort for the first phase of sorting: they separate data into one of many "bins" based on the beginning of its value. Sort Benchmark data is random and especially well-suited to this optimization.</li>
<li>Compacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.</li>
<li>Because the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.</li>
</ul>
</li>
</ul>
<ul>
<li>Multiple disk drives can be used in parallel in order to improve sequential read and write speed. This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.</li>
<li>Sorting software can use multiple threads, to speed up the process on modern multicore computers.</li>
<li>Software can use asynchronous I/O so that one run of data can be sorted or merged while other runs are being read from or written to disk.</li>
<li>Multiple machines connected by fast network links can each sort part of a huge dataset in parallel.</li>
</ul>
<ul>
<li>Using more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.</li>
<li>Fast external memory, like 15K RPM disks or solid-state drives, can speed sorts (but adds substantial costs proportional to the data size).</li>
<li><i>Many</i> other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. "Balancing" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.</li>
<li>Cost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.</li>
</ul>
<ul>
<li>Some Sort Benchmark entrants use a variation on radix sort for the first phase of sorting: they separate data into one of many "bins" based on the beginning of its value. Sort Benchmark data is random and especially well-suited to this optimization.</li>
<li>Compacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.</li>
<li>Because the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.</li>
</ul>
<h2>Other algorithms</h2>
<p>External merge sort is not the only external sorting algorithm; there are also <i>distribution sorts</i>, which work by partitioning the unsorted values into smaller "buckets" that can be sorted in main memory. Like merge sort, external distribution sort also has a main-memory sibling; see bucket sort. There is a duality, or fundamental similarity, between merge- and distribution-based algorithms that can aid in thinking about sorting and other external memory algorithms. There are in-place algorithms for external sort, which require no more disk space than the original data.</p>
</body>
</html>