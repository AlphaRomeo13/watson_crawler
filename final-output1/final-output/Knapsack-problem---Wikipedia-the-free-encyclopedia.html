<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Knapsack-problem---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Knapsack problem</h1>
<p>The <b>knapsack problem</b> or <b>rucksack problem</b> is a problem in combinatorial optimization: Given a set of items, each with a mass and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.</p>
<p>The problem often arises in resource allocation where there are financial constraints and is studied in fields such as combinatorics, computer science, complexity theory, cryptography and applied mathematics.</p>
<p>The knapsack problem has been studied for more than a century, with early works dating as far back as 1897. It is not known how the name "knapsack problem" originated, though the problem was referred to as such in the early works of mathematician Tobias Dantzig (1884–1956), suggesting that the name could have existed in folklore before a mathematical problem had been fully defined.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Applications</li>
<li>2 Definition</li>
<li>3 Computational complexity</li>
<li>4 Solving
<ul>
<li>4.1 Dynamic programming
<ul>
<li>4.1.1 Unbounded knapsack problem</li>
<li>4.1.2 0/1 knapsack problem</li>
</ul>
</li>
<li>4.2 Meet-in-the-middle</li>
<li>4.3 Approximation algorithms
<ul>
<li>4.3.1 Greedy approximation algorithm</li>
<li>4.3.2 Fully polynomial time approximation scheme</li>
</ul>
</li>
<li>4.4 Dominance relations</li>
</ul>
</li>
<li>5 Variations
<ul>
<li>5.1 Multi-objective knapsack problem</li>
<li>5.2 Multi-dimensional knapsack problem</li>
<li>5.3 Multiple knapsack problem</li>
<li>5.4 Quadratic knapsack problem</li>
<li>5.5 Subset-sum problem</li>
</ul>
</li>
<li>6 Software</li>
<li>7 Popular culture</li>
<li>8 See also</li>
<li>9 Notes</li>
<li>10 References</li>
<li>11 External links</li>
</ul>
<ul>
<li>4.1 Dynamic programming
<ul>
<li>4.1.1 Unbounded knapsack problem</li>
<li>4.1.2 0/1 knapsack problem</li>
</ul>
</li>
<li>4.2 Meet-in-the-middle</li>
<li>4.3 Approximation algorithms
<ul>
<li>4.3.1 Greedy approximation algorithm</li>
<li>4.3.2 Fully polynomial time approximation scheme</li>
</ul>
</li>
<li>4.4 Dominance relations</li>
</ul>
<ul>
<li>4.1.1 Unbounded knapsack problem</li>
<li>4.1.2 0/1 knapsack problem</li>
</ul>
<ul>
<li>4.3.1 Greedy approximation algorithm</li>
<li>4.3.2 Fully polynomial time approximation scheme</li>
</ul>
<ul>
<li>5.1 Multi-objective knapsack problem</li>
<li>5.2 Multi-dimensional knapsack problem</li>
<li>5.3 Multiple knapsack problem</li>
<li>5.4 Quadratic knapsack problem</li>
<li>5.5 Subset-sum problem</li>
</ul>
<p></p>
<h2>Applications</h2>
<p>A 1998 study of the Stony Brook University Algorithm Repository showed that, out of 75 algorithmic problems, the knapsack problem was the 18th most popular and the 4th most needed after kd-trees, suffix trees, and the bin packing problem.</p>
<p>Knapsack problems appear in real-world decision-making processes in a wide variety of fields, such as finding the least wasteful way to cut raw materials, seating contest of investments and portfolios, seating contest of assets for asset-backed securitization, and generating keys for the Merkle–Hellman and other knapsack cryptosystems.</p>
<p>One early application of knapsack algorithms was in the construction and scoring of tests in which the test-takers have a choice as to which questions they answer. For small examples it is a fairly simple process to provide the test-takers with such a choice. For example, if an exam contains 12 questions each worth 10 points, the test-taker need only answer 10 questions to achieve a maximum possible score of 100 points. However, on tests with a heterogeneous distribution of point values—i.e. different questions are worth different point values— it is more difficult to provide choices. Feuerman and Weiss proposed a system in which students are given a heterogeneous test with a total of 125 possible points. The students are asked to answer all of the questions to the best of their abilities. Of the possible subsets of problems whose total point values add up to 100, a knapsack algorithm would determine which subset gives each student the highest possible score.</p>
<h2>Definition</h2>
<p>The most common problem being solved is the <b>0-1 knapsack problem</b>, which restricts the number <i>x<sub>i</sub></i> of copies of each kind of item to zero or one.</p>
<p>Mathematically the 0-1-knapsack problem can be formulated as:</p>
<p>Let there be <img class="mwe-math-fallback-image-inline tex" alt="n" src="//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png"> items, <img class="mwe-math-fallback-image-inline tex" alt="z_1" src="//upload.wikimedia.org/math/7/b/1/7b130c2e8f1d1584f9440cbfd6883b1a.png"> to <img class="mwe-math-fallback-image-inline tex" alt="z_n" src="//upload.wikimedia.org/math/f/7/3/f7399cf440e4873eab272482b710d776.png"> where <img class="mwe-math-fallback-image-inline tex" alt="z_i" src="//upload.wikimedia.org/math/b/7/2/b72ebeb95f4bb3e84afd94141a57b13b.png"> has a value <img class="mwe-math-fallback-image-inline tex" alt="v_i" src="//upload.wikimedia.org/math/f/0/e/f0e66f55342ef85ba8be3415dd92d8e2.png"> and weight <img class="mwe-math-fallback-image-inline tex" alt="w_i" src="//upload.wikimedia.org/math/d/f/c/dfc96001568235d60dc12dca4b32d260.png">. <img class="mwe-math-fallback-image-inline tex" alt="x_i" src="//upload.wikimedia.org/math/0/5/e/05e42209d67fe1eb15a055e9d3b3770e.png"> is the number of copies of the item <img class="mwe-math-fallback-image-inline tex" alt="z_i" src="//upload.wikimedia.org/math/b/7/2/b72ebeb95f4bb3e84afd94141a57b13b.png">, which, mentioned above, must be zero or one. The maximum weight that we can carry in the bag is <i>W</i>. It is common to assume that all values and weights are nonnegative. To simplify the representation, we also assume that the items are listed in increasing order of weight.</p>
<ul>
<li>Maximize <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{i=1}^n v_ix_i" src="//upload.wikimedia.org/math/7/e/2/7e23ed26d8a7173b65223f111ab5064e.png"> subject to <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{i=1}^n w_ix_i \leqslant W, \quad \quad x_i \in \{0,1\}" src="//upload.wikimedia.org/math/6/7/a/67a4f17cf90100a099ef54718e959c5d.png"></li>
</ul>
<p>Maximize the sum of the values of the items in the knapsack so that the sum of the weights must be less than or equal to the knapsack's capacity.</p>
<p>The <b>bounded knapsack problem</b> (<b>BKP</b>) removes the restriction that there is only one of each item, but restricts the number <img class="mwe-math-fallback-image-inline tex" alt="x_i" src="//upload.wikimedia.org/math/0/5/e/05e42209d67fe1eb15a055e9d3b3770e.png"> of copies of each kind of item to an integer value <img class="mwe-math-fallback-image-inline tex" alt="c_i" src="//upload.wikimedia.org/math/d/9/8/d9899588b2b28a768a63ade0f3523596.png">.</p>
<p>Mathematically the bounded knapsack problem can be formulated as:</p>
<ul>
<li>Maximize <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{i=1}^n v_ix_i" src="//upload.wikimedia.org/math/7/e/2/7e23ed26d8a7173b65223f111ab5064e.png"> subject to <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{i=1}^n w_ix_i \leqslant W, \quad \quad x_i \in \{0,1,\ldots,c_i\}" src="//upload.wikimedia.org/math/4/0/e/40e5f4792af145bff1034c2fc962c32b.png"></li>
</ul>
<p>The <b>unbounded knapsack problem</b> (<b>UKP</b>) places no upper bound on the number of copies of each kind of item and can be formulated as above except for that the only restriction on <img class="mwe-math-fallback-image-inline tex" alt="x_i" src="//upload.wikimedia.org/math/0/5/e/05e42209d67fe1eb15a055e9d3b3770e.png"> is that it is a non-negative integer.</p>
<p>Mathematically the unbounded knapsack problem can be formulated as:</p>
<ul>
<li>Maximize <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{i=1}^n v_ix_i" src="//upload.wikimedia.org/math/7/e/2/7e23ed26d8a7173b65223f111ab5064e.png"> subject to <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{i=1}^n w_ix_i \leqslant W, \quad \quad x_i \geqslant 0" src="//upload.wikimedia.org/math/1/5/3/153c3d5ec20d7ede2150c5b251123fff.png"></li>
</ul>
<p>One example of the unbounded knapsack problem is given using the figure shown at the beginning of this article and the text "if any number of each box is available" in the caption of that figure.</p>
<h2>Computational complexity</h2>
<p>The knapsack problem is interesting from the perspective of computer science for many reasons:</p>
<ul>
<li>The decision problem form of the knapsack problem (<i>Can a value of at least</i> V <i>be achieved without exceeding the weight</i> W<i>?</i>) is NP-complete, thus there is no possible algorithm both correct and fast (polynomial-time) on all cases, unless P=NP.</li>
<li>While the decision problem is NP-complete, the optimization problem is NP-hard, its resolution is at least as difficult as the decision problem, and there is no known polynomial algorithm which can tell, given a solution, whether it is optimal (which would mean that there is no solution with a larger <i>V</i>, thus solving the decision problem NP-complete).</li>
<li>There is a pseudo-polynomial time algorithm using dynamic programming.</li>
<li>There is a fully polynomial-time approximation scheme, which uses the pseudo-polynomial time algorithm as a subroutine, described below.</li>
<li>Many cases that arise in practice, and "random instances" from some distributions, can nonetheless be solved exactly.</li>
</ul>
<p>There is a link between the "decision" and "optimization" problems in that if there exists a polynomial algorithm that solves the "decision" problem, then one can find the maximum value for the optimization problem in polynomial time by applying this algorithm iteratively while increasing the value of k . On the other hand, if an algorithm finds the optimal value of optimization problem in polynomial time, then the decision problem can be solved in polynomial time by comparing the value of the solution output by this algorithm with the value of k . Thus, both versions of the problem are of similar difficulty.</p>
<p>One theme in research literature is to identify what the "hard" instances of the knapsack problem look like, or viewed another way, to identify what properties of instances in practice might make them more amenable than their worst-case NP-complete behaviour suggests. The goal in finding these "hard" instances is for their use in public key cryptography systems, such as the Merkle-Hellman knapsack cryptosystem.</p>
<h2>Solving</h2>
<p>Several algorithms are available to solve knapsack problems, based on dynamic programming approach, branch and bound approach or hybridizations of both approaches.</p>
<h3>Dynamic programming</h3>
<h4>Unbounded knapsack problem</h4>
<p>If all weights (<img class="mwe-math-fallback-image-inline tex" alt="w_1,\ldots,w_n" src="//upload.wikimedia.org/math/1/0/f/10f8ec30c82615d0169c2f8907a723c2.png">) are nonnegative integers, the knapsack problem can be solved in pseudo-polynomial time using dynamic programming. The following describes a dynamic programming solution for the <i>unbounded</i> knapsack problem.</p>
<p>To simplify things, assume all weights are strictly positive (<i>w<sub>i</sub></i> &gt; 0). We wish to maximize total value subject to the constraint that total weight is less than or equal to <i>W</i>. Then for each <i>w</i> ≤ <i>W</i>, define <i>m[w]</i> to be the maximum value that can be attained with total weight less than or equal to <i>w</i>. <i>m[W]</i> then is the solution to the problem.</p>
<p>Observe that <i>m[w]</i> has the following properties:</p>
<ul>
<li><img class="mwe-math-fallback-image-inline tex" alt="m[0]=0\,\!" src="//upload.wikimedia.org/math/e/c/e/ece37db2427459763955834381136a84.png"> (the sum of zero items, i.e., the summation of the empty set)</li>
<li><img class="mwe-math-fallback-image-inline tex" alt="m[w]= \max_{w_i \le w}(v_i+m[w-w_i])" src="//upload.wikimedia.org/math/6/3/9/639a8c4e9056cd0402e3fef9cbac56d6.png"></li>
</ul>
<p>where <img class="mwe-math-fallback-image-inline tex" alt="v_i" src="//upload.wikimedia.org/math/f/0/e/f0e66f55342ef85ba8be3415dd92d8e2.png"> is the value of the <i>i</i>-th kind of item.</p>
<p>(To formulate the equation above, the idea used is that the solution for a knapsack is the same as the value of one correct item plus the solution for a knapsack with smaller capacity, specifically one with the capacity reduced by the weight of that chosen item.)</p>
<p>Here the maximum of the empty set is taken to be zero. Tabulating the results from <img class="mwe-math-fallback-image-inline tex" alt="m[0]" src="//upload.wikimedia.org/math/5/d/5/5d506c9110bdf5f64cbc61d65fb8cb06.png"> up through <img class="mwe-math-fallback-image-inline tex" alt="m[W]" src="//upload.wikimedia.org/math/8/a/f/8afec71f3cebff8a6340c67f15aab4cc.png"> gives the solution. Since the calculation of each <img class="mwe-math-fallback-image-inline tex" alt="m[w]" src="//upload.wikimedia.org/math/2/9/9/2998e2124b58268125618f4aa3790514.png"> involves examining <img class="mwe-math-fallback-image-inline tex" alt="n" src="//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png"> items, and there are <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png"> values of <img class="mwe-math-fallback-image-inline tex" alt="m[w]" src="//upload.wikimedia.org/math/2/9/9/2998e2124b58268125618f4aa3790514.png"> to calculate, the running time of the dynamic programming solution is <img class="mwe-math-fallback-image-inline tex" alt="O(nW)" src="//upload.wikimedia.org/math/4/a/c/4ac5cc39b948b125cb213ebb2ecec418.png">. Dividing <img class="mwe-math-fallback-image-inline tex" alt="w_1,\,w_2,\,\ldots,\,w_n,\,W" src="//upload.wikimedia.org/math/d/0/2/d0250c59c93335754edf2fe46fb9fb1a.png"> by their greatest common divisor is a way to improve the running time.</p>
<p>The <img class="mwe-math-fallback-image-inline tex" alt="O(nW)" src="//upload.wikimedia.org/math/4/a/c/4ac5cc39b948b125cb213ebb2ecec418.png"> complexity does not contradict the fact that the knapsack problem is NP-complete, since <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png">, unlike <img class="mwe-math-fallback-image-inline tex" alt="n" src="//upload.wikimedia.org/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png">, is not polynomial in the length of the input to the problem. The length of the <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png"> input to the problem is proportional to the number of bits in <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png">, <img class="mwe-math-fallback-image-inline tex" alt="\log W" src="//upload.wikimedia.org/math/9/b/6/9b6c86d5c7e98cad39724f8818797673.png">, not to <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png"> itself.</p>
<h4>0/1 knapsack problem</h4>
<p>A similar dynamic programming solution for the 0/1 knapsack problem also runs in pseudo-polynomial time. Assume <img class="mwe-math-fallback-image-inline tex" alt="w_1,\,w_2,\,\ldots,\,w_n,\, W" src="//upload.wikimedia.org/math/d/0/2/d0250c59c93335754edf2fe46fb9fb1a.png"> are strictly positive integers. Define <img class="mwe-math-fallback-image-inline tex" alt="m[i,w]" src="//upload.wikimedia.org/math/5/a/8/5a8eab40d97b8f81718de6dce56307ee.png"> to be the maximum value that can be attained with weight less than or equal to <img class="mwe-math-fallback-image-inline tex" alt="w" src="//upload.wikimedia.org/math/f/1/2/f1290186a5d0b1ceab27f4e77c0c5d68.png"> using items up to <img class="mwe-math-fallback-image-inline tex" alt="i" src="//upload.wikimedia.org/math/8/6/5/865c0c0b4ab0e063e5caa3387c1a8741.png"> (first <img class="mwe-math-fallback-image-inline tex" alt="i" src="//upload.wikimedia.org/math/8/6/5/865c0c0b4ab0e063e5caa3387c1a8741.png"> items).</p>
<p>We can define <img class="mwe-math-fallback-image-inline tex" alt="m[i,w]" src="//upload.wikimedia.org/math/5/a/8/5a8eab40d97b8f81718de6dce56307ee.png"> recursively as follows:</p>
<ul>
<li><img class="mwe-math-fallback-image-inline tex" alt="m[i,\,w]=m[i-1,\,w]" src="//upload.wikimedia.org/math/c/b/a/cba5389c96e82ea7b3f6e6933afb92f0.png"> if <img class="mwe-math-fallback-image-inline tex" alt="w_i &gt; w\,\!" src="//upload.wikimedia.org/math/2/1/5/215af154097bc0a617656cead4fc16dd.png"> (the new item is more than the current weight limit)</li>
<li><img class="mwe-math-fallback-image-inline tex" alt="m[i,\,w]=\max(m[i-1,\,w],\,m[i-1,w-w_i]+v_i)" src="//upload.wikimedia.org/math/5/c/2/5c279d36c84bb28e88de6bae7b62fa27.png"> if <img class="mwe-math-fallback-image-inline tex" alt="w_i \leqslant w" src="//upload.wikimedia.org/math/f/3/c/f3cafbe48b33df0972ae4e53ea57b2cc.png">.</li>
</ul>
<p>The solution can then be found by calculating <img class="mwe-math-fallback-image-inline tex" alt="m[n,W]" src="//upload.wikimedia.org/math/4/c/9/4c984e5e6b255700ce8397efb693daee.png">. To do this efficiently we can use a table to store previous computations.ii</p>
<p>The following is pseudo code for the dynamic program:</p>
<p>WHATSON? e9ef9890-bc87-4ea4-9975-767580b7e1fb</p>
<pre>
// Input:
// Values (stored in array v)
// Weights (stored in array w)
// Number of distinct items (n)
// Knapsack capacity (W)
for j from 0 to W do
  m[0, j] := 0
end for 
for i from 1 to n do
  for j from 0 to W do
    if w[i] &lt;= j then
      m[i, j] := max(m[i-1, j], m[i-1, j-w[i]] + v[i])
    else
      m[i, j] := m[i-1, j]
    end if
  end for
end for
</pre>
<p>This solution will therefore run in <img class="mwe-math-fallback-image-inline tex" alt="O(nW)" src="//upload.wikimedia.org/math/4/a/c/4ac5cc39b948b125cb213ebb2ecec418.png"> time and <img class="mwe-math-fallback-image-inline tex" alt="O(nW)" src="//upload.wikimedia.org/math/4/a/c/4ac5cc39b948b125cb213ebb2ecec418.png"> space. Additionally, if we use only a 1-dimensional array <img class="mwe-math-fallback-image-inline tex" alt="m[w]" src="//upload.wikimedia.org/math/2/9/9/2998e2124b58268125618f4aa3790514.png"> to store the current optimal values and pass over this array <img class="mwe-math-fallback-image-inline tex" alt="i+1" src="//upload.wikimedia.org/math/1/5/a/15ab2d2b0b92c13f328635e5c4bdbe64.png"> times, rewriting from <img class="mwe-math-fallback-image-inline tex" alt="m[W]" src="//upload.wikimedia.org/math/8/a/f/8afec71f3cebff8a6340c67f15aab4cc.png"> to <img class="mwe-math-fallback-image-inline tex" alt="m[1]" src="//upload.wikimedia.org/math/c/3/a/c3a489b7e47d8b726a5643c2266f9f78.png"> every time, we get the same result for only <img class="mwe-math-fallback-image-inline tex" alt="O(W)" src="//upload.wikimedia.org/math/5/a/3/5a3de35be9bbbae9beea20ce3cbca720.png"> space.</p>
<h3>Meet-in-the-middle</h3>
<p>Another algorithm for 0-1 knapsack, discovered in 1974  and sometimes called "meet-in-the-middle" due to parallels to a similarly named algorithm in cryptography, is exponential in the number of different items but may be preferable to the DP algorithm when <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png"> is large compared to n. In particular, if the <img class="mwe-math-fallback-image-inline tex" alt="w_i" src="//upload.wikimedia.org/math/d/f/c/dfc96001568235d60dc12dca4b32d260.png"> are nonnegative but not integers, we could still use the dynamic programming algorithm by scaling and rounding (i.e. using fixed-point arithmetic), but if the problem requires <img class="mwe-math-fallback-image-inline tex" alt="d" src="//upload.wikimedia.org/math/8/2/7/8277e0910d750195b448797616e091ad.png"> fractional digits of precision to arrive at the correct answer, <img class="mwe-math-fallback-image-inline tex" alt="W" src="//upload.wikimedia.org/math/6/1/e/61e9c06ea9a85a5088a499df6458d276.png"> will need to be scaled by <img class="mwe-math-fallback-image-inline tex" alt="10^d" src="//upload.wikimedia.org/math/f/3/4/f34794101dad4c15d5d7ad49f7281a5d.png">, and the DP algorithm will require <img class="mwe-math-fallback-image-inline tex" alt="O(W*10^d)" src="//upload.wikimedia.org/math/8/a/a/8aaec94377c41df65470b4bc36097696.png"> space and <img class="mwe-math-fallback-image-inline tex" alt="O(nW*10^d)" src="//upload.wikimedia.org/math/e/7/0/e70158fabc3c2f951bc2e4285268d705.png"> time.</p>
<p>WHATSON? a66b4dea-0384-4b8b-88b6-df5fb3883c5b</p>
<pre>
  <b>input:</b> 
    a set of items with weights and values
  <b>output:</b> 
    the greatest combined value of a subset
  partition the set {1...n} into two sets A and B of approximately equal size
  compute the weights and values of all subsets of each set
  <b>for</b> each subset of A
    find the subset of B of greatest value such that the combined weight is less than W
  keep track of the greatest combined value seen so far
</pre>
<p>The algorithm takes <img class="mwe-math-fallback-image-inline tex" alt="O(2^{n/2})" src="//upload.wikimedia.org/math/2/b/0/2b0b61254849cd343a71088f7429addd.png"> space, and efficient implementations of step 3 (for instance, sorting the subsets of B by weight, discarding subsets of B which weigh more than other subsets of B of greater or equal value, and using binary search to find the best match) result in a runtime of <img class="mwe-math-fallback-image-inline tex" alt="O(n*2^{n/2})" src="//upload.wikimedia.org/math/f/d/3/fd3d69f0d3c3357ac9cef6d8dff794dd.png">. As with the meet in the middle attack in cryptography, this improves on the <img class="mwe-math-fallback-image-inline tex" alt="O(n*2^n)" src="//upload.wikimedia.org/math/e/b/9/eb9965d941d40430eb9a6d27388f46a4.png"> runtime of a naive brute force approach (examining all subsets of {1...n}), at the cost of using exponential rather than constant space (see also baby-step giant-step).</p>
<h3>Approximation algorithms</h3>
<p>As for most NP-complete problems, it may be enough to find workable solutions even if they are not optimal. Preferably, however, the approximation comes with a guarantee on the difference between the value of the solution found and the value of the optimal solution.</p>
<p>As with many useful but computationally complex algorithms, there has been substantial research on creating and analyzing algorithms that approximate a solution. The knapsack problem, though NP-Hard, is one of a collection of algorithms that can still be approximated to any specified degree. This means that the problem has a polynomial time approximation scheme. To be exact, the knapsack problem has a fully polynomial time approximation scheme (FPTAS).</p>
<h4>Greedy approximation algorithm</h4>
<p>George Dantzig proposed a greedy approximation algorithm to solve the unbounded knapsack problem. His version sorts the items in decreasing order of value per unit of weight, <img class="mwe-math-fallback-image-inline tex" alt="v_i/w_i" src="//upload.wikimedia.org/math/0/8/3/083201d5fd0b97287e051491a2ca728d.png">. It then proceeds to insert them into the sack, starting with as many copies as possible of the first kind of item until there is no longer space in the sack for more. Provided that there is an unlimited supply of each kind of item, if <img class="mwe-math-fallback-image-inline tex" alt="m" src="//upload.wikimedia.org/math/6/f/8/6f8f57715090da2632453988d9a1501b.png"> is the maximum value of items that fit into the sack, then the greedy algorithm is guaranteed to achieve at least a value of <img class="mwe-math-fallback-image-inline tex" alt="m/2" src="//upload.wikimedia.org/math/d/3/e/d3e24e6d55ec45bff18f5ca808a448e1.png">. However, for the bounded problem, where the supply of each kind of item is limited, the algorithm may be far from optimal.</p>
<h4>Fully polynomial time approximation scheme</h4>
<p>The fully polynomial time approximation scheme (FPTAS) for the knapsack problem takes advantage of the fact that the reason the problem has no polynomial time solutions is because the profits associated with the items are not restricted. If one rounds off some of the least significant digits of the profit values then they will be bounded by a polynomial and 1/ε where ε is a bound on the correctness of the solution. This restriction then means that an algorithm can find a solution in polynomial time that is correct within a factor of (1-ε) of the optimal solution.</p>
<p>WHATSON? ff67ff76-6e05-4f9d-9174-90d1bae926c8</p>
<pre>
 <b>input</b>: 
   ε ∈ [0,1]
   a list A of n items, specified by their values, <img class="mwe-math-fallback-image-inline tex" alt="v_i" src="//upload.wikimedia.org/math/f/0/e/f0e66f55342ef85ba8be3415dd92d8e2.png">, and weights
 <b>output</b>:
   S' the FPTAS solution
</pre>
<p>WHATSON? 6ea2609e-ce51-4dd9-b93c-4353fccfb3bc</p>
<pre>
 P := max <img class="mwe-math-fallback-image-inline tex" alt="\{v_i\mid 1 \leq i \leq n\} " src="//upload.wikimedia.org/math/6/3/5/6354f67ae7a77a13ac2fc9cc8911e7d2.png">  // the highest item value
 K := εP/n
 <b>for</b> i <b>from</b> 1 <b>to</b> n <b>do</b>
    <img class="mwe-math-fallback-image-inline tex" alt="v'_i" src="//upload.wikimedia.org/math/7/0/7/7076ed19e9415385fbe83e84133a9a89.png"> := ⌊<img class="mwe-math-fallback-image-inline tex" alt="v_i" src="//upload.wikimedia.org/math/f/0/e/f0e66f55342ef85ba8be3415dd92d8e2.png">/K⌋
 <b>end for</b>
 <b>return</b> the solution, S', using the <img class="mwe-math-fallback-image-inline tex" alt="v'_i" src="//upload.wikimedia.org/math/7/0/7/7076ed19e9415385fbe83e84133a9a89.png"> values in the dynamic program outlined above
</pre>
<p><b>Theorem:</b> The set <img class="mwe-math-fallback-image-inline tex" alt="S'" src="//upload.wikimedia.org/math/0/0/2/00226656ea0692401f9834fe6994da11.png"> computed by the algorithm above satisfies <img class="mwe-math-fallback-image-inline tex" alt="\mathrm{profit}(S') \geq (1-\varepsilon) \cdot \mathrm{profit}(S^*)" src="//upload.wikimedia.org/math/1/0/f/10f6ab73548508025d7639bdb9bfd38b.png">, where <img class="mwe-math-fallback-image-inline tex" alt="S^*" src="//upload.wikimedia.org/math/c/9/7/c978220d0883c8ecfff145151e364724.png"> is an optimal solution.</p>
<h3>Dominance relations</h3>
<p>Solving the unbounded knapsack problem can be made easier by throwing away items which will never be needed. For a given item <i>i</i>, suppose we could find a set of items <i>J</i> such that their total weight is less than the weight of <i>i</i>, and their total value is greater than the value of <i>i</i>. Then <i>i</i> cannot appear in the optimal solution, because we could always improve any potential solution containing <i>i</i> by replacing <i>i</i> with the set <i>J</i>. Therefore we can disregard the <i>i</i>-th item altogether. In such cases, <i>J</i> is said to <b>dominate</b> <i>i</i>. (Note that this does not apply to bounded knapsack problems, since we may have already used up the items in <i>J</i>.)</p>
<p>Finding dominance relations allows us to significantly reduce the size of the search space. There are several different types of dominance relations, which all satisfy an inequality of the form:</p>
<p><img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{j \in J} w_j\,x_j \ \le  \alpha\,w_i" src="//upload.wikimedia.org/math/d/c/b/dcbba380c857cae9978911310eb82db5.png">, and <img class="mwe-math-fallback-image-inline tex" alt="\qquad \sum_{j \in J} v_j\,x_j \ \ge \alpha\,v_i\," src="//upload.wikimedia.org/math/b/0/2/b02d2b431db6bdc19eebc02d9014e5bc.png"> for some <img class="mwe-math-fallback-image-inline tex" alt="x \in Z _+^n " src="//upload.wikimedia.org/math/5/d/d/5dd40b187bc7d10f1e6b537d18caa850.png"></p>
<p>where <img class="mwe-math-fallback-image-inline tex" alt="\alpha\in Z_+ \,,J\subseteq N" src="//upload.wikimedia.org/math/b/8/7/b8796853a79149113e7103f6605980ad.png"> and <img class="mwe-math-fallback-image-inline tex" alt="i\not\in J" src="//upload.wikimedia.org/math/d/1/1/d1115d0cad95e2ac98cc2a429a6a0dfb.png">. The vector <img class="mwe-math-fallback-image-inline tex" alt="x" src="//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png"> denotes the number of copies of each member of <i>J</i>.</p>
<h2>Variations</h2>
<p>There are many variations of the knapsack problem that have arisen from the vast number of applications of the basic problem. The main variations occur by changing the number of some problem parameter such as the number of items, number of objectives, or even the number of knapsacks.</p>
<h3>Multi-objective knapsack problem</h3>
<p>This variation changes the goal of the individual filling the knapsack. Instead of one objective, such as maximizing the monetary profit, the objective could have several dimensions. For example there could be environmental or social concerns as well as economic goals. Problems frequently addressed include portfolio and transportation logistics optimizations </p>
<p>As a concrete example, suppose you ran a cruise ship. You have to decide how many famous comedians to hire. This boat can handle no more than one ton of passengers and the entertainers must weigh less than 1000 lbs. Each comedian has a weight, brings in business based on their popularity and asks for a specific salary. In this example you have multiple objectives. You want, of course, to maximize the popularity of your entertainers while minimizing their salaries. Also, you want to have as many entertainers as possible.</p>
<h3>Multi-dimensional knapsack problem</h3>
<p>In this variation, the weight of knapsack item <img class="mwe-math-fallback-image-inline tex" alt="i" src="//upload.wikimedia.org/math/8/6/5/865c0c0b4ab0e063e5caa3387c1a8741.png"> is given by a D-dimnesional vector <img class="mwe-math-fallback-image-inline tex" alt="\overline{w_i}=(w_{i1},\ldots,w_{iD})" src="//upload.wikimedia.org/math/5/f/0/5f00cfcf3c3605e69d1d73618980d1e5.png"> and the knapsack has a D-dimensional capacity vector <img class="mwe-math-fallback-image-inline tex" alt="(W_1,\ldots,W_D)" src="//upload.wikimedia.org/math/6/0/f/60f21d157a115fab62e45d2073e28855.png">. The target is to maximize the sum of the values of the items in the knapsack so that the sum of weights in each dimension <img class="mwe-math-fallback-image-inline tex" alt="d" src="//upload.wikimedia.org/math/8/2/7/8277e0910d750195b448797616e091ad.png"> does not exceed <img class="mwe-math-fallback-image-inline tex" alt="W_d" src="//upload.wikimedia.org/math/d/8/6/d86f99dcf2dd992395972df20a3a9420.png">.</p>
<p>Multi-dimensional knapsack is computationally harder than knapsack; Even for <img class="mwe-math-fallback-image-inline tex" alt="D=2" src="//upload.wikimedia.org/math/c/0/0/c00df26b710d9c7901a623fa867ce6a0.png">, the problem does not have EPTAS unless P<img class="mwe-math-fallback-image-inline tex" alt="=" src="//upload.wikimedia.org/math/4/3/e/43ec3e5dee6e706af7766fffea512721.png">NP. However, the algorithm in  is shown to solve sparse instances efficiently. An instance of multi-dimensional knapsack is sparse if there is a set <img class="mwe-math-fallback-image-inline tex" alt="J=\{1,2,\ldots,m\}" src="//upload.wikimedia.org/math/d/6/f/d6f832aed8aef0feb1438e883737231a.png"> for <img class="mwe-math-fallback-image-inline tex" alt="m&lt;D" src="//upload.wikimedia.org/math/e/a/1/ea122c9e11870dfd374ad150b28fc514.png"> such that for every knapsack item <img class="mwe-math-fallback-image-inline tex" alt="i" src="//upload.wikimedia.org/math/8/6/5/865c0c0b4ab0e063e5caa3387c1a8741.png">, <img class="mwe-math-fallback-image-inline tex" alt=" \exists z&gt;m" src="//upload.wikimedia.org/math/b/0/1/b0197061102ff223355ffe1664839bc6.png"> such that <img class="mwe-math-fallback-image-inline tex" alt="\forall j\in J\cup \{z\},\ w_{ij}\geq 0" src="//upload.wikimedia.org/math/b/a/0/ba0b1f0d0f33ba139841877cb30fe419.png"> and <img class="mwe-math-fallback-image-inline tex" alt="\forall y\notin J\cup\{z\}, w_{iy}=0" src="//upload.wikimedia.org/math/0/0/5/00515982841ebfaa04dbee267a7ba489.png">. Such instances occur, for example, when scheduling packets in a wireless network with relay nodes. The algorithm from  also solves sparse instances of the multiple choice variant, multiple-choice multi-dimensional knapsack.</p>
<h3>Multiple knapsack problem</h3>
<p>This variation is similar to the Bin Packing Problem. It differs from the Bin Packing Problem in that a subset of items can be selected, whereas, in the Bin Packing Problem, all items have to be packed to certain bins. The concept is that there are multiple knapsacks. This may seem like a trivial change, but it is not equivalent to adding to the capacity of the initial knapsack. This variation is used in many loading and scheduling problems in Operations Research and has a PTAS</p>
<h3>Quadratic knapsack problem</h3>
<p>As described by Wu et al.:</p>
<p>The quadratic knapsack problem (QKP) maximizes a quadratic objective function subject to a binary and linear capacity constraint.</p>
<p>The quadratic knapsack problem was discussed under that title by Gallo, Hammer, and Simeone in 1980. However, Gallo and Simeone attribute the first treatment of the problem to Witzgall in 1975.</p>
<h3>Subset-sum problem</h3>
<p>The subset sum problem, is a special case of the decision and <b>0-1</b> problems where each kind of item, the weight equals the value: <img class="mwe-math-fallback-image-inline tex" alt="w_i=v_i" src="//upload.wikimedia.org/math/7/7/0/770202303f9fdca04a9a3e38b72044a0.png">. In the field of cryptography, the term <i>knapsack problem</i> is often used to refer specifically to the subset sum problem and is commonly known as one of Karp's 21 NP-complete problems.</p>
<p>The generalization of subset sum problem is called multiple subset-sum problem, in which multiple bins exist with the same capacity. It has been shown that the generalization does not have an FPTAS.</p>
<h2>Software</h2>
<h2>Popular culture</h2>
<ul>
<li>Neal Stephenson provides an example of the knapsack problem in chapter 70 of his novel, <i>Cryptonomicon</i>, to distribute family heirlooms.</li>
</ul>
<h2>See also</h2>
<ul>
<li>Change-making problem</li>
<li>Combinatorial auction</li>
<li>Combinatorial optimization</li>
<li>Continuous knapsack problem</li>
<li>Cutting stock problem</li>
<li>List of knapsack problems</li>
<li>Packing problem</li>
</ul>
<h2>Notes</h2>
<ol>
<li><b>^</b> Mathews, G. B. (25 June 1897). "On the partition of numbers". <i>Proceedings of the London Mathematical Society</i> <b>28</b>: 486–490. </li>
<li><b>^</b> Dantzig, Tobias. Numbers: The Language of Science, 1930.</li>
<li><b>^</b> Kellerer, Pferschy, and Pisinger 2004, p. 3</li>
<li><b>^</b> Skiena, S. S. (September 1999). "Who is Interested in Algorithms and Why? Lessons from the Stony Brook Algorithm Repository". <i>AGM SIGACT News</i> <b>30</b> (3): 65–74. doi:10.1145/333623.333627. ISSN 0163-5700. </li>
<li><b>^</b> Kellerer, Pferschy, and Pisinger 2004, p. 449</li>
<li><b>^</b> Kellerer, Pferschy, and Pisinger 2004, p. 461</li>
<li><b>^</b> Kellerer, Pferschy, and Pisinger 2004, p. 465</li>
<li><b>^</b> Kellerer, Pferschy, and Pisinger 2004, p. 472</li>
<li><b>^</b> Feuerman, Martin; Weiss, Harvey (April 1973). "A Mathematical Programming Model for Test Construction and Scoring". <i>Management Science</i> <b>19</b> (8): 961–966. doi:10.1287/mnsc.19.8.961. JSTOR 2629127. </li>
<li><b>^</b> Pisinger, D. 2003. Where are the hard knapsack problems? Technical Report 2003/08, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark.</li>
<li><b>^</b> L. Caccetta, A. Kulanoot, Computational Aspects of Hard Knapsack Problems, Nonlinear Analysis 47 (2001) 5547–5558.</li>
<li>^    Vincent Poirriez, Nicola Yanev, Rumen Andonov (2009) A Hybrid Algorithm for the Unbounded Knapsack Problem Discrete Optimization http://dx.doi.org/10.1016/j.disopt.2008.09.004</li>
<li>^   Rumen Andonov, Vincent Poirriez, Sanjay Rajopadhye (2000) Unbounded Knapsack Problem : dynamic programming revisited European Journal of Operational Research 123: 2. 168–181 http://dx.doi.org/10.1016/S0377-2217(99)00265-9</li>
<li><b>^</b> S. Martello, P. Toth, Knapsack Problems: Algorithms and Computer Implementation, John Wiley and Sons, 1990</li>
<li><b>^</b> S. Martello, D. Pisinger, P. Toth, Dynamic programming and strong bounds for the 0-1 knapsack problem, <i>Manag. Sci.</i>, 45:414–424, 1999.</li>
<li><b>^</b> G. Plateau, M. Elkihel, A hybrid algorithm for the 0-1 knapsack problem, <i>Methods of Oper. Res.</i>, 49:277–293, 1985.</li>
<li><b>^</b> S. Martello, P. Toth, A mixture of dynamic programming and branch-and-bound for the subset-sum problem, <i>Manag. Sci.</i>, 30:765–771</li>
<li><b>^</b> Horowitz, Ellis; Sahni, Sartaj (1974), <i>Computing partitions with applications to the knapsack problem</i>, <i>Journal of the Association for Computing Machinery</i> <b>21</b>: 277–292, doi:10.1145/321812.321823, MR 0354006 </li>
<li>^   Vazirani, Vijay. Approximation Algorithms. Springer-Verlag Berlin Heidelberg, 2003.</li>
<li><b>^</b> George B. Dantzig, Discrete-Variable Extremum Problems, Operations Research Vol. 5, No. 2, April 1957, pp. 266–288, doi:10.1287/opre.5.2.266</li>
<li><b>^</b> Chang, T. J., et al. Heuristics for Cardinality Constrained Portfolio Optimization. Technical Report, London SW7 2AZ, England: The Management School, Imperial College, May 1998</li>
<li><b>^</b> Chang, C. S., et al. "Genetic Algorithm Based Bicriterion Optimization for Traction Substations in DC Railway System." In Fogel [102], 11-16.</li>
<li><b>^</b> Kulik, A. and Shachnai, H. 2010. There is no EPTAS for two dimensional knapsack. <i>Inf. Process. Lett</i>, 110, 16, 707–710.</li>
<li>^    Cohen, R. and Grebla, G. 2014. "Multi-Dimensional OFDMA Scheduling in a Wireless Network with Relay Nodes". in <i>Proc. IEEE INFOCOM’14</i>, 2427–2435.</li>
<li><b>^</b> Chandra Chekuri and Sanjeev Khanna. 2000. A PTAS for the multiple knapsack problem. In Proceedings of the eleventh annual ACM-SIAM symposium on Discrete algorithms (SODA '00). Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 213-222.</li>
<li><b>^</b> Wu, Z. Y.; Yang, Y. J.;Bai, F. S.;Mammadov, M. (2011). "Global Optimality Conditions and Optimization Methods for Quadratic Knapsack Problems". <i>J Optim Theory Appl</i> <b>151</b>: 241–259. doi:10.1007/s10957-011-9885-4. </li>
<li><b>^</b> Gallo, G.; Hammer, P. L.; Simeone, B. (1980). "Quadratic knapsack problems". <i>Mathematical Programming Studies</i> <b>12</b>: 132–149. doi:10.1007/BFb0120892. </li>
<li><b>^</b> Gallo, G.; Simeone, B. (1988). "On the Supermodular Knapsack Problem". <i>Mathematical Programming Studies</i> (North-Holland) <b>45</b>: 295–309. doi:10.1007/bf01589108. </li>
<li><b>^</b> Witzgall, C. (1975). <i>Mathematical methods of site seating contest for Electronic Message Systems (EMS)</i>. NBS Internal report. </li>
<li><b>^</b> Richard M. Karp (1972). "Reducibility Among Combinatorial Problems". In R. E. Miller and J. W. Thatcher (editors). Complexity of Computer Computations. New York: Plenum. pp. 85–103</li>
<li><b>^</b> Alberto Caprara, Hans Kellerer, and Ulrich Pferschy. 2000. The Multiple Subset Sum Problem. SIAM J. on Optimization 11, 2 (February 2000), 308-319.</li>
<li><b>^</b> "PAAL". paal.mimuw.edu.pl. Retrieved 2014-10-03. </li>
<li><b>^</b> "KSP". OpenOpt. Retrieved 2014-04-06. </li>
<li><b>^</b> "Problems". OpenOpt. Retrieved 2014-04-06. </li>
<li><b>^</b> "CRAN - Package adagio". Cran.r-project.org. 2013-09-28. Retrieved 2014-04-06. </li>
</ol>
</body>
</html>