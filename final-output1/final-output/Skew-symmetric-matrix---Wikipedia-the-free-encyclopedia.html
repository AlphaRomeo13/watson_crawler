<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Skew-symmetric-matrix---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Skew-symmetric matrix</h1>
<p>In mathematics, and in particular linear algebra, a <b>skew-symmetric</b> (or <b>antisymmetric</b> or <b>antimetric</b>) <b>matrix</b> is a square matrix <i>A</i> whose transpose is also its negative; that is, it satisfies the condition -<i>A</i> = <i>A</i>. If the entry in the <i>i</i> th row and <i>j</i> th column is <i>a<sub>ij</sub></i>, i.e. <i>A</i> = (<i>a</i><sub><i>ij</i></sub>) then the skew symmetric condition is <i>a<sub>ij</sub></i> = −<i>a<sub>ji</sub></i>. For example, the following matrix is skew-symmetric:</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Properties
<ul>
<li>1.1 Determinant</li>
<li>1.2 Spectral theory</li>
</ul>
</li>
<li>2 Skew-symmetric and alternating forms</li>
<li>3 Infinitesimal rotations</li>
<li>4 Coordinate-free</li>
<li>5 Skew-symmetrizable matrix</li>
<li>6 See also</li>
<li>7 References</li>
<li>8 Further reading</li>
<li>9 External links</li>
</ul>
<ul>
<li>1.1 Determinant</li>
<li>1.2 Spectral theory</li>
</ul>
<p></p>
<h2>Properties</h2>
<p>We assume that the underlying field is not of characteristic 2: that is, that 1 + 1 ≠ 0 where 1 denotes the multiplicative identity and 0 the additive identity of the given field. Otherwise, a skew-symmetric matrix is just the same thing as a symmetric matrix.</p>
<p>Sums and scalar multiples of skew-symmetric matrices are again skew-symmetric. Hence, the skew-symmetric matrices form a vector space. Its dimension is <i>n</i>(<i>n</i>−1)/2.</p>
<p>Let Mat<sub><i>n</i></sub> denote the space of <i>n</i> × <i>n</i> matrices. A skew-symmetric matrix is determined by <i>n</i>(<i>n</i> − 1)/2 scalars (the number of entries above the main diagonal); a symmetric matrix is determined by <i>n</i>(<i>n</i> + 1)/2 scalars (the number of entries on or above the main diagonal). If Skew<sub><i>n</i></sub> denotes the space of <i>n</i> × <i>n</i> skew-symmetric matrices and Sym<sub><i>n</i></sub> denotes the space of <i>n</i> × <i>n</i> symmetric matrices and then since Mat<sub><i>n</i></sub> = Skew<sub><i>n</i></sub> + Sym<sub><i>n</i></sub> and Skew<sub><i>n</i></sub> ∩ Sym<sub><i>n</i></sub> = {0}, i.e.</p>
<p>where ⊕ denotes the direct sum. Let A ∈ Mat<sub><i>n</i></sub> then</p>
<p>Notice that ½(<i>A</i> − <i>A</i>) ∈ Skew<sub><i>n</i></sub> and ½(<i>A</i> + <i>A</i>) ∈ Sym<sub><i>n</i></sub>. This is true for every square matrix <i>A</i> with entries from any field whose characteristic is different from 2.</p>
<p>Denote with <img class="mwe-math-fallback-image-inline tex" alt="\langle \cdot,\cdot \rangle" src="//upload.wikimedia.org/math/e/0/2/e02eaeb6eb365f078ca029f67f7a6973.png"> the standard inner product on <b>R</b>. The real <i>n</i>-by-<i>n</i> matrix <i>A</i> is skew-symmetric if and only if</p>
<p>This is also equivalent to <img class="mwe-math-fallback-image-inline tex" alt="\langle x,Ax \rangle = 0" src="//upload.wikimedia.org/math/4/1/0/410f1b41af11c2a777994bcf24fe2d6f.png"> for all <i>x</i> (one implication being obvious, the other a plain consequence of <img class="mwe-math-fallback-image-inline tex" alt="\langle x+y, A(x+y)\rangle =0" src="//upload.wikimedia.org/math/e/4/1/e4159430c98f57617a520242b0dc78b7.png"> for all x and y). Since this definition is independent of the choice of basis, skew-symmetry is a property that depends only on the linear operator A and a choice of inner product.</p>
<p>All main diagonal entries of a skew-symmetric matrix must be zero, so the trace is zero. If <i>A</i> = (<i>a<sub>ij</sub></i>) is skew-symmetric, <i>a<sub>ij</sub></i> = −<i>a<sub>ji</sub></i>; hence <i>a<sub>ii</sub></i> = 0.</p>
<p>3x3 skew symmetric matrices can be used to represent cross products as matrix multiplications.</p>
<h3>Determinant</h3>
<p>Let <i>A</i> be a <i>n</i>×<i>n</i> skew-symmetric matrix. The determinant of <i>A</i> satisfies</p>
<p>In particular, if <i>n</i> is odd, and since the underlying field is not of characteristic 2, the determinant vanishes. This result is called <b>Jacobi's theorem</b>, after Carl Gustav Jacobi (Eves, 1980).</p>
<p>The even-dimensional case is more interesting. It turns out that the determinant of <i>A</i> for <i>n</i> even can be written as the square of a polynomial in the entries of <i>A</i>, which was first proved by Cayley:</p>
<p>This polynomial is called the <i>Pfaffian</i> of <i>A</i> and is denoted Pf(<i>A</i>). Thus the determinant of a real skew-symmetric matrix is always non-negative. However this last fact can be proved in an elementary way as follows: the eigenvalues of a real skew-symmetric matrix are purely imaginary (see below) and to every eigenvalue there corresponds the conjugate eigenvalue with the same multiplicity; therefore, as the determinant is the product of the eigenvalues, each one repeated according to its multiplicity, it follows at once that the determinant, if it is not 0, is a positive real number.</p>
<p>The number of distinct terms <i>s</i>(<i>n</i>) in the expansion of the determinant of a skew-symmetric matrix of order <i>n</i> has been considered already by Cayley, Sylvester, and Pfaff. Due to cancellations, this number is quite small as compared the number of terms of a generic matrix of order <i>n</i>, which is <i>n</i>!. The sequence <i>s</i>(<i>n</i>) (sequence A002370 in OEIS) is</p>
<p>and it is encoded in the exponential generating function</p>
<p>The latter yields to the asymptotics (for <i>n</i> even)</p>
<p>The number of positive and negative terms are approximatively a half of the total, although their difference takes larger and larger positive and negative values as <i>n</i> increases (sequence A167029 in OEIS).</p>
<h3>Spectral theory</h3>
<p>Since a matrix is similar to its own transpose, they must have the same eigenvalues. It follows that the eigenvalues of a skew-symmetric matrix always come in pairs ±λ (except in the odd-dimensional case where there is an additional unpaired 0 eigenvalue). From the spectral theorem, for a real skew-symmetric matrix the nonzero eigenvalues are all pure imaginary and thus are of the form <i>i</i>λ<sub>1</sub>, −<i>i</i>λ<sub>1</sub>, <i>i</i>λ<sub>2</sub>, −<i>i</i>λ<sub>2</sub>, … where each of the λ<sub><i>k</i></sub> are real.</p>
<p>Real skew-symmetric matrices are normal matrices (they commute with their adjoints) and are thus subject to the spectral theorem, which states that any real skew-symmetric matrix can be diagonalized by a unitary matrix. Since the eigenvalues of a real skew-symmetric matrix are imaginary it is not possible to diagonalize one by a real matrix. However, it is possible to bring every skew-symmetric matrix to a block diagonal form by an orthogonal transformation. Specifically, every 2<i>n</i> × 2<i>n</i> real skew-symmetric matrix can be written in the form <i>A</i> = <i>Q</i> Σ <i>Q</i> where <i>Q</i> is orthogonal and</p>
<p>for real λ<sub><i>k</i></sub>. The nonzero eigenvalues of this matrix are ±<i>i</i>λ<sub><i>k</i></sub>. In the odd-dimensional case Σ always has at least one row and column of zeros.</p>
<p>More generally, every complex skew-symmetric matrix can be written in the form <i>A</i> = <i>U</i> Σ <i>U</i> where <i>U</i> is unitary and Σ has the block-diagonal form given above with complex λ<sub><i>k</i></sub>. This is an example of the Youla decomposition of a complex square matrix.</p>
<h2>Skew-symmetric and alternating forms</h2>
<p>A <b>skew-symmetric form</b> <i>φ</i> on a vector space <i>V</i> over a field <i>K</i> of arbitrary characteristic is defined to be a bilinear form</p>
<p>such that for all <i>v</i>, <i>w</i> in <i>V</i>,</p>
<p>This defines a form with desirable properties for vector spaces over fields of characteristic not equal to 2, but in a vector space over a field of characteristic 2, the definition is equivalent to that of a symmetric form, as every element is its own additive inverse.</p>
<p>Where the vector space <i>V</i> is over a field of arbitrary characteristic including characteristic 2, we may define an <b>alternating form</b> as a bilinear form <i>φ</i> such that for all vectors <i>v</i> in <i>V</i></p>
<p>This is equivalent to a skew-symmetric form when the field is not of characteristic 2 as seen from</p>
<p>whence,</p>
<p>A bilinear form <i>φ</i> will be represented by a matrix <i>A</i> such that <i>φ</i>(<i>v</i>, <i>w</i>) = <i>v</i><i>Aw</i>, once a basis of <i>V</i> is chosen, and conversely an <i>n</i> × <i>n</i> matrix <i>A</i> on <i>K</i> gives rise to a form sending (<i>v</i>, <i>w</i>) to <i>v</i><i>Aw</i>. For each of symmetric, skew-symmetric and alternating forms, the representing matrices are symmetric, skew-symmetric and alternating respectively.</p>
<h2>Infinitesimal rotations</h2>
<p>Skew-symmetric matrices over the field of real numbers form the tangent space to the real orthogonal group O(<i>n</i>) at the identity matrix; formally, the special orthogonal Lie algebra. In this sense, then, skew-symmetric matrices can be thought of as <i>infinitesimal rotations</i>.</p>
<p>Another way of saying this is that the space of skew-symmetric matrices forms the Lie algebra o(<i>n</i>) of the Lie group O(<i>n</i>). The Lie bracket on this space is given by the commutator:</p>
<p>It is easy to check that the commutator of two skew-symmetric matrices is again skew-symmetric:</p>
<p>The matrix exponential of a skew-symmetric matrix <i>A</i> is then an orthogonal matrix <i>R</i>:</p>
<p>The image of the exponential map of a Lie algebra always lies in the connected component of the Lie group that contains the identity element. In the case of the Lie group O(<i>n</i>), this connected component is the special orthogonal group SO(<i>n</i>), consisting of all orthogonal matrices with determinant 1. So <i>R</i> = exp(<i>A</i>) will have determinant +1. Moreover, since the exponential map of a connected compact Lie group is always surjective, it turns out that <i>every</i> orthogonal matrix with unit determinant can be written as the exponential of some skew-symmetric matrix. In the particular important case of dimension <i>n</i>=2, the exponential representation for an orthogonal matrix reduces to the well-known polar form of a complex number of unit modulus. Indeed, if n=2, a special orthogonal matrix has the form</p>
<p>with a+b=1. Therefore, putting <i>a</i>=cos<i>θ</i> and <i>b</i>=sin<i>θ</i>, it can be written</p>
<p>which corresponds exactly to the polar form cos<i>θ</i> + <i>i</i>sin<i>θ</i> = e of a complex number of unit modulus.</p>
<p>The exponential representation of an orthogonal matrix of order <i>n</i> can also be obtained starting from the fact that in dimension <i>n</i> any special orthogonal matrix <i>R</i> can be written as R = Q S Q, where Q is orthogonal and S is a block diagonal matrix with <img class="mwe-math-fallback-image-inline tex" alt="\scriptstyle\lfloor {n/2}\rfloor" src="//upload.wikimedia.org/math/f/9/d/f9d7b9c35916d6af8596c8c345eef995.png"> blocks of order 2, plus one of order 1 if n is odd; since each single block of order 2 is also an orthogonal matrix, it admits an exponential form. Correspondingly, the matrix S writes as exponential of a skew-symmetric block matrix Σ of the form above, S=exp(Σ), so that R = Q exp(Σ)Q = exp(Q Σ Q), exponential of the skew-symmetric matrix Q Σ Q. Conversely, the surjectivity of the exponential map, together with the above mentioned block-diagonalization for skew-symmetric matrices, implies the block-diagonalization for orthogonal matrices.</p>
<h2>Coordinate-free</h2>
<p>More intrinsically (i.e., without using coordinates), skew-symmetric linear transformations on a vector space <i>V</i> with an inner product may be defined as the bivectors on the space, which are sums of simple bivectors (2-blades) <img class="mwe-math-fallback-image-inline tex" alt="v \wedge w" src="//upload.wikimedia.org/math/8/3/6/836e0e59d939ed5dddaa75f2e9cd3ffa.png">. The correspondence is given by the map <img class="mwe-math-fallback-image-inline tex" alt="v \wedge w \mapsto v^* \otimes w - w^* \otimes v," src="//upload.wikimedia.org/math/a/0/0/a00d078d0de62c600f928bd12db001bb.png"> where <img class="mwe-math-fallback-image-inline tex" alt="v^*" src="//upload.wikimedia.org/math/f/4/4/f44e5a074aaa0228e4bf0cc90295c756.png"> is the covector dual to the vector <img class="mwe-math-fallback-image-inline tex" alt="v" src="//upload.wikimedia.org/math/9/e/3/9e3669d19b675bd57058fd4664205d2a.png">; in orthonormal coordinates these are exactly the elementary skew-symmetric matrices. This characterization is used in interpreting the curl of a vector field (naturally a 2-vector) as an infinitesimal rotation or "curl", hence the name.</p>
<h2>Skew-symmetrizable matrix</h2>
<p>An <i>n</i>-by-<i>n</i> matrix <i>A</i> is said to be <b>skew-symmetrizable</b> if there exist an invertible diagonal matrix <i>D</i> and skew-symmetric matrix <i>S</i> such that <i>S</i> = <i>DA</i>. For <b>real</b> <i>n</i>-by-<i>n</i> matrices, sometimes the condition for <i>D</i> to have positive entries is added.</p>
<h2>See also</h2>
<ul>
<li>Symmetric matrix</li>
<li>Skew-Hermitian matrix</li>
<li>Symplectic matrix</li>
<li>Symmetry in mathematics</li>
</ul>
</body>
</html>