<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>BPP-complexity---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>BPP (complexity)</h1>
<p>In computational complexity theory, <b>BPP</b>, which stands for <b>bounded-error probabilistic polynomial time</b> is the class of decision problems solvable by a probabilistic Turing machine in polynomial time with an error probability bounded away from 1/2 for all instances. <b>BPP</b> is one of the largest <i>practical</i> classes of problems, meaning most problems of interest in <b>BPP</b> have efficient probabilistic algorithms that can be run quickly on real modern machines. <b>BPP</b> also contains <b>P</b>, the class of problems solvable in polynomial time with a deterministic machine, since a deterministic machine is a special case of a probabilistic machine.</p>
<p>Informally, a problem is in <b>BPP</b> if there is an algorithm for it that has the following properties:</p>
<ul>
<li>It is allowed to flip coins and make random decisions</li>
<li>It is guaranteed to run in polynomial time</li>
<li>On any given run of the algorithm, it has a probability of at most 1/3 of giving the wrong answer, whether the answer is YES or NO.</li>
</ul>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Definition</li>
<li>2 Problems</li>
<li>3 Related classes</li>
<li>4 Complexity-theoretic properties</li>
<li>5 Derandomization</li>
<li>6 References</li>
<li>7 External links</li>
</ul>
<p></p>
<h2>Definition</h2>
<p>A language <i>L</i> is in <b>BPP</b> if and only if there exists a probabilistic Turing machine <i>M</i>, such that</p>
<ul>
<li><i>M</i> runs for polynomial time on all inputs</li>
<li>For all <i>x</i> in <i>L</i>, <i>M</i> outputs 1 with probability greater than or equal to 2/3</li>
<li>For all <i>x</i> not in <i>L</i>, <i>M</i> outputs 1 with probability less than or equal to 1/3</li>
</ul>
<p>Unlike the complexity class <b>ZPP</b>, the machine <i>M</i> is required to run for polynomial time on all inputs, regardless of the outcome of the random coin flips.</p>
<p>Alternatively, <b>BPP</b> can be defined using only deterministic Turing machines. A language <i>L</i> is in <b>BPP</b> if and only if there exists a polynomial <i>p</i> and deterministic Turing machine <i>M</i>, such that</p>
<ul>
<li><i>M</i> runs for polynomial time on all inputs</li>
<li>For all <i>x</i> in <i>L</i>, the fraction of strings <i>y</i> of length <i>p</i>(|<i>x</i>|) which satisfy <i>M(x,y)</i> = 1 is greater than or equal to 2/3</li>
<li>For all <i>x</i> not in <i>L</i>, the fraction of strings <i>y</i> of length <i>p</i>(|<i>x</i>|) which satisfy <i>M(x,y)</i> = 1 is less than or equal to 1/3</li>
</ul>
<p>In this definition, the string <i>y</i> corresponds to the output of the random coin flips that the probabilistic Turing machine would have made. For some applications this definition is preferable since it does not mention probabilistic Turing machines.</p>
<p>In practice, an error probability of 1/3 might not be acceptable, however, the choice of 1/3 in the definition is arbitrary. It can be any constant between 0 and 1/2 (exclusive) and the set <b>BPP</b> will be unchanged. It does not even have to be constant: the same class of problems is defined by allowing error as high as 1/2 − <i>n</i> on the one hand, or requiring error as small as 2</i></sup> on the other hand, where <i>c</i> is any positive constant, and <i>n</i> is the length of input. The idea is that there is a probability of error, but if the algorithm is run many times, the chance that the majority of the runs are wrong drops off exponentially as a consequence of the Chernoff bound. This makes it possible to create a highly accurate algorithm by merely running the algorithm several times and taking a "majority vote" of the answers. For example, if one defined the class with the restriction that the algorithm can be wrong with probability at most 1/2, this would result in the same class of problems.</p>
<p><br></p>
<h2>Problems</h2>
<p>Besides the problems in <b>P</b>, which are obviously in <b>BPP</b>, many problems were known to be in <b>BPP</b> but not known to be in <b>P</b>. The number of such problems is decreasing, and it is conjectured that <b>P</b> = <b>BPP</b>.</p>
<p>For a long time, one of the most famous problems that was known to be in <b>BPP</b> but not known to be in <b>P</b> was the problem of determining whether a given number is prime. However, in the 2002 paper <i>PRIMES is in <b>P</b></i>, Manindra Agrawal and his students Neeraj Kayal and Nitin Saxena found a deterministic polynomial-time algorithm for this problem, thus showing that it is in <b>P</b>.</p>
<p>An important example of a problem in <b>BPP</b> (in fact in <b>co-RP</b>) still not known to be in <b>P</b> is polynomial identity testing, the problem of determining whether a polynomial is identically equal to the zero polynomial. In other words, is there an assignment of variables such that when the polynomial is evaluated the result is nonzero? It suffices to choose each variable's value uniformly at random from a finite subset of at least <i>d</i> values to achieve bounded error probability, where <i>d</i> is the total degree of the polynomial.</p>
<h2>Related classes</h2>
<p>If the access to randomness is removed from the definition of <b>BPP</b>, we get the complexity class <b>P</b>. In the definition of the class, if we replace the ordinary Turing machine with a quantum computer, we get the class <b>BQP</b>.</p>
<p>Adding postselection to <b>BPP</b>, or allowing computation paths to have different lengths, gives the class <b>BPP</b><sub>path</sub>. <b>BPP</b><sub>path</sub> is known to contain <b>NP</b>, and it is contained in its quantum counterpart <b>PostBQP</b>.</p>
<p>A Monte Carlo algorithm is a randomized algorithm which is likely to be correct. Problems in the class <b>BPP</b> have Monte Carlo algorithms with polynomial bounded running time. This is compared to a Las Vegas algorithm which is a randomized algorithm which either outputs the correct answer, or outputs "fail" with low probability. Las Vegas algorithms with polynomial bound running times are used to define the class <b>ZPP</b>. Alternatively, <b>ZPP</b> contains probabilistic algorithms that are always correct and have expected polynomial running time. This is weaker than saying it is a polynomial time algorithm, since it may run for super-polynomial time, but with very low probability.</p>
<h2>Complexity-theoretic properties</h2>
<p>It is known that <b>BPP</b> is closed under complement; that is, <b>BPP</b> = <b>co-BPP</b>. <b>BPP</b> is low for itself, meaning that a <b>BPP</b> machine with the power to solve <b>BPP</b> problems instantly (a <b>BPP</b> oracle machine) is not any more powerful than the machine without this extra power. In symbols, <b>BPP</b> = <b>BPP</b>.</p>
<p>The relationship between <b>BPP</b> and <b>NP</b> is unknown: it is not known whether <b>BPP</b> is a subset of <b>NP</b>, <b>NP</b> is a subset of <b>BPP</b> or neither. If <b>NP</b> is contained in <b>BPP</b>, which is considered unlikely since it would imply practical solutions for NP-complete problems, then <b>NP</b> = <b>RP</b> and <b>PH</b> ⊆ <b>BPP</b>.</p>
<p>It is known that <b>RP</b> is a subset of <b>BPP</b>, and <b>BPP</b> is a subset of <b>PP</b>. It is not known whether those two are strict subsets, since we don't even know if <b>P</b> is a strict subset of <b>PSPACE</b>. <b>BPP</b> is contained in the second level of the polynomial hierarchy and therefore it is contained in <b>PH</b>. More precisely, the Sipser–Lautemann theorem states that <img class="mwe-math-fallback-image-inline tex" alt="\ BPP \subseteq \Sigma_2 \cap \Pi_2 " src="//upload.wikimedia.org/math/6/1/f/61fb347a47fa1f711cec35f64032fcae.png">. As a result, <b>P</b> = <b>NP</b> leads to <b>P</b> = <b>BPP</b> since <b>PH</b> collapses to <b>P</b> in this case. Thus either <b>P</b> = <b>BPP</b> or <b>P</b> ≠ <b>NP</b> or both.</p>
<p>Adleman's theorem states that membership in any language in <b>BPP</b> can be determined by a family of polynomial-size Boolean circuits, which means <b>BPP</b> is contained in <b>P/poly</b>. Indeed, as a consequence of the proof of this fact, every <b>BPP</b> algorithm operating on inputs of bounded length can be derandomized into a deterministic algorithm using a fixed string of random bits. Finding this string may be expensive, however.</p>
<p>Relative to oracles, we know that there exist oracles A and B, such that <b>P</b> = <b>BPP</b> and <b>P</b> ≠ <b>BPP</b>. Moreover, relative to a random oracle with probability 1, <b>P</b> = <b>BPP</b> and <b>BPP</b> is strictly contained in <b>NP</b> and <b>co-NP</b>.</p>
<h2>Derandomization</h2>
<p>The existence of certain strong pseudorandom number generators is conjectured by most experts of the field. This conjecture implies that randomness does not give additional computational power to polynomial time computation, that is, <b>P</b> = <b>RP</b> = <b>BPP</b>. Note that ordinary generators are not sufficient to show this result; any probabilistic algorithm implemented using a typical random number generator will always produce incorrect results on certain inputs irrespective of the seed (though these inputs might be rare).</p>
<p>László Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson showed that unless <b>EXPTIME</b> collapses to <b>MA</b>, <b>BPP</b> is contained in</p>
<p>The class <b>i.o.-SUBEXP</b>, which stands for infinitely often <b>SUBEXP</b>, contains problems which have sub-exponential time algorithms for infinitely many input sizes. They also showed that <b>P</b> = <b>BPP</b> if the exponential-time hierarchy, which is defined in terms of the polynomial hierarchy and <b>E</b> as <b>E</b>, collapses to <b>E</b>; however, note that the exponential-time hierarchy is usually conjectured <i>not</i> to collapse.</p>
<p>Russell Impagliazzo and Avi Wigderson showed that if any problem in <b>E</b>, where</p>
<p>has circuit complexity 2 then <b>P</b> = <b>BPP</b>.</p>
</body>
</html>