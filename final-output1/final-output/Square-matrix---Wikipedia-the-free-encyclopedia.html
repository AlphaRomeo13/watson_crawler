<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Square-matrix---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Square matrix</h1>
<p>In mathematics, a <b>square matrix</b> is a matrix with the same number of rows and columns. An <i>n</i>-by-<i>n</i> matrix is known as a square matrix of order <i>n</i>. Any two square matrices of the same order can be added and multiplied.</p>
<p>Square matrices are often used to represent simple linear transformations, such as shearing or rotation. For example, if <b>R</b> is a square matrix representing a rotation (rotation matrix) and <b>v</b> is a column vector describing the position of a point in space, the product <b>Rv</b> yields another column vector describing the position of that point after that rotation. If <b>v</b> is a row vector, the same transformation can be obtained using <b>vR</b>, where <b>R</b> is the transpose of <b>R</b>.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Main diagonal</li>
<li>2 Special kinds
<ul>
<li>2.1 Diagonal or triangular matrix</li>
<li>2.2 Identity matrix</li>
<li>2.3 Symmetric or skew-symmetric matrix</li>
<li>2.4 Invertible matrix and its inverse</li>
<li>2.5 Definite matrix</li>
<li>2.6 Orthogonal matrix</li>
</ul>
</li>
<li>3 Operations
<ul>
<li>3.1 Trace</li>
<li>3.2 Determinant</li>
<li>3.3 Eigenvalues and eigenvectors</li>
</ul>
</li>
<li>4 Notes</li>
</ul>
<ul>
<li>2.1 Diagonal or triangular matrix</li>
<li>2.2 Identity matrix</li>
<li>2.3 Symmetric or skew-symmetric matrix</li>
<li>2.4 Invertible matrix and its inverse</li>
<li>2.5 Definite matrix</li>
<li>2.6 Orthogonal matrix</li>
</ul>
<ul>
<li>3.1 Trace</li>
<li>3.2 Determinant</li>
<li>3.3 Eigenvalues and eigenvectors</li>
</ul>
<p></p>
<h2>Main diagonal</h2>
<p>The entries <i>a</i><sub><i>ii</i></sub> (<i>i</i> = 1, ..., <i>n</i>) form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix. For instance, the main diagonal of the 4-by-4 matrix above contains the elements <i>a</i><sub>11</sub> = 9, <i>a</i><sub>22</sub> = 11, <i>a</i><sub>33</sub> = 4, <i>a</i><sub><i>44</i></sub> = 10.</p>
<p>The diagonal of a square matrix from the top right to the bottom left corner is called <i>antidiagonal</i> or <i>counterdiagonal</i>.</p>
<h2>Special kinds</h2>
<h3>Diagonal or triangular matrix</h3>
<p>If all entries outside the main diagonal are zero, <b>A</b> is called a diagonal matrix. If only all entries above (or below) the main diagonal are zero, <b>A</b> is called a lower (or upper) triangular matrix.</p>
<h3>Identity matrix</h3>
<p>The identity matrix <b>I</b><sub><i>n</i></sub> of size <i>n</i> is the <i>n</i>-by-<i>n</i> matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, e.g.</p>
<p>It is a square matrix of order <i>n</i>, and also a special kind of diagonal matrix. It is called identity matrix because multiplication with it leaves a matrix unchanged:</p>
<h3>Symmetric or skew-symmetric matrix</h3>
<p>A square matrix <b>A</b> that is equal to its transpose, i.e., <b>A</b> = <b>A</b>, is a symmetric matrix. If instead, <b>A</b> was equal to the negative of its transpose, i.e., <b>A</b> = −<b>A</b>, then <b>A</b> is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy <b>A</b> = <b>A</b>, where the star or asterisk denotes the conjugate transpose of the matrix, i.e., the transpose of the complex conjugate of <b>A</b>.</p>
<p>By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; i.e., every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real. This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.</p>
<h3>Invertible matrix and its inverse</h3>
<p>A square matrix <b>A</b> is called <i>invertible</i> or <i>non-singular</i> if there exists a matrix <b>B</b> such that</p>
<p>If <b>B</b> exists, it is unique and is called the <i>inverse matrix</i> of <b>A</b>, denoted <b>A</b>.</p>
<h3>Definite matrix</h3>
<p>A symmetric <i>n</i>×<i>n</i>-matrix is called <i>positive-definite</i> (respectively negative-definite; indefinite), if for all nonzero vectors <b>x</b> ∈ <b>R</b> the associated quadratic form given by</p>
<p>takes only positive values (respectively only negative values; both some negative and some positive values). If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.</p>
<p>A symmetric matrix is positive-definite if and only if all its eigenvalues are positive. The table at the right shows two possibilities for 2-by-2 matrices.</p>
<p>Allowing as input two different vectors instead yields the bilinear form associated to <b>A</b>:</p>
<h3>Orthogonal matrix</h3>
<p>An <i>orthogonal matrix</i> is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors). Equivalently, a matrix <i>A</i> is orthogonal if its transpose is equal to its inverse:</p>
<p>which entails</p>
<p>where <i>I</i> is the identity matrix.</p>
<p>An orthogonal matrix <i>A</i> is necessarily invertible (with inverse <i>A</i> = <i>A</i>), unitary (<i>A</i> = <i>A</i>*), and normal (<i>A</i>*<i>A</i> = <i>AA</i>*). The determinant of any orthogonal matrix is either +1 or −1. A <i>special orthogonal matrix</i> is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant −1 is either a pure reflection, or a composition of reflection and rotation.</p>
<p>The complex analogue of an orthogonal matrix is a unitary matrix.</p>
<h2>Operations</h2>
<h3>Trace</h3>
<p>The trace, tr(<b>A</b>) of a square matrix <b>A</b> is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:</p>
<p>This is immediate from the definition of matrix multiplication:</p>
<p>Also, the trace of a matrix is equal to that of its transpose, i.e.,</p>
<h3>Determinant</h3>
<p>The <i>determinant</i> det(<b>A</b>) or |<b>A</b>| of a square matrix <b>A</b> is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in <b>R</b>) or volume (in <b>R</b>) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.</p>
<p>The determinant of 2-by-2 matrices is given by</p>
<p>The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.</p>
<p>The determinant of a product of square matrices equals the product of their determinants:</p>
<p>Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1. Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, i.e., determinants of smaller matrices. This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.</p>
<h3>Eigenvalues and eigenvectors</h3>
<p>A number λ and a non-zero vector <b>v</b> satisfying</p>
<p>are called an <i>eigenvalue</i> and an <i>eigenvector</i> of <b>A</b>, respectively. The number λ is an eigenvalue of an <i>n</i>×<i>n</i>-matrix <b>A</b> if and only if <b>A</b>−λ<b>I</b><sub><i>n</i></sub> is not invertible, which is equivalent to</p>
<p>The polynomial <i>p</i><sub><b>A</b></sub> in an indeterminate <i>X</i> given by evaluation the determinant det(<i>X</i><b>I</b><sub><i>n</i></sub>−<b>A</b>) is called the characteristic polynomial of <b>A</b>. It is a monic polynomial of degree <i>n</i>. Therefore the polynomial equation <i>p</i><sub><b>A</b></sub>(λ) = 0 has at most <i>n</i> different solutions, i.e., eigenvalues of the matrix. They may be complex even if the entries of <b>A</b> are real. According to the Cayley–Hamilton theorem, <i>p</i><sub><b>A</b></sub>(<b>A</b>) = <b>0</b>, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.</p>
<h2>Notes</h2>
<ol>
<li><b>^</b> Horn &amp; Johnson 1985, Theorem 2.5.6</li>
<li><b>^</b> Brown 1991, Definition I.2.28</li>
<li><b>^</b> Brown 1991, Definition I.5.13</li>
<li><b>^</b> Horn &amp; Johnson 1985, Chapter 7</li>
<li><b>^</b> Horn &amp; Johnson 1985, Theorem 7.2.1</li>
<li><b>^</b> Horn &amp; Johnson 1985, Example 4.0.6, p. 169</li>
<li><b>^</b> Brown 1991, Definition III.2.1</li>
<li><b>^</b> Brown 1991, Theorem III.2.12</li>
<li><b>^</b> Brown 1991, Corollary III.2.16</li>
<li><b>^</b> Mirsky 1990, Theorem 1.4.1</li>
<li><b>^</b> Brown 1991, Theorem III.3.18</li>
<li><b>^</b> Brown 1991, Definition III.4.1</li>
<li><b>^</b> Brown 1991, Definition III.4.9</li>
<li><b>^</b> Brown 1991, Corollary III.4.10</li>
</ol>
<ol>
<li><b>^</b> <i>Eigen</i> means "own" in German and in Dutch.</li>
</ol>
<ul>
<li>v</li>
<li>t</li>
<li>e</li>
</ul>
<ul>
<li>Scalar</li>
<li>Vector</li>
<li>Vector space</li>
<li>Vector projection</li>
<li>Linear span</li>
<li>Linear map</li>
<li>Linear projection</li>
<li>Linear independence</li>
<li>Linear combination</li>
<li>Basis</li>
<li>Column space</li>
<li>Row space</li>
<li>Dual space</li>
<li>Orthogonality</li>
<li>Kernel</li>
<li>Eigenvalues and eigenvectors</li>
<li>Least squares regressions</li>
<li>Outer product</li>
<li>Inner product space</li>
<li>Dot product</li>
<li>Transpose</li>
<li>Gram–Schmidt process</li>
<li>Linear equations</li>
</ul>
<ul>
<li>Block</li>
<li>Decomposition</li>
<li>Invertible</li>
<li>Minor</li>
<li>Multiplication</li>
<li>Rank</li>
<li>Transformation</li>
<li>Cramer's rule</li>
<li>Gaussian elimination</li>
</ul>
<ul>
<li>Floating point</li>
<li>Numerical stability</li>
<li>Basic Linear Algebra Subprograms (BLAS)</li>
<li>Sparse matrix</li>
<li>Comparison of linear algebra libraries</li>
<li>Comparison of numerical analysis software</li>
</ul>
</body>
</html>