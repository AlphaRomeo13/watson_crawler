<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Triangular-matrix---Wikipedia-the-free-encyclopedia.html</title></head>
<body>
<h1>Triangular matrix</h1>
<p>In the mathematical discipline of linear algebra, a <b>triangular matrix</b> is a special kind of square matrix. A square matrix is called <b>lower triangular</b> if all the entries <i>above</i> the main diagonal are zero. Similarly, a square matrix is called <b>upper triangular</b> if all the entries <i>below</i> the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.</p>
<p>Because matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix <i>L</i> and an upper triangular matrix <i>U</i> if and only if all its leading principal minors are non-zero.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li>1 Description
<ul>
<li>1.1 Examples</li>
</ul>
</li>
<li>2 Special forms
<ul>
<li>2.1 Unitriangular matrix</li>
<li>2.2 Strictly triangular matrix</li>
<li>2.3 Atomic triangular matrix
<ul>
<li>2.3.1 Examples</li>
</ul>
</li>
</ul>
</li>
<li>3 Special properties</li>
<li>4 Triangularisability
<ul>
<li>4.1 Simultaneous triangularisability</li>
</ul>
</li>
<li>5 Generalizations
<ul>
<li>5.1 Borel subgroups and Borel subalgebras</li>
<li>5.2 Examples</li>
</ul>
</li>
<li>6 Forward and back substitution
<ul>
<li>6.1 Forward substitution</li>
<li>6.2 Algorithm</li>
<li>6.3 Applications</li>
</ul>
</li>
<li>7 See also</li>
<li>8 Notes</li>
<li>9 References</li>
</ul>
<ul>
<li>1.1 Examples</li>
</ul>
<ul>
<li>2.1 Unitriangular matrix</li>
<li>2.2 Strictly triangular matrix</li>
<li>2.3 Atomic triangular matrix
<ul>
<li>2.3.1 Examples</li>
</ul>
</li>
</ul>
<ul>
<li>2.3.1 Examples</li>
</ul>
<ul>
<li>4.1 Simultaneous triangularisability</li>
</ul>
<ul>
<li>5.1 Borel subgroups and Borel subalgebras</li>
<li>5.2 Examples</li>
</ul>
<ul>
<li>6.1 Forward substitution</li>
<li>6.2 Algorithm</li>
<li>6.3 Applications</li>
</ul>
<p></p>
<h2>Description</h2>
<p>A matrix of the form</p>
<p>is called a <b>lower triangular matrix</b> or <b>left triangular matrix</b>, and analogously a matrix of the form</p>
<p>is called an <b>upper triangular matrix</b> or <b>right triangular matrix</b>. The variable <i>L</i> (standing for lower or left) is commonly used to represent a lower triangular matrix, while the variable <i>U</i> (standing for upper) or <i>R</i> (standing for right) is commonly used for upper triangular matrix. A matrix that is both upper and lower triangular is diagonal.</p>
<p>Matrices that are similar to triangular matrices are called <b>triangularisable</b>.</p>
<p>Many operations on upper triangular matrices preserve the shape:</p>
<ul>
<li>The sum of two upper triangular matrices is upper triangular.</li>
<li>The product of two upper triangular matrices is upper triangular.</li>
<li>The inverse of an invertible upper triangular matrix is upper triangular.</li>
<li>The product of an upper triangular matrix by a constant is an upper triangular matrix.</li>
</ul>
<p>Together these facts mean that the upper triangular matrices form a subalgebra of the associative algebra of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the Lie algebra of square matrices of a fixed size, where the Lie bracket [<i>a</i>,<i>b</i>] given by the commutator <i>ab-ba</i>. The Lie algebra of all upper triangular matrices is often referred to as a Borel subalgebra of the Lie algebra of all square matrices.</p>
<p>All these results hold if "upper triangular" is replaced by "lower triangular" throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.</p>
<h3>Examples</h3>
<p>This matrix</p>
<p>is upper triangular and this matrix</p>
<p>is lower triangular.</p>
<h2>Special forms</h2>
<h3>Unitriangular matrix</h3>
<p>If the entries on the main diagonal of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) <b>unitriangular</b>. All unitriangular matrices are unipotent. Other names used for these matrices are <b>unit</b> (upper or lower) <b>triangular</b> (of which "unitriangular" might be a contraction), or very rarely <b>normed</b> (upper or lower) <b>triangular</b>. However a <i>unit</i> triangular matrix is not the same as <b>the</b> <i>unit matrix</i>, and a <i>normed</i> triangular matrix has nothing to do with the notion of matrix norm. The identity matrix is the only matrix which is both upper and lower unitriangular.</p>
<p>The set of unitriangular matrices forms a Lie group.</p>
<h3>Strictly triangular matrix</h3>
<p>If the entries on the main diagonal of a (upper or lower) triangular matrix are all 0, the matrix is called <b>strictly</b> (upper or lower) <b>triangular</b>. All strictly triangular matrices are nilpotent, and the set of strictly upper (or lower) triangular matrices forms a nilpotent Lie algebra, denoted <img class="mwe-math-fallback-image-inline tex" alt="\mathfrak{n}." src="//upload.wikimedia.org/math/b/3/c/b3c12986e260f0240e52dbd7e525516b.png"> This algebra is the derived Lie algebra of <img class="mwe-math-fallback-image-inline tex" alt="\mathfrak{b}" src="//upload.wikimedia.org/math/4/3/a/43aac52268f1de093871f0d36deea0f2.png">, the Lie algebra of all upper triangular matrices; in symbols, <img class="mwe-math-fallback-image-inline tex" alt="\mathfrak{n} = [\mathfrak{b},\mathfrak{b}]." src="//upload.wikimedia.org/math/9/b/7/9b7f9841e00a0c333de3b7a9593e962a.png"> In addition, <img class="mwe-math-fallback-image-inline tex" alt="\mathfrak{n}" src="//upload.wikimedia.org/math/a/d/e/ade85bce619aa42026d4bbb286d36f48.png"> is the Lie algebra of the Lie group of unitriangular matrices.</p>
<p>In fact, by Engel's theorem, any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable.</p>
<h3>Atomic triangular matrix</h3>
<p>An <b>atomic</b> (upper or lower) <b>triangular matrix</b> is a special form of unitriangular matrix, where all of the off-diagonal entries are zero, except for the entries in a single column. Such a matrix is also called a <b>Gauss matrix</b> or a <b>Gauss transformation matrix</b>. So an atomic lower triangular matrix is of the form</p>
<p>The inverse of an <b>atomic</b> triangular matrix is again atomic triangular. Indeed, we have</p>
<p>i.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.</p>
<h4>Examples</h4>
<p>The matrix</p>
<p>is atomic lower triangular. Its inverse is</p>
<h2>Special properties</h2>
<p>A matrix which is simultaneously triangular and normal is also diagonal. This can be seen by looking at the diagonal entries of <i>A</i><i>A</i> and <i>AA</i>, where <i>A</i> is a normal, triangular matrix.</p>
<p>The transpose of an upper triangular matrix is a lower triangular matrix and vice versa.</p>
<p>The determinant of a triangular matrix equals the product of the diagonal entries. Since for any triangular matrixÂ <i>A</i> the matrix <img class="mwe-math-fallback-image-inline tex" alt="x I-A" src="//upload.wikimedia.org/math/3/b/2/3b2da26bcb84195111209bbb94892b0f.png">, whose determinant is the characteristic polynomial of <i>A</i>, is also triangular, the diagonal entries of <i>A</i> in fact give the multiset of eigenvalues of <i>A</i> (an eigenvalue with multiplicity <i>m</i> occurs exactly <i>m</i> times as diagonal entry).</p>
<h2>Triangularisability</h2>
<p>A matrix that is similar to a triangular matrix is referred to as <b>triangularisable</b>. Abstractly, this is equivalent to stabilising a flag: upper triangular matrices are precisely those that preserve the standard flag, which is given by the standard ordered basis <img class="mwe-math-fallback-image-inline tex" alt="(e_1,\ldots,e_n)" src="//upload.wikimedia.org/math/f/c/9/fc9ad905e2bd77b6c84a9bc94fa9282e.png"> and the resulting flag <img class="mwe-math-fallback-image-inline tex" alt="0 &lt; \left\langle e_1\right\rangle &lt; \left\langle e_1,e_2\right\rangle &lt; \cdots &lt; \left\langle e_1,\ldots,e_n \right\rangle = K^n." src="//upload.wikimedia.org/math/a/a/9/aa95e2d304afa87c283acec391c2639f.png"> All flags are conjugate (as the general linear group acts transitively on bases), so any matrix that stabilises a flag is similar to one that stabilises the standard flag.</p>
<p>Any complex square matrix is triangularisable. In fact, a matrix <i>A</i> over a field containing all of the eigenvalues of <i>A</i> (for example, any matrix over an algebraically closed field) is similar to a triangular matrix. This can be proven by using induction on the fact that <i>A</i> has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that <i>A</i> stabilises a flag, and is thus triangularisable with respect to a basis for that flag.</p>
<p>A more precise statement is given by the Jordan normal form theorem, which states that in this situation, <i>A</i> is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.</p>
<p>In the case of complex matrices, it is possible to say more about triangularisation, namely, that any square matrix <i>A</i> has a Schur decomposition. This means that <i>A</i> is unitarily equivalent (i.e. similar, using a unitary matrix as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.</p>
<h3>Simultaneous triangularisability</h3>
<p>A set of matrices <img class="mwe-math-fallback-image-inline tex" alt="A_1, \ldots, A_k" src="//upload.wikimedia.org/math/8/e/2/8e2e73a89e30d417e48dbd77bd9aca48.png"> are said to be <b>simultaneously triangularisable</b> if there is a basis under which they are all upper triangular; equivalently, if they are upper triangularizable by a single similarity matrix <i>P.</i> Such a set of matrices is more easily understood by considering the algebra of matrices it generates, namely all polynomials in the <img class="mwe-math-fallback-image-inline tex" alt="A_i," src="//upload.wikimedia.org/math/2/f/f/2ff4927014100b169b91212191b31848.png"> denoted <img class="mwe-math-fallback-image-inline tex" alt="K[A_1,\ldots,A_k]." src="//upload.wikimedia.org/math/d/7/0/d70ba709035d6d99a45eb125201a2e96.png"> Simultaneous triangularizability means that this algebra is conjugate into the Lie subalgebra of upper triangular matrices, and is equivalent to this algebra being a Lie subalgebra of a Borel subalgebra.</p>
<p>The basic result is that (over an algebraically closed field), the commuting matrices <img class="mwe-math-fallback-image-inline tex" alt="A,B" src="//upload.wikimedia.org/math/6/c/3/6c30b42101939c7bdf95f4c1052d615c.png"> or more generally <img class="mwe-math-fallback-image-inline tex" alt="A_1,\ldots,A_k" src="//upload.wikimedia.org/math/8/e/2/8e2e73a89e30d417e48dbd77bd9aca48.png"> are simultaneously triangularizable. This can be proven by first showing that commuting matrices have a common eigenvector, and then inducting on dimension as before. This was proven by Frobenius, starting in 1878 for a commuting pair, as discussed at commuting matrices. As for a single matrix, over the complex numbers these can be triangularized by unitary matrices.</p>
<p>The fact that commuting matrices have a common eigenvector can be interpreted as a result of Hilbert's Nullstellensatz: commuting matrices form a commutative algebra <img class="mwe-math-fallback-image-inline tex" alt="K[A_1,\ldots,A_k]" src="//upload.wikimedia.org/math/d/b/d/dbda72e2b1b61f51a1442bd1781ff5de.png"> over <img class="mwe-math-fallback-image-inline tex" alt="K[x_1,\ldots,x_k]" src="//upload.wikimedia.org/math/7/7/9/779553a2b74d2a329828422ea3b2993a.png"> which can be interpreted as a variety in <i>k</i>-dimensional affine space, and the existence of a (common) eigenvalue (and hence a common eigenvector) corresponds to this variety having a point (being non-empty), which is the content of the (weak) Nullstellensatz. In algebraic terms, these operators correspond to an algebra representation of the polynomial algebra in <i>k</i> variables.</p>
<p>This is generalized by Lie's theorem, which shows that any representation of a solvable Lie algebra is simultaneously upper triangularisable, the case of commuting matrices being the abelian Lie algebra case, abelian being a fortiori solvable.</p>
<p>More generally and precisely, a set of matrices <img class="mwe-math-fallback-image-inline tex" alt="A_1,\ldots,A_k" src="//upload.wikimedia.org/math/8/e/2/8e2e73a89e30d417e48dbd77bd9aca48.png"> is simultaneously triangularisable if and only if the matrix <img class="mwe-math-fallback-image-inline tex" alt="p(A_1,\ldots,A_k)[A_i,A_j]" src="//upload.wikimedia.org/math/3/8/c/38c2cea024e76c24642eb10a47f57c88.png"> is nilpotent for all polynomials <i>p</i> in <i>k</i> <i>non</i>-commuting variables, where <img class="mwe-math-fallback-image-inline tex" alt="[A_i,A_j]" src="//upload.wikimedia.org/math/8/0/d/80daa455b55366bd4046000ae0ef741f.png"> is the commutator; note that for commuting <img class="mwe-math-fallback-image-inline tex" alt="A_i" src="//upload.wikimedia.org/math/e/8/a/e8aaf87d9a5c35b14cfbc370d3fd7b21.png"> the commutator vanishes so this holds. This was proven in (Drazin, Dungey &amp; Gruenberg 1951); a brief proof is given in (Prasolov 1994, pp. 178â179). One direction is clear: if the matrices are simultaneously triangularisable, then <img class="mwe-math-fallback-image-inline tex" alt="[A_i, A_j]" src="//upload.wikimedia.org/math/8/0/d/80daa455b55366bd4046000ae0ef741f.png"> is <i>strictly</i> upper triangularizable (hence nilpotent), which is preserved by multiplication by any <img class="mwe-math-fallback-image-inline tex" alt="A_k" src="//upload.wikimedia.org/math/d/9/3/d93f57d24bbe3378bf1116d752877d4f.png"> or combination thereof â it will still have 0s on the diagonal in the triangularizing basis.</p>
<h2>Generalizations</h2>
<p>Because the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an algebra. Algebras of upper triangular matrices have a natural generalization in functional analysis which yields nest algebras on Hilbert spaces.</p>
<p>A non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a trapezoid.</p>
<h3>Borel subgroups and Borel subalgebras</h3>
<p>The set of invertible triangular matrices of a given kind (upper or lower) forms a group, indeed a Lie group, which is a subgroup of the general linear group of all invertible matrices; invertible is equivalent to all diagonal entries being invertible (non-zero).</p>
<p>Over the real numbers, this group is disconnected, having <img class="mwe-math-fallback-image-inline tex" alt="2^n" src="//upload.wikimedia.org/math/9/a/a/9aa0ec0374c89d2f7f3d9cd2e05a4bc5.png"> components accordingly as each diagonal entry is positive or negative. The identity component is invertible triangular matrices with positive entries on the diagonal, and the group of all invertible triangular matrices is a semidirect product of this group and diagonal entries with <img class="mwe-math-fallback-image-inline tex" alt="\pm 1" src="//upload.wikimedia.org/math/9/6/7/967ffa3ca82c4b8aad1075067fb3fec5.png"> on the diagonal, corresponding to the components.</p>
<p>The Lie algebra of the Lie group of invertible upper triangular matrices is the set of all upper triangular matrices, not necessarily invertible, and is a solvable Lie algebra. These are, respectively, the standard Borel subgroup <i>B</i> of the Lie group GL<sub>n</sub> and the standard Borel subalgebra <img class="mwe-math-fallback-image-inline tex" alt="\mathfrak{b}" src="//upload.wikimedia.org/math/4/3/a/43aac52268f1de093871f0d36deea0f2.png"> of the Lie algebra gl<sub>n</sub>.</p>
<p>The upper triangular matrices are precisely those that stabilize the standard flag. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are Borel subgroups. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.</p>
<p>The stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are <i>not</i> all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called parabolic subgroups.</p>
<h3>Examples</h3>
<p>The group of 2 by 2 upper unitriangular matrices is isomorphic to the additive group of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic MÃ¶bius transformations; the 3 by 3 upper unitriangular matrices form the Heisenberg group.</p>
<h2>Forward and back substitution</h2>
<p>A matrix equation in the form <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{L}\mathbf{x} = \mathbf{b}" src="//upload.wikimedia.org/math/9/4/0/940fa8acc183af0aedacb0cd5e991d2a.png"> or <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{U}\mathbf{x} = \mathbf{b}" src="//upload.wikimedia.org/math/7/b/d/7bdda21a20dd03f468a086ec8ce5cba7.png"> is very easy to solve by an iterative process called <b>forward substitution</b> for lower triangular matrices and analogously <b>back substitution</b> for upper triangular matrices. The process is so called because for lower triangular matrices, one first computes <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png">, then substitutes that <i>forward</i> into the <i>next</i> equation to solve for <img class="mwe-math-fallback-image-inline tex" alt="x_2" src="//upload.wikimedia.org/math/8/f/4/8f43fce8dbdf3c4f8d0ac91f0de1d43d.png">, and repeats through to <img class="mwe-math-fallback-image-inline tex" alt="x_n" src="//upload.wikimedia.org/math/6/7/b/67b68721103b5a16194f4b3e3ec222db.png">. In an upper triangular matrix, one works <i>backwards,</i> first computing <img class="mwe-math-fallback-image-inline tex" alt="x_n" src="//upload.wikimedia.org/math/6/7/b/67b68721103b5a16194f4b3e3ec222db.png">, then substituting that <i>back</i> into the <i>previous</i> equation to solve for <img class="mwe-math-fallback-image-inline tex" alt="x_{n-1}" src="//upload.wikimedia.org/math/3/2/0/320eb171f9774f65eafb058ec460bf39.png">, and repeating through <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png">.</p>
<p>Notice that this does not require inverting the matrix.</p>
<h3>Forward substitution</h3>
<p>The matrix equation <b>L</b><i>x</i> = <i>b</i> can be written as a system of linear equations</p>
<p>Observe that the first equation (<img class="mwe-math-fallback-image-inline tex" alt="l_{1,1} x_1 = b_1" src="//upload.wikimedia.org/math/0/6/0/060613ad08096f6710237ba9f3657935.png">) only involves <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png">, and thus one can solve for <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png"> directly. The second equation only involves <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png"> and <img class="mwe-math-fallback-image-inline tex" alt="x_2" src="//upload.wikimedia.org/math/8/f/4/8f43fce8dbdf3c4f8d0ac91f0de1d43d.png">, and thus can be solved once one substitutes in the already solved value for <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="//upload.wikimedia.org/math/f/9/a/f9a3b8e9e501458e8face47cae8826de.png">. Continuing in this way, the <img class="mwe-math-fallback-image-inline tex" alt="k" src="//upload.wikimedia.org/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png">-th equation only involves <img class="mwe-math-fallback-image-inline tex" alt="x_1,\dots,x_k" src="//upload.wikimedia.org/math/1/f/a/1facb280d4bea97c13d4f7f744510eae.png">, and one can solve for <img class="mwe-math-fallback-image-inline tex" alt="x_k" src="//upload.wikimedia.org/math/4/2/b/42b7fa117615f02a4d47823ab2862a10.png"> using the previously solved values for <img class="mwe-math-fallback-image-inline tex" alt="x_1,\dots,x_{k-1}" src="//upload.wikimedia.org/math/c/8/f/c8f1214f78b6f1de245fe0c2006e77d5.png">.</p>
<p>The resulting formulas are:</p>
<p>A matrix equation with an upper triangular matrix <b>U</b> can be solved in an analogous way, only working backwards.</p>
<h3>Algorithm</h3>
<p>The following is an example implementation of this algorithm in the C# programming language. Note that the algorithm performs poorly in C# due to the inefficient handling of non-jagged matrices in this language. Nonetheless, the method of forward and backward substitution <i>can</i> be highly efficient.</p>
<p>WHATSON? fa09ad51-80c3-46c0-abac-62404282164d</p>
<pre>
 double[] luEvaluate(double[,] L, double[,] U, Vector b)
  {
  // Ax = b -&gt; LUx = b. Then y is defined to be Ux
  int i = 0;
  int j = 0;
  int n = b.Count;
  double[] x = new double[n];
  double[] y = new double[n];
  // Forward solve Ly = b
  for (i = 0; i &lt; n; i++)
  {
    y[i] = b[i];
    for (j = 0; j &lt; i; j++)
    {
      y[i] -= L[i, j] * y[j];
    }
    y[i] /= L[i, i];
  }
  // Backward solve Ux = y
  for (i = n - 1; i &gt;= 0; i--)
  {
    x[i] = y[i];
    for (j = i + 1; j &lt; n; j++)
    {
      x[i] -= U[i, j] * x[j];
    }
    x[i] /= U[i, i];
  }
  return x;
 }
</pre>
<h3>Applications</h3>
<p>Forward substitution is used in financial bootstrapping to construct a yield curve.</p>
<h2>See also</h2>
<ul>
<li>Gaussian elimination</li>
<li>QR decomposition</li>
<li>Cholesky decomposition</li>
<li>Hessenberg matrix</li>
<li>Tridiagonal matrix</li>
<li>Invariant subspace</li>
</ul>
<h2>Notes</h2>
</body>
</html>