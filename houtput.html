<h1 id="firstHeading" class="firstHeading" lang="en">Array data structure</h1>
<h3>array identified structure tuple index consisting each computed least mathematical</h3>
<p>In computer science, an <b>array data structure</b> or simply an <b>array</b> is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored so that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.</p>
<h3>2000 10 i 32-bit 2036 2004 2008 × 9 4</h3>
<p>For example, an array of 10 32-bit integer variables, with indices 0 through 9, may be stored as 10 words at memory addresses 2000, 2004, 2008, … 2036, so that the element with index i has the address 2000 + 4 × i.</p>
<h3>mathematical sometimes two-dimensional tables correctly grid synonym used concept matrix</h3>
<p>Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term "vector" is used in computing to refer to an array, although tuples rather than vectors are more correctly the mathematical equivalent. Arrays are often used to implement tables, especially lookup tables; the word table is sometimes used as a synonym of array.</p>
<h3>processors computers exploit oldest devices most they optimized logic many</h3>
<p>Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.</p>
<h3>statement mostly required elements iterative things run arbitrarily tuples reason</h3>
<p>Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.</p>
<h3>implemented run-time selected often languages type provided kind term mean</h3>
<p>The term array is often used to mean array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.</p>
<h3>abstract capture adt intended model description theoretical essential term mean</h3>
<p>The term is also used, especially in the description of algorithms, to mean associative array or "abstract array", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.</p>
<h3></h3>
<p></p>
<h1 id="firstHeading" class="firstHeading" lang="en">Stack (abstract data type)</h1>
<h3>collection entity top pop push lifo structure known operations relation</h3>
<p>In computer science, a <b>stack</b> is a particular kind of abstract data type or collection in which the principal (or only) operations on the collection are the addition of an entity to the collection, known as push and removal of an entity, known as pop. The relation between the push and pop operations is such that the stack is a Last-In-First-Out (LIFO) data structure. In a LIFO data structure, the last element added to the structure must be the first one to be removed. This is equivalent to the requirement that, considered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. Often a peek or top operation is also implemented, returning the value of the top element without removing it.</p>
<h3></h3>
<p></p>
<h2>History</h2>
<h3>first neumann von stored-program purposes array-sorting indirect self-modifying digital computerp 159</h3>
<p>The first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. Von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer.p. 159 Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.</p>
<h3>1960 multi-dimensional arrays support runtime-flexible 60 1983 earliest 1972 cobol</h3>
<p>Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.</p>
<h2>Applications</h2>
<h3>consist rectangular kinds databases matrices vectors mathematical large arrays include</h3>
<p>Arrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.</p>
<h3>heaps deques vlists strings queues tables hash implement stacks other</h3>
<p>Arrays are used to implement other data structures, such as heaps, hash tables, deques, queues, stacks, strings, and VLists.</p>
<h3>allocation sometimes memory dynamic in-program portably historically emulate particularly allocate</h3>
<p>One or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate "dynamic memory" portably.</p>
<h3>control subroutine flow statements partial acted repetitive determine conjunction according</h3>
<p>Arrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple <strong>IF</strong> statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.</p>
<h3>stack state pop items reveals previously accept goes concealed empty</h3>
<p>A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A pop either reveals previously concealed items or results in an empty stack, but, if the stack is empty, it goes into underflow state, which means no items are present in stack to be removed.</p>
<h3>order stack longest elements nature therefore lower reverse natural performed</h3>
<p>A stack is a restricted data structure, because only a small number of operations are performed on it. The nature of the pop and push operations also means that stack elements have a natural order. Elements are removed from the stack in the reverse order to the order of their addition. Therefore, the lower elements are those that have been on the stack the longest.</p>
<h3></h3>
<p></p>
<h3></h3>
<p></p>
<h2>History</h2>
<h3>proposed subroutines 1957 was charles patent 1946 friedrich leonard australian</h3>
<p>The stack was first proposed in 1946, in the computer design of Alan M. Turing (who used the terms "bury" and "unbury") as a means of calling and returning from subroutines. Subroutines had already been implemented in Konrad Zuse's Z4 in 1945. Klaus Samelson and Friedrich L. Bauer of Technical University Munich proposed the idea in 1955 and filed a patent in 1957. The same concept was developed, independently, by the Australian Charles Leonard Hamblin in the first half of 1957.</p>
<h2>Abstract definition</h2>
<h3>defined implementation-free manner additions deletion restricted generally basic abstract science</h3>
<p>A stack is a basic computer science data structure and can be defined in an abstract, implementation-free manner, or it can be generally defined as a linear list of items in which all additions and deletion are restricted to one end that is Top.</p>
<h3>development vdm vienna description method stack this</h3>
<p>This is a VDM (Vienna Development Method) description of a stack:</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Data structure</h1>
<h3>computer organizing efficiently way particular science data used structure can</h3>
<p>In computer science, a <b>data structure</b> is a particular way of organizing data in a computer so that it can be used efficiently.</p>
<h3>kinds databases tables different tasks highly b-tree percentages suited retrieval</h3>
<p>Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use B-tree indexes for small percentages of data retrieval and compilers and databases use dynamic hash tables as look up tables.</p>
<h3>design large key algorithms efficient structures secondary manage retrieving emphasize</h3>
<p>Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Queue (abstract data type)</h1>
<h3>collection entities queue front added terminal removed fifo element known</h3>
<p>In computer science, a <b>queue</b> (/ˈkjuː/ <b>KEW</b>) is a particular kind of abstract data type or collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position, known as enqueue, and removal of entities from the front terminal position, known as dequeue. This makes the queue a First-In-First-Out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed. Often a peek or front operation is also entered, returning the value of the front element without dequeuing it. A queue is an example of a linear data structure, or more abstractly a sequential collection.</p>
<h3>held transport various performs events persons contexts processed entities buffer</h3>
<p>Queues provide services in computer science, transport, and operations research where various entities such as data, objects, persons, or events are stored and held to be processed later. In these contexts, the queue performs the function of a buffer.</p>
<h3>common coupled buffers object-oriented classes programs routines queues abstract implementations</h3>
<p>Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes. Common implementations are circular buffers and linked lists.</p>
<h3></h3>
<p></p>
<h1 id="firstHeading" class="firstHeading" lang="en">Associative array</h1>
<h3>collection pairs composed symbol appears map once dictionary table abstract</h3>
<p>In computer science, an <b>associative array</b>, <b>map</b>, <b>symbol table</b>, or <b>dictionary</b> is an abstract data type composed of a collection of  pairs, such that each possible key appears at most once in the collection.</p>
<h3>associated allow type operations with this data</h3>
<p>Operations associated with this data type allow:</p>
<h3>problem dictionary search classic addressed maintains task solve designing solution</h3>
<p>The <b>dictionary problem</b> is a classic computer science problem: the task of designing a data structure that maintains a set of data during 'search' 'delete' and 'insert' operations. A standard solution to the dictionary problem is a hash table; in some cases it is also possible to solve the problem using directly addressed arrays, binary search trees, or other more specialized structures.</p>
<h2>Element identifier and addressing formulas</h2>
<h3>objects scalar subscripts non-negative stored selected index object individual maps</h3>
<p>When data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indices are also called subscripts. An index maps the array value to a stored object.</p>
<h3>ways three indexed there elements which can an array be</h3>
<p>There are three ways in which the elements of an array can be indexed:</p>
<h3>major three column row two-dimensional multiple language thus four n-dimensional</h3>
<p>Arrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example a two-dimensional array <strong>A</strong> with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression <strong>A</strong> (in a row major language) or <strong>A</strong> (in a column major language) in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and n for an n-dimensional array.</p>
<h3>dimensionality rank specify dimension needed number indices called element or</h3>
<p>The number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.</p>
<h3>consecutive enumerated integers computed certain range formula restricted standard values</h3>
<p>In standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a "linear" formula on the indices.</p>
<h3>single subscript accessing represent involves column row dimension one-dimensional either</h3>
<p>A one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.</p>
<h3>declaration anarrayname consider int c example an</h3>
<p>As an example consider the C declaration <strong>int anArrayName;</strong></p>
<h3>datatype syntax  anarrayname</h3>
<p>Syntax : datatype anArrayname;</p>
<h3>anarrayname inclusive 0-9 expressions 10 example respectively contain elements int</h3>
<p>In the given example the array can contain 10 elements of any value available to the <strong>int</strong> type. In C, the array element indices are 0-9 inclusive in this case. For example, the expressions <strong>anArrayName</strong> and <strong>anArrayName</strong> are the first and last elements respectively.</p>
<h3>signatures function</h3>
<p>Function signatures:</p>
<h3>-&gt stack n isempty init boolean u error x </h3>
<p><i>
  init: -&gt; Stack
  push: N x Stack -&gt; Stack
  top: Stack -&gt; (N U ERROR)
  pop: Stack -&gt; Stack
  isempty: Stack -&gt; Boolean
</i></p>
<h3>indicates union u natural numbers n set where case element</h3>
<p>(where N indicates an element (natural numbers in this case), and U indicates set union)</p>
<h3>semantics</h3>
<p>Semantics:</p>
<h3>= s poppushi false isemptyinit toppushis popinit isemptypushi topinit init</h3>
<p><i>
  top(init()) = ERROR
  top(push(i,s)) = i
  pop(init()) = init()
  pop(push(i, s)) = s
  isempty(init()) = true
  isempty(push(i, s)) = false
</i></p>
<h2>Inessential operations</h2>
<h3>stack pop implementations top push top-most just observes condition empty</h3>
<p>In many implementations, a stack has more operations than "push" and "pop". An example is "top of stack", or "peek", which observes the top-most element without removing it from the stack. Since this can be done with a "pop" and a "push" with the same data, it is not essential. An underflow condition can occur in the "stack top" operation if the stack is empty, the same as "pop". Also, implementations often have a function which just returns whether the stack is empty.</p>
<h2>Software stacks</h2>
<h3>either helper demonstrate identifies allowed high few user interface what</h3>
<p>In most high level languages, a stack can be easily implemented either through an array or a linked list. What identifies the data structure as a stack in either case is not the implementation but the interface: the user is only allowed to pop or push items onto the array or linked list, with few other helper operations. The following will demonstrate both implementations, using C.</p>
<h3>zero-offset two-element aims stack element off popped pushed bottom therefore</h3>
<p>The <b>array implementation</b> aims to create an array where the first element (usually at the zero-offset) is the bottom. That is, <strong>array</strong> is the first element pushed onto the stack and the last element popped off. The program must keep track of the size, or the length of the stack. The stack itself can therefore be effectively implemented as a two-element structure in C:</p>
<h3></h3>
<p></p>
<h3></h3>
<p></p>
<h2>Overview</h2>
<h3>based structures data addresses itself memory items arithmetic principles pointer –</h3>
<p>Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer –  a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations; while the linked data structures are based on storing addresses of data items within the structure itself. Many data structures use both principles, sometimes combined in non-trivial ways (as in XOR linking).</p>
<h3>those structure operations instances writing analyzed observation motivates indirectly theoretical</h3>
<p>The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).</p>
<h2>Examples</h2>
<h3>types numerous upon primitive simpler built generally there data structures</h3>
<p>There are numerous types of data structures, generally built upon simpler primitive data types:</p>
<h2>Language support</h2>
<h3>languages assembly built-in support records such higher-level bcpl structs masm</h3>
<p>Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.</p>
<h3></h3>
<p></p>
<h2>Queue implementation</h2>
<h3>added new regardless impossible theoretically characteristic element how contained capacity</h3>
<p>Theoretically, one characteristic of a queue is that it does not have a specific capacity. Regardless of how many elements are already contained, a new element can always be added. It can also be empty, at which point removing an element will be impossible until a new element has been added again.</p>
<h3>queue circle trying size high array declared capacity overflow level</h3>
<p>Fixed length arrays are limited in capacity, but it is not true that items need to be copied towards the head of the queue. The simple trick of turning the array into a closed circle and letting the head and tail drift around endlessly in that circle makes it unnecessary to ever move items stored in the array. If n is the size of the array, then computing indices modulo n will turn the array into a circle. This is still the conceptually simplest way to construct a queue in a high level language, but it does admittedly slow things down a little, because the array indices must be compared to zero and the array size, which is comparable to the time taken to check whether an array index is out of bounds, which some languages do, but this will certainly be the method of choice for a quick and dirty implementation, or for any high level language that does not have pointer syntax. The array size must be declared ahead of time, but some implementations simply double the declared array size when overflow occurs. Most modern languages with objects or pointers can implement or come with libraries for dynamic lists. Such data structures may have not specified fixed capacity limit besides memory constraints. Queue overflow results from trying to add an element onto a full queue and queue underflow happens when trying to remove an element from an empty queue.</p>
<h3>queue bounded limited fixed items number to</h3>
<p>A bounded queue is a queue limited to a fixed number of items.</p>
<h3>efficient dequeuing—in operations—enqueuing fifo perform o1 queues implementations several there</h3>
<p>There are several efficient implementations of FIFO queues. An efficient implementation is one that can perform the operations—enqueuing and dequeuing—in O(1) time.</p>
<h3>implemented deque unshift double-ended dequeue enqueue shift popping ends ruby</h3>
<p>Queues may be implemented as a separate data type, or may be considered a special case of a double-ended queue (deque) and not implemented separately. For example, Perl and Ruby allow pushing and popping an array from both ends, so one can use <b>push</b> and <b>shift</b> functions to enqueue and dequeue a list (or, in reverse, one can use <b>unshift</b> and <b>pop</b>), although in some cases these operations are not efficient.</p>
<h3>queue class library since linkedlist j2se50 j2se gearman party arraydeque</h3>
<p>C++'s Standard Template Library provides a "<strong>queue</strong>" templated class which is restricted to only push/pop operations. Since J2SE5.0, Java's library contains a <strong>Queue</strong> interface that specifies queue operations; implementing classes include <strong>LinkedList</strong> and (since J2SE 1.6) <strong>ArrayDeque</strong>. PHP has an SplQueue class and third party libraries like beanstalk'd and Gearman.</p>
<h3>content-addressable hardware-level associative others primitive software many direct libraries arrays</h3>
<p>Many programming languages include associative arrays as primitive data types, and they are available in software libraries for many others. Content-addressable memory is a form of direct hardware-level support for associative arrays.</p>
<h3>fundamental memoization decorator pattern patterns applications including associative programming many</h3>
<p>Associative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern.</p>
<h3></h3>
<p></p>
<h3></h3>
<p></p>
<h2>Operations</h2>
<h3>association binding word refer creating process between known key new</h3>
<p>In an associative array, the association between a key and a value is often known as a "binding", and the same word "binding" may also be used to refer to the process of creating a new association.</p>
<h3>defined usually associative operations are an array for that</h3>
<p>The operations that are usually defined for an associative array are:</p>
<h3>bindings iterator returned determining constructing arbitrary such addition loop over</h3>
<p>In addition, associative arrays may also include other operations such as determining the number of bindings or constructing an iterator to loop over all the bindings. Usually, for such an operation, the order in which the bindings are returned may be arbitrary.</p>
<h3>associated key value related looks generalizes multimap bidirectional unique operate</h3>
<p>A multimap generalizes an associative array by allowing multiple values to be associated with a single key. A bidirectional map is a related abstract data type in which the bindings operate in both directions: each value must be associated with a unique key, and a second lookup operation takes a value as argument and looks up the key associated with that value.</p>
<h2>Example</h2>
<h3>address i b fixed c × located stride increment +</h3>
<p>For a vector with linear addressing, the element with index i is located at the address B + c × i, where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.</p>
<h3>begin 0 zeroth element indices specifies programmers reason rather valid</h3>
<p>If the valid element indices begin at 0, the constant B is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element "zeroth" rather than "first".</p>
<h3>b base address numbering 30c five 31 35 appropriate replaced</h3>
<p>However, one can choose the index of the first element by an appropriate choice of the base address B. For example, if the array has five elements, indexed 1 through 5, and the base address B is replaced by B + 30c, then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant B may not be the address of any element.</p>
<h3>· d + c ij address increments j coefficients respectively</h3>
<p>For a two-dimensional array, the element with indices i,j would have address B + c · i + d · j, where the coefficients c and d are the row and column address increments, respectively.</p>
<h3>i2 i1 k-dimensional ik … generally address indices more element</h3>
<p>More generally, in a k-dimensional array, the address of an element with indices i<sub>1</sub>, i<sub>2</sub>, …, i<sub>k</sub> is</p>
<h3>int example for</h3>
<p>For example: int a;</p>
<h3>row a21 a22 a23 a12 6 a11 a13 continuing stored</h3>
<p>This means that array a has 3 rows and 2 columns, and the array is of integer type. Here we can store 6 elements they are stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a<sub>11</sub>, a<sub>12</sub>, a<sub>13</sub>, a<sub>21</sub>, a<sub>22</sub>, a<sub>23</sub>.</p>
<h3>k fit coefficient power multiplications moreover shifting multiplication replaced any</h3>
<p>This formula requires only k multiplications and k additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.</p>
<h3>ck tuple distinct chosen coefficients valid maps every must address</h3>
<p>The coefficients c<sub>k</sub> must be chosen so that every valid index tuple maps to the address of a distinct element.</p>
<h3> size_t struct typedef int items size stack</h3>
<p><i>
typedef struct {
    size_t size;
    int items;
} STACK;
</i></p>
<h3>responsible prevent overrun counter initialize incrementing copying ps-&gtitems full ps-&gtsize</h3>
<p>The <strong>push()</strong> operation is used both to initialize the stack, and to store values to it. It is responsible for inserting (copying) the value into the <strong>ps-&gt;items</strong> array and for incrementing the element counter (<strong>ps-&gt;size</strong>). In a responsible C implementation, it is also necessary to check whether the array is already full to prevent an overrun.</p>
<h3> x overflow\n stacksize void pushstack *ps ps-&gtitems == stderr</h3>
<p><i>
void push(STACK *ps, int x)
{
    if (ps-&gt;size == STACKSIZE) {
        fputs("Error: stack overflow\n", stderr);
        abort();
    } else
        ps-&gt;items = x;
}
</i></p>
<h3>responsible value decrementing ps-&gtsize already check removing need pop c</h3>
<p>The <strong>pop()</strong> operation is responsible for removing a value from the stack, and decrementing the value of <strong>ps-&gt;size</strong>. A responsible C implementation will also need to check that the array is not already empty.</p>
<h3> underflow\n *ps popstack ps-&gtitems == stderr abort fputserror ps-&gtsize</h3>
<p><i>
int pop(STACK *ps)
{
    if (ps-&gt;size == 0){
        fputs("Error: stack underflow\n", stderr);
        abort();
    } else
        return ps-&gt;items;
}
</i></p>
<h3>dynamic we items grow shrink stack size amortized array o1</h3>
<p>If we use a dynamic array, then we can implement a stack that can grow or shrink as much as needed. The size of the stack is simply the size of the dynamic array. A dynamic array is a very efficient implementation of a stack, since adding items to or removing items from the end of a dynamic array is amortized O(1) time.</p>
<h3>simple head equally stack—it becoming straightforward sufficient linked-list only popped</h3>
<p>The <b>linked-list</b> implementation is equally simple and straightforward. In fact, a simple singly linked list is sufficient to implement a stack—it only requires that the head node or element can be removed, or popped, and a node can only be inserted by becoming the new head node.</p>
<h3>unlike corresponds typedef entire structure single implementation not stack node</h3>
<p>Unlike the array implementation, our structure typedef corresponds not to the entire stack structure, but to a single node:</p>
<h3>struct  *next stack typedef int data</h3>
<p><i>
typedef struct stack {
    int data;
    struct stack *next;
} STACK;
</i></p>
<h3>identical typical least those c implemented node singly such at</h3>
<p>Such a node is identical to a typical singly linked list node, at least to those that are implemented in C.</p>
<h3>framework standard library reused microsofts most mechanism collections net sort</h3>
<p>Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and Microsoft's .NET Framework.</p>
<h3>hide opaque details modular module separation clients programming object-oriented smalltalk</h3>
<p>Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java and Smalltalk may use classes for this purpose.</p>
<h3>versions concurrent threads simultaneously computing multiple known allow access many</h3>
<p>Many known data structures have concurrent versions that allow multiple computing threads to access the data structure simultaneously.</p>
<h3>e black paul nist bounded dictionary queue algorithms structures at</h3>
<p>Paul E. Black, Bounded queue at the NIST Dictionary of Algorithms and Data Structures.</p>
<h3>represented books patron patrons library out checked notation which may</h3>
<p>Suppose that the set of loans made by a library is to be represented in a data structure. Each book in a library may be checked out only by a single library patron at a time. However, a single patron may be able to check out multiple books. Therefore, the information about which books are checked out to which patrons may be represented by an associative array, in which the books are the keys and the patrons are the values. For instance (using notation from Python, or JSON (JavaScript Object Notation), in which a binding is represented by placing a colon between the key and the value), the current checkouts may be represented by an associative array</p>
<h3>alice  wuthering prejudice pride heights great john expectations and</h3>
<p><i>
{
    "Great Expectations": "John",
    "Pride and Prejudice": "Alice",
    "Wuthering Heights": "Alice"
}
</i></p>
<h3>book would john cause operation out leading checked returns checks</h3>
<p>A lookup operation with the key "Great Expectations" in this array would return the name of the person who checked out that book, John. If John returns his book, that would cause a deletion operation in the associative array, and if Pat checks out another book, that would cause an insertion operation, leading to a different state:</p>
<h3>alice  brothers karamazov pat wuthering heights prejudice pride and</h3>
<p><i>
{
    "Pride and Prejudice": "Alice",
    "The Brothers Karamazov": "Pat",
    "Wuthering Heights": "Alice"
}
</i></p>
<h3>key raise exception longer great expectations present state lookup would</h3>
<p>In this new state, the same lookup as before, with the key "Great Expectations", would raise an exception, because this key is no longer present in the array.</p>
<h2>Implementation</h2>
<h3>bindings dictionary small implement sense total factors running time association</h3>
<p>For dictionaries with very small numbers of bindings, it may make sense to implement the dictionary using an association list, a linked list of bindings. With this implementation, the time to perform the basic dictionary operations is linear in the total number of bindings; however, it is easy to implement and the constant factors in its running time are small.</p>
<h3>keyspace technique k binding cell simple absence usable narrow value</h3>
<p>Another very simple implementation technique, usable when the keys are restricted to a narrow range of integers, is direct addressing into an array: the value for a given key k is stored at the array cell A, or if there is no binding for k then the cell stores a special sentinel value that indicates the absence of a binding. As well as being simple, this technique is fast: each dictionary operation takes constant time. However, the space requirement for this structure is the size of the entire keyspace, making it impractical unless the keyspace is small.</p>
<h3>hash table key binding cell looking given function based instead</h3>
<p>The most frequently used general purpose implementation of an associative array is with a hash table: an array of bindings, together with a hash function that maps each possible key into an array index. The basic idea of a hash table is that the binding for a given key is stored at the position given by applying the hash function to that key, and that lookup operations are performed by looking at that cell of the array and using the binding found there. However, hash table based dictionaries must be prepared to handle collisions that occur when two keys are mapped by the hash function to the same index, and many different collision resolution strategies have been developed for dealing with this situation, often based either on open addressing (looking at a sequence of hash table indices instead of a single index, until finding either the given key or an empty cell) or on hash chaining (storing a small association list instead of a single binding in each hash table cell).</p>
<h1 id="firstHeading" class="firstHeading" lang="en">Linked list</h1>
<h3>b 1 c1 fortran respectively minimum 0 like indices through</h3>
<p>If the minimum legal value for every index is 0, then B is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address B. Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing B by B + c<sub>1</sub> - − 3 c<sub>1</sub> will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition; while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.</p>
<h3>dope vector useful reversing parameters descriptor sub-array pass selecting completely</h3>
<p>The addressing formula is completely defined by the dimension d, the base address B, and the increments c<sub>1</sub>, c<sub>2</sub>, …, c<sub>k</sub>. It is often useful to pack these parameters into a record called the array's descriptor or stride vector or dope vector. The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.</p>
<h3>contiguous sub-arrays non-contiguous occupy slicing chosen coefficients area necessary even</h3>
<p>Often the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.</p>
<h3>layouts systematic matrix compact consider two-dimensional there two example for</h3>
<p>There are two systematic compact layouts for a two-dimensional array. For example, consider the matrix</p>
<h3>row consecutive adopted elements layout statically row-major declared lower positions</h3>
<p>In the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:</p>
<h3>column consecutive traditionally elements column-major fortran lower than order address</h3>
<p>In column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:</p>
<h3>major index order respect differ puts analogous tuples positions three</h3>
<p>For arrays with three or more indices, "row major order" puts in consecutive positions any two elements whose index tuples differ only by one in the last index. "Column major order" is analogous with respect to the first index.</p>
<h3>column-major order use scan scattered product a·b sparsely sophisticated row-</h3>
<p>In systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product A·B of two matrices, it would be best to have A stored in row-major order, and B in column-major order.</p>
<h3>dynamic old static version infrequently consequently array insertions see allocating</h3>
<p>Static arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a dynamic version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.</p>
<h3>new push initializes adds receiving along target stack allocating non-empty</h3>
<p>The <strong>push()</strong> operation both initializes an empty stack, and adds a new node to a non-empty one. It works by receiving a data value to push onto the stack, along with a target stack, creating a new node by allocating memory for it, and then inserting it into a linked list as the new head:</p>
<h3>*/ /*  = *head new *node mallocsizeofstack ? node\n</h3>
<p><i>
void push(STACK **head, int value)
{
    STACK *node = malloc(sizeof(STACK));  /* create a new node */
 
    if (node == NULL){
        fputs("Error: no space available for node\n", stderr);
        abort();
    } else {                                      /* initialize node */
        node-&gt;data = value;
        node-&gt;next = empty(*head) ? NULL : *head; /* insert new head if any */
        *head = node;
    }
}
</i></p>
<h3>head assigns removes checks popping from second whether previous pop</h3>
<p>A <strong>pop()</strong> operation removes the head from the linked list, and assigns the pointer to the head to the previous second node. It checks whether the list is empty before popping from it:</p>
<h3> *head = int freetop top-&gtnext top-&gtdata *top //pop value</h3>
<p><i>
int pop(STACK **head)
{
    if (empty(*head)) {                          /* stack is empty */
       fputs("Error: stack underflow\n", stderr);
       abort();
    } else {                                     //pop a node
        STACK *top = *head;
        int value = top-&gt;data;
        *head = top-&gt;next;
        free(top);
        return value;
    }
}
</i></p>
<h3>lisp adobe language-defined forth-like visible postscript designed languages around manipulated</h3>
<p>Some languages, like Perl, LISP and Python, do not call for stack implementations, since <b>push</b> and <b>pop</b> functions are available for any list. All Forth-like languages (such as Adobe PostScript) are also designed around language-defined stacks that are directly visible to and manipulated by the programmer. Examples from Common Lisp:</p>
<h3>⇒ b c  new setf list pop push</h3>
<p><i>
(setf list (list 'a 'b 'c))
;; ⇒ (A B C)
(pop list)
;; ⇒ A
list
;; ⇒ (B C)
(push 'new list)
;; ⇒ (NEW B C)
</i></p>
<h3>class library ignores specialization vector---this inherited flaw constraint splstack stack</h3>
<p>C++'s Standard Template Library provides a "<strong>stack</strong>" templated class which is restricted to only push/pop operations. Java's library contains a <strong>Stack</strong> class that is a specialization of <strong>Vector</strong>---this could be considered a design flaw, since the inherited get() method from <strong>Vector</strong> ignores the LIFO constraint of the <strong>Stack</strong>. PHP has an SplStack class.</p>
<h2>Hardware stacks</h2>
<h3>accessing allocating architecture level means common stacks use memory at</h3>
<p>A common use of stacks at the architecture level is as a means of allocating and accessing memory.</p>
<h3>trees handle key radix queried query greater closest ones restrictions</h3>
<p>Dictionaries may also be stored in binary search trees or in data structures specialized to a particular type of keys such as radix trees, tries, Judy arrays, or van Emde Boas trees, but these implementation methods are less efficient than hash tables as well as placing greater restrictions on the types of data that they can handle. The advantages of these alternative structures come from their ability to handle operations beyond the basic ones of an associative array, such as finding the binding whose key is the closest to a queried key, when the query is not itself present in the set of bindings.</p>
<h2>Language support</h2>
<h3>standard language subscripting array-like package systems system syntax part built</h3>
<p>Associative arrays can be implemented in any programming language as a package and many language systems provide them as part of their standard library. In some languages, they are not only built into the standard system, but have special syntax, often using array-like subscripting.</p>
<h3>associative support tcl container syntactic mumps optionally snobol4 awk rexx</h3>
<p>Built-in syntactic support for associative arrays was introduced by SNOBOL4, under the name "table". MUMPS made multi-dimensional associative arrays, optionally persistent, its key data structure. SETL supported them as one possible implementation of sets and maps. Most modern scripting languages, starting with AWK and including Rexx, Perl, Tcl, JavaScript, Python, Ruby, and Lua, support associative arrays as a primary container type. In many more languages, they are available as library functions without special syntax.</p>
<h3>they called c++ map see associative all tables realbasic objective-c</h3>
<p>In Smalltalk, Objective-C, .NET, Python, REALbasic, and Swift they are called dictionaries; in Perl, Ruby and Seed7 they are called hashes; in C++, Java, Go, Clojure, Scala, OCaml, Haskell they are called maps (see map (C++), unordered_map (C++), and <strong>Map</strong>); in Common Lisp and Windows PowerShell, they are called hash tables (since both typically use this implementation). In PHP, all arrays can be associative, except that the keys are limited to integers and strings. In JavaScript (see also JSON), all objects behave as associative arrays. In Lua, they are called tables, and are used as the primitive building block for all data structures. In Visual FoxPro, they are called Collections. The D language also has support for associative arrays </p>
<h3>sequence group complex under composed represent consisting words variants removal</h3>
<p>In computer science, a <b>linked list</b> is a data structure consisting of a group of nodes which together represent a sequence. Under the simplest form, each node is composed of a data and a reference (in other words, a link) to the next node in the sequence; more complex variants add additional links. This structure allows for efficient insertion or removal of elements from any position in the sequence.</p>
<h3>abstract common implement though s-expressions basis uncommon other data among</h3>
<p>Linked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists (the abstract data type), stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement the other data structures directly without using a list as the basis of implementation.</p>
<h3>removed link disk compiling reorganization maintained conventional contiguously running reallocation</h3>
<p>The principal benefit of a linked list over a conventional array is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while an array has to be declared in the source code, before compiling and running the program. Linked lists allow insertion and removal of nodes at any point in the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.</p>
<h3>node inserted — datum obtaining follows- operations — locating last maintained scanning</h3>
<p>On the other hand, simple linked lists by themselves do not allow random access to the data, or any form of efficient indexing. Thus, many basic operations — such as obtaining the last node of the list (assuming that the last node is not maintained as separate node reference in the list structure), or finding a node that contains a given datum, or locating the place where a new node should be inserted — may require sequential scanning of most or all of the list elements. The advantages and disadvantages of using linked lists are as follows:-</p>
<h3>advantages</h3>
<p>Advantages:</p>
<h3>disadvantages</h3>
<p>Disadvantages:</p>
<h3></h3>
<p></p>
<h3></h3>
<p></p>
<h2>History</h2>
<h3>count do reallocate size pascal array effectively capacity strings maximum</h3>
<p>Some array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.</p>
<h3>formulas non-linear degree triangular occasionally polynomial instance complicated compact 2</h3>
<p>More complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.</p>
<h2>Efficiency</h2>
<h3>take deterministic worst select hold n store constant space number</h3>
<p>Both store and select take (deterministic worst case) constant time. Arrays take linear (O(n)) space in the number of elements n that they hold.</p>
<h3>faster cache misses iteration locations optimized contiguous than size elements</h3>
<p>In an array with element size k and on a machine with a cache line size of B bytes, iterating through an array of n elements requires the minimum of ceiling(nk/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/k better than the number of cache misses needed to access n elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does not mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.</p>
<h3>single compact overhead bit stored up different happen packed per-array</h3>
<p>Memory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead, e.g. to store index bounds, but this is language-dependent. It can also happen that elements stored in an array require less memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called packed arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.</p>
<h3>accesses parallelism predictable statically patterns major source access with data</h3>
<p>Array accesses with statically predictable access patterns are a major source of data parallelism.</p>
<h3>reserve additional storage arrays particularly Θn growable whereas similar deleting</h3>
<p>Growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ(n)) additional storage, whereas arrays do not reserve additional storage.</p>
<h3>values huge functionality patricia overheads billion sparse associative arrays boas</h3>
<p>Associative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.</p>
<h3>stack origin zero size points initially pointer recently referenced register</h3>
<p>A typical stack is an area of computer memory with a fixed origin and a variable size. Initially the size of the stack is zero. A stack pointer, usually in the form of a hardware register, points to the most recently referenced location on the stack; when the stack has a size of zero, the stack pointer points to the origin of the stack.</p>
<h3>applicable stacks all two operations are to</h3>
<p>The two operations applicable to all stacks are:</p>
<h3>stack away begins displaced variations principle expands indicate extent origin</h3>
<p>There are many variations on the basic principle of stack operations. Every stack has a fixed location in memory at which it begins. As data items are added to the stack, the stack pointer is displaced to indicate the current extent of the stack, which expands away from the origin.</p>
<h3>stack origin 1000 pointer beyond causes grows occurs addresses cross</h3>
<p>Stack pointers may point to the origin of a stack or to a limited range of addresses either above or below the origin (depending on the direction in which the stack grows); however, the stack pointer cannot cross the origin of the stack. In other words, if the origin of the stack is at address 1000 and the stack grows downwards (towards addresses 999, 998, and so on), the stack pointer must never be incremented beyond 1000 (to 1001, 1002, etc.). If a pop operation on the stack causes the stack pointer to move past the origin of the stack, a stack underflow occurs. If a push operation causes the stack pointer to increment or decrement beyond the maximum extent of the stack, a stack overflow occurs.</p>
<h3>rely heavily environments additional provide stacks example operations on some</h3>
<p>Some environments that rely heavily on stacks may provide additional operations, for example:</p>
<h3>bottom visualized growing top right stacks from image growth visualization</h3>
<p>Stacks are often visualized growing from the bottom up (like real-world stacks). They may also be visualized growing from left to right, so that "topmost" becomes "rightmost", or even growing from top to bottom. The important feature is that the top of the stack is in a fixed position. The image above and to the right is an example of a top to bottom growth visualization: the top (28) is the stack 'bottom', since the stack 'top' is where items are pushed or popped from. Sometimes stacks are also visualized metaphorically, such as coin holders or Pez dispensers.</p>
<h3>third second rotate visualizations right here equivalent move first process</h3>
<p>A right rotate will move the first element to the third position, the second to the first and the third to the second. Here are two equivalent visualizations of this process:</p>
<h3>apple cucumber banana ===right rotate==&gt </h3>
<p><i>
apple                         banana
banana    ===right rotate==&gt;  cucumber
cucumber                      apple
</i></p>
<h3>apple cucumber banana ===left rotate==&gt </h3>
<p><i>
cucumber                      apple
banana    ===left rotate==&gt;   cucumber
apple                         banana
</i></p>
<h3>processing theory conference newell translation proceedings language information 1957 simon</h3>
<p>Linked lists were developed in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation as the primary data structure for their Information Processing Language. IPL was used by the authors to develop several early artificial intelligence programs, including the Logic Theory Machine, the General Problem Solver, and a computer chess program. Reports on their work appeared in IRE Transactions on Information Theory in 1956, and several conference proceedings from 1957 to 1959, including Proceedings of the Western Joint Computer Conference in 1957 and 1958, and Information Processing (Proceedings of the first UNESCO International Conference on Information Processing) in 1959. The now-classic diagram consisting of blocks representing list nodes with arrows pointing to successive list nodes appears in "Programming the Logic Theory Machine" by Newell and Shaw in Proc. WJCC, February 1957. Newell and Simon were recognized with the ACM Turing Award in 1975 for having "made basic contributions to artificial intelligence, the psychology of human cognition, and list processing". The problem of machine translation for natural language processing led Victor Yngve at Massachusetts Institute of Technology (MIT) to use linked lists as data structures in his COMIT programming language for computer research in the field of linguistics. A report on this language entitled "A programming language for mechanical translation" appeared in Mechanical Translation in 1958.</p>
<h3>he was symbolic computation standing mccarthy lisps paper 1960 1958</h3>
<p>LISP, standing for list processor, was created by John McCarthy in 1958 while he was at MIT and in 1960 he published its design in a paper in the Communications of the ACM, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I". One of LISP's major data structures is the linked list.</p>
<h3>review article languages summarized laboratory established 1964 april 1961 bobrow</h3>
<p>By the early 1960s, the utility of both linked lists and languages which use these structures as their primary data representation was well established. Bert Green of the MIT Lincoln Laboratory published a review article entitled "Computer languages for symbol manipulation" in IRE Transactions on Human Factors in Electronics in March 1961 which summarized the advantages of the linked list approach. A later review article, "A Comparison of list-processing computer languages" by Bobrow and Raphael, appeared in Communications of the ACM in April 1964.</p>
<h3>cpu file systems motorola developed sector marketed pointed consultants mini-flex</h3>
<p>Several operating systems developed by Technical Systems Consultants (originally of West Lafayette Indiana, and later of Chapel Hill, North Carolina) used singly linked lists as file structures. A directory entry pointed to the first sector of a file, and succeeding portions of the file were located by traversing pointers. Systems using this technique included Flex (for the Motorola 6800 CPU), mini-Flex (same CPU), and Flex9 (for the Motorola 6809 CPU). A variant developed by TSC for and marketed by Smoke Signal Broadcasting in California, used doubly linked lists in the same manner.</p>
<h3>was system forward backward file link crash catalog flea utility</h3>
<p>The TSS/360 operating system, developed by IBM for the System 360/370 machines, used a double linked list for their file system catalog. The directory structure was similar to Unix, where a directory could contain files and/or other directories and extend to any depth. A utility flea was created to fix file system problems after a crash, since modified portions of the file catalog were sometimes in memory when a crash occurred. Problems were detected by comparing the forward and backward links for consistency. If a forward link was corrupt, then if a backward link to the infected node was found, the forward link was set to the node with the backward link. A humorous comment in the source code where this utility was invoked stated "Everyone knows a flea collar gets rid of bugs in cats".</p>
<h2>Basic concepts and nomenclature</h2>
<h3>record called often each element node or linked an list</h3>
<p>Each record of a linked list is often called an <b>element</b> or <b>node</b>.</p>
<h3>next fields cargo remaining payload information known field contains link</h3>
<p>The field of each node that contains the address of the next node is usually called the <b>next link</b> or <b>next pointer</b>. The remaining fields are known as the <b>data</b>, <b>information</b>, <b>value</b>, <b>cargo</b>, or <b>payload</b> fields.</p>
<h3>olog require time n permit balanced growable Θn whereas arbitrary</h3>
<p>Balanced trees require O(log n) time for indexed access, but also permit inserting or deleting elements in O(log n) time, whereas growable arrays require linear (Θ(n)) time to insert or delete elements at an arbitrary position.</p>
<h3>worse linear middle time still removal take typically indexed insertion</h3>
<p>Linked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.</p>
<h3>row alternative vector column one pointers each would worthwhile saves</h3>
<p>An Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row. Thus an element in row i and column j of an array A would be accessed by double indexing (A in typical notation). This alternative structure allows ragged or jagged arrays, where each row may have a different size — or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.<br clear="left"></p>
<h2>Dimension</h2>
<h3>dimension thus subset rectangle discrete array domain select combinations seen</h3>
<p>The dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array a rectangle of data, a three-dimensional array a block of data, etc.</p>
<h3>matrices 20-dimensional confused domain similarly three-dimensional 5 4 rows columns</h3>
<p>This should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.</p>
<h3>towards bottom stack addresses top memory actually holding higher irrespective</h3>
<p>A stack is usually represented in computers by a block of memory cells, with the "bottom" at a fixed location, and the stack pointer holding the address of the current "top" cell in the stack. The top and bottom terminology are used irrespective of whether the stack actually grows towards lower memory addresses or towards higher memory addresses.</p>
<h3>item stack depending topmost updated pushed new onto pointer next</h3>
<p>Pushing an item on to the stack adjusts the stack pointer by the size of the item (either decrementing or incrementing, depending on the direction in which the stack grows in memory), pointing it to the next cell, and copies the new top item to the stack area. Depending again on the exact implementation, at the end of a push operation, the stack pointer may point to the next unused location in the stack, or it may point to the topmost item in the stack. If the stack points to the current topmost item, the stack pointer will be updated before a new item is pushed onto the stack; if it points to the next available location in the stack, it will be updated after the new item is pushed onto the stack.</p>
<h3>inverse stack opposite topmost popping updated pushing item simply removed</h3>
<p>Popping the stack is simply the inverse of pushing. The topmost item in the stack is removed and the stack pointer is updated, in the opposite order of that used in the push operation.</p>
<h3>68000 modes instructions via registers processors like stack addressing pointers</h3>
<p>Most CPUs have registers that can be used as stack pointers. Processor families like the x86, Z80, 6502, and many others have special instructions that implicitly use a dedicated (hardware) stack pointer to conserve opcode space. Some processors, like the PDP-11 and the 68000, also have special addressing modes for implementation of stacks, typically with a semi-dedicated stack pointer as well (such as A7 in the 68000). However, in most processors, several different registers may be used as additional stack pointers as needed (whether updated via addressing modes or via add/sub instructions).</p>
<h3>x87 register registers implementations code possible also top-of-stack bandwidth organised</h3>
<p>The x87 floating point architecture is an example of a set of registers organised as a stack where direct access to individual registers (relative the current top) is also possible. As with stack-based machines in general, having the top-of-stack as an implicit argument allows for a small machine code footprint with a good usage of bus bandwidth and code caches, but it also prevents some types of optimizations possible on processors permitting random access to the register file for all (two or three) operands. A stack structure also makes superscalar implementations with register renaming (for speculative execution) somewhat more complex to implement, although it is still feasible, as exemplified by modern x87 implementations.</p>
<h3>sun strategy am29000 register-stack amd sparc architectures i960 intel windows</h3>
<p>Sun SPARC, AMD Am29000, and Intel i960 are all examples of architectures using register windows within a register-stack as another strategy to avoid the use of slow main memory for function arguments and return values.</p>
<h3>were microcontrollers microprocessors machines directly accessible nc4016 mini pic microcode</h3>
<p>There are also a number of small microprocessors that implements a stack directly in hardware and some microcontrollers have a fixed-depth stack that is not directly accessible. Examples are the PIC microcontrollers, the Computer Cowboys MuP21, the Harris RTX line, and the Novix NC4016. Many stack-based microprocessors were used to implement the programming language Forth at the microcode level. Stacks were also used as a basis of a number of mainframes and mini computers. Such machines were called stack machines, the most famous being the Burroughs B5000.</p>
<h2>Applications</h2>
<h3>pile book books top blank everyday life printer sheets tray</h3>
<p>Stacks are present everyday life, from the books in a library, to the blank sheets of paper in a printer tray. All these applications follow the Last In First Out (LIFO) logic, which means that (for example) a book is added on top of a pile of books, while removing a book from a pile also takes the book on top of a pile.</p>
<h3>head rest pronounced could-er derived node called payload may car</h3>
<p>The <b>head</b> of a list is its first node. The <b>tail</b> of a list may refer either to the rest of the list after the head, or to the last node in the list. In Lisp and some derived languages, the next node may be called the <b>cdr</b> (pronounced could-er) of the list, while the payload of the head node may be called the <b>car</b>.</p>
<h3>field singly next nodes traversal line performed deletion contain points</h3>
<p>Singly linked lists contain nodes which have a data field as well as a <b>next</b> field, which points to the next node in line of nodes. Operations that can be performed on singly linked lists include insertion, deletion and traversal.</p>
<h3>link prevprevious forwards besides backwards next-node sequence second pointing links</h3>
<p>In a <b>doubly linked list</b>, each node contains, besides the next-node link, a second link field pointing to the previous node in the sequence. The two links may be called <b>forward</b>(<b>s</b>) and <b>backwards</b>, or <b>next</b> and <b>prev</b>(<b>previous</b>).</p>
<h3>technique xor-linking high-level therefore ability bit requires known addresses allows</h3>
<p>A technique known as XOR-linking allows a doubly linked list to be implemented using a single link field in each node. However, this technique requires the ability to do bit operations on addresses, and therefore may not be available in some high-level languages.</p>
<h3>multiply each date connect orders department birth treated more opposite</h3>
<p>In a <b>multiply linked list</b>, each node contains two or more link fields, each field being used to connect the same set of data records in a different order (e.g., by name, by department, by date of birth, etc.). While doubly linked lists can be seen as special cases of multiply linked list, the fact that the two orders are opposite to each other leads to simpler and more efficient algorithms, so they are usually treated as a separate case.</p>
<h3>said further lack open indicate convention otherwise make less circularly</h3>
<p>In the last node of a list, the link field often contains a null reference, a special value used to indicate the lack of further nodes. A less common convention is to make it point to the first node of the list; in that case the list is said to be 'circular' or 'circularly linked'; otherwise it is said to be 'open' or 'linear'.</p>
<h3>versa vice change occurs said back front tail head doubly</h3>
<p>In the case of a circular doubly linked list, the only change that occurs is that the end, or "tail", of the said list is linked back to the front, or "head", of the list and vice versa.</p>
<h3>dereferenced list-handling accelerates last ensuring safely simplifies convention dummy and/or</h3>
<p>In some implementations, an extra <b>sentinel</b> or <b>dummy</b> node may be added before the first data record and/or after the last one. This convention simplifies and accelerates some list-handling algorithms, by ensuring that all links can be safely dereferenced and that every list (even one that contains no data elements) always has a "first" and "last" node.</p>
<h3>sentinel nodes usually saying empty has said zero being records</h3>
<p>An empty list is a list that contains no data records. This is usually the same as saying that it has zero nodes. If sentinel nodes are being used, the list is usually said to be empty when it has only sentinel nodes.</p>
<h3>few below computing applications stacks are in</h3>
<p>Below are a few applications of stacks in computing.</p>
<h3>expressions stack employing notations prefix polish parsed infix context-free low</h3>
<p>Calculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low level code. Most programming languages are context-free languages, allowing them to be parsed with stack based machines.</p>
<h3>path we point backtracking destination wrong stacks done find return</h3>
<p>Another important application of stacks is backtracking. Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking.</p>
<h3>stack-oriented stack virtual machine return operand character p-code define meaning</h3>
<p>A number of programming languages are stack-oriented, meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine.</p>
<h3>calling caller runtime function call stack return restore protocol conventions –</h3>
<p>Almost all calling conventions – computer runtime memory environments – use a special stack (the "call stack") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer.</p>
<h3>procedure local security introducing implications aware serious exits stack programming</h3>
<p>Some programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program.</p>
<h2>Security</h2>
<h3>environments vulnerable working pitfalls attacks breaches care programmers security ways</h3>
<p>Some computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations.</p>
<h3>records link physically stored indices referenced part separate fields need</h3>
<p>The link fields need not be physically part of the nodes. If the data records are stored in an array and referenced by their indices, the link field may be stored in a separate array with the same indices as the data records.</p>
<h3>handles handle algorithms reference resulting lists situations input convenient often</h3>
<p>Since a reference to the first node gives access to the whole list, that reference is often called the <b>address</b>, <b>pointer</b>, or <b>handle</b> of the list. Algorithms that manipulate linked lists usually get such handles to the input lists and return the handles to the resulting lists. In fact, in the context of such algorithms, the word "list" often means "list handle". In some situations, however, it may be convenient to refer to a list by a handle that consists of two links, pointing to its first and last nodes.</p>
<h3>sentinels alternatives listed circular arbitrarily combined almost etc above lists</h3>
<p>The alternatives listed above may be arbitrarily combined in almost every way, so one may have circular doubly linked lists without sentinels, circular singly linked lists with sentinels, etc.</p>
<h2>Tradeoffs</h2>
<h3>well involving circumstances tradeoffs choices problems suited work might cause</h3>
<p>As with most choices in computer programming and design, no method is well suited to all circumstances. A linked list data structure might work well in one case, but cause problems in another. This is a list of some of the common tradeoffs involving linked list structures.</p>
<h3>dynamic reserved allocates exceeded reallocated keeps contiguously possibly copied count</h3>
<p>A dynamic array is a data structure that allocates all elements contiguously in memory, and keeps a count of the current number of elements. If the space reserved for the dynamic array is exceeded, it is reallocated and (possibly) copied, an expensive operation.</p>
<h3>insertion point dynamic before impedes slot marking somehow fragmentation vacant</h3>
<p>Linked lists have several advantages over dynamic arrays. Insertion or deletion of an element at a specific point of a list, assuming that we have indexed a pointer to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. While one can "delete" an element from an array in constant time by somehow marking its slot as "vacant", this causes fragmentation that impedes the performance of iteration.</p>
<h3>due reallocation cost still reallocate — eventually wasting contiguity underlying resized</h3>
<p>Moreover, arbitrarily many elements may be inserted into a linked list, limited only by the total memory available; while a dynamic array will eventually fill up its underlying array data structure and will have to reallocate — an expensive operation, one that may not even be possible if memory is fragmented, although the cost of reallocation can be averaged over insertions, and the cost of an insertion due to reallocation would still be amortized O(1). This helps with appending elements at the array's end, but inserting into (or removing from) middle positions still carries prohibitive costs due to data moving to maintain contiguity. An array from which many elements are removed may also have to be resized in order to avoid wasting too much space.</p>
<h3>access sequential lists on allow dynamic unsuitable optimal fixed-size heapsort</h3>
<p>On the other hand, dynamic arrays (as well as fixed-size array data structures) allow constant-time random access, while linked lists allow only sequential access to elements. Singly linked lists, in fact, can be easily traversed in only one direction. This makes linked lists unsuitable for applications where it's useful to look up an element by its index quickly, such as heapsort. Sequential access on arrays and dynamic arrays is also faster than on linked lists on many machines, because they have optimal locality of reference and thus make good use of data caching.</p>
<h3>procedure return moved calls stack information location program causing critical</h3>
<p>For example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail.</p>
<h3>attacker input program provided within return such doing smashing parties</h3>
<p>Malicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations.</p>
<h3>verify attack security do items breach variation undersized extremely popular</h3>
<p>This type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.</p>
<h3>small storage exceed allocator wasteful naïve pools characters solved control</h3>
<p>Another disadvantage of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data items such as characters or boolean values, because the storage overhead for the links may exceed by a factor of two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data). It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.</p>
<h3>references extends hybrid combine increasing coding representations try solutions referencing</h3>
<p>Some hybrid solutions try to combine the advantages of the two representations. Unrolled linked lists store several elements in each list node, increasing cache performance while decreasing memory overhead for references. CDR coding does both these as well, by replacing references with the actual data referenced, which extends off the end of the referencing record.</p>
<h3>person circle you josephus people election nth vs poor shows</h3>
<p>A good example that highlights the pros and cons of using dynamic arrays vs. linked lists is by implementing a program that resolves the Josephus problem. The Josephus problem is an election method that works by having a group of people stand in a circle. Starting at a predetermined person, you count around the circle n times. Once you reach the nth person, take them out of the circle and have the members close the circle. Then count around the circle the same n times and repeat the process, until only one person is left. That person wins the election. This shows the strengths and weaknesses of a linked list vs. a dynamic array, because if you view the people as connected nodes in a circular linked list then it shows how easily the linked list is able to delete nodes (as it only has to rearrange the links to the different nodes). However, the linked list will be poor at finding the next person to remove and will need to search through the list until it finds that person. A dynamic array, on the other hand, will be poor at deleting nodes (or elements) as it cannot remove one node without individually shifting all the elements up the list by one. However, it is exceptionally easy to find the nth person in the circle by directly referencing them by their position in the array.</p>
<h3>problem subject solving concerns ranking algorithm trivial conversion conventional parallel</h3>
<p>The list ranking problem concerns the efficient conversion of a linked list representation into an array. Although trivial for a conventional computer, solving this problem by a parallel algorithm is complicated and has been the subject of much research.</p>
<h3>trees balanced maintain tree overhead schemes avl manipulations automatically access</h3>
<p>A balanced tree has similar memory access patterns and space overhead to a linked list while permitting much more efficient indexing, taking O(log n) time instead of O(n) for a random access. However, insertion and deletion operations are more expensive due to the overhead of tree manipulations to maintain balance. Schemes exist for trees to automatically maintain themselves in a balanced state: AVL trees or red-black trees.</p>
<h3>advantages offer preferable lists linear situations and/or make some over</h3>
<p>While doubly linked and/or circular lists have advantages over singly linked linear lists, linear lists offer some advantages that make them preferable in some situations.</p>
<h3>recursive smaller commands enumerating adapted singly lists linear iterative solutions</h3>
<p>A singly linked linear list is a recursive data structure, because it contains a pointer to a smaller object of the same type. For that reason, many operations on singly linked linear lists (such as merging two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using iterative commands. While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.</p>
<h3>portion different new remains sub-list one — lists belong terminal final</h3>
<p>Linear singly linked lists also allow tail-sharing, the use of a common final portion of sub-list as the terminal portion of two different lists. In particular, if a new node is added at the beginning of a list, the former list remains available as the tail of the new one — a simple example of a persistent data structure. Again, this is not true with the other variants: a node may never belong to two different circular or doubly linked lists.</p>
<h3>end-sentinel car cdr lisp every denoted non-circular nil proper safely</h3>
<p>In particular, end-sentinel nodes can be shared among singly linked non-circular lists. The same end-sentinel node may be used for every such list. In Lisp, for example, every proper list ends with a link to a special node, denoted by <strong>nil</strong> or <strong>()</strong>, whose <strong>CAR</strong> and <strong>CDR</strong> links point to itself. Thus a Lisp procedure can safely take the <strong>CAR</strong> or <strong>CDR</strong> of any list.</p>
<h3>emulated fancy efficiency cost complexity variants variables together limited extra</h3>
<p>The advantages of the fancy variants are often limited to the complexity of the algorithms, not in their efficiency. A circular list, in particular, can usually be emulated by a linear list together with two variables that point to the first and last nodes, at no extra cost.</p>
<h3>directions require node allow doubly do elementary per easier one</h3>
<p>Double-linked lists require more space per node (unless one uses XOR-linking), and their elementary operations are more expensive; but they are often easier to manipulate because they allow fast and easy sequential access to the list in both directions. In a doubly linked list, one can insert or delete a node in a constant number of operations given only that node's address. To do the same in a singly linked list, one must have the address of the pointer to that node, which is either the handle for the whole list (in case of the first node) or the link field in the previous node. Some algorithms require access in both directions. On the other hand, doubly linked lists do not allow tail-sharing and cannot be used as persistent data structures.</p>
<h3>order round-robin polygon option released processes corners serves time-shared naturally</h3>
<p>A circularly linked list may be a natural option to represent arrays that are naturally circular, e.g. the corners of a polygon, a pool of buffers that are used and released in FIFO ("first in, first out") order, or a set of processes that should be time-shared in round-robin order. In these applications, a pointer to any node serves as a handle to the whole list.</p>
<h3>circular pointer access gives ends eg easy instead require one</h3>
<p>With a circular list, a pointer to the last node gives easy access also to the first node, by following one link. Thus, in applications that require access to both ends of the list (e.g., in the implementation of a queue), a circular structure allows one to handle the structure by a single pointer, instead of two.</p>
<h3>two quad-edge giving greatly split joins face-edge piece circular into</h3>
<p>A circular list can be split into two circular lists, in constant time, by giving the addresses of the last node of each piece. The operation consists in swapping the contents of the link fields of those two nodes. Applying the same operation to any two nodes in two distinct lists joins the two list into one. This property greatly simplifies some algorithms and data structures, such as the quad-edge and face-edge.</p>
<h3>denote fewer thing special null empty sense creates contrast indicating</h3>
<p>The simplest representation for an empty circular list (when such a thing makes sense) is a null pointer, indicating that the list has no nodes. Without this choice, many algorithms have to test for this special case, and handle it separately. By contrast, the use of null to denote an empty linear list is more natural and often creates fewer special cases.</p>
<h3>end-of-list sentinels x sentinel node field setting eliminate +∞ tests</h3>
<p>Sentinel node may simplify certain list operations, by ensuring that the next and/or previous nodes exist for every element, and that even empty lists have at least one node. One may also use a sentinel node at the end of the list, with an appropriate data field, to eliminate some end-of-list tests. For example, when scanning the list looking for a node with a given value x, setting the sentinel's data field to x makes it unnecessary to test for end-of-list inside the loop. Another example is the merging two sorted lists: if their sentinels have data fields set to +∞, the choice of the next output node does not need special handling for empty lists.</p>
<h3>creation complicate short especially use extra sentinel applications up space</h3>
<p>However, sentinel nodes use up extra space (especially in applications that use many short lists), and they may complicate other operations (such as the creation of a new empty list).</p>
<h3>sentinel empty itself simulate alone last via merely next-node list</h3>
<p>However, if the circular list is used merely to simulate a linear list, one may avoid some of this complexity by adding a single sentinel node to every list, between the last and the first data nodes. With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link. The list handle should then be a pointer to the last data node, before the sentinel, if the list is not empty; or to the sentinel itself, if the list is empty.</p>
<h3>single doubly turning simplify handling trick dummy should sentinel itself</h3>
<p>The same trick can be used to simplify the handling of a doubly linked linear list, by turning it into a circular doubly linked list with a single sentinel node. However, in this case, the handle should be a single pointer to the dummy node itself.</p>
<h2>Linked list operations</h2>
<h3>in-place pseudocode invalidated subtle marker throughout section assignments end-of-list manipulating</h3>
<p>When manipulating linked lists in-place, care must be taken to not use values that you have invalidated in previous assignments. This makes algorithms for inserting or deleting linked list nodes somewhat subtle. This section gives pseudocode for adding or removing nodes from singly, doubly, and circularly linked lists in-place. Throughout we will use null to refer to an end-of-list marker or sentinel, which may be implemented in a number of ways.</p>
<h3>firstnode variable keep always points fields we node null empty</h3>
<p>Our node data structure will have two fields. We also keep a variable firstNode which always points to the first node in the list, or is null for an empty list.</p>
<h3> node // next being record reference null last stored</h3>
<p><i>
 <b>record</b> Node
 {
    data; // The data being stored in the node
    Node next // A reference to the next node, null for last node
 }
</i></p>
<h3> firstnode points // record node null empty list first</h3>
<p><i>
 <b>record</b> List
 {
     Node firstNode // points to first node of list; null for empty list
 }
</i></p>
<h3>traversal come until following simple beginning we end link singly</h3>
<p>Traversal of a singly linked list is simple, beginning at the first node and following each next link until we come to the end:</p>
<h3>= nodedata something listfirstnode node nodenext  do null while</h3>
<p><i>
 node := list.firstNode
 <b>while</b> node not null
     (do something with node.data)
     node := node.next
</i></p>
<h3>existing node after inserts shows how diagram track works done</h3>
<p>The following code inserts a node after an existing node in a singly linked list. The diagram shows how it works. Inserting a node before an existing one cannot be done directly; instead, one must keep track of the previous node and insert a node after it.</p>
<h3>newnode nodenext = insertafternode newnodenext node  insert after //</h3>
<p><i>
 <b>function</b> insertAfter(Node node, Node newNode) // insert newNode after node
     newNode.next := node.next
     node.next    := newNode
</i></p>
<h3>requires updating firstnode inserting separate beginning function at this list</h3>
<p>Inserting at the beginning of the list requires a separate function. This requires updating firstNode.</p>
<h3>listfirstnode newnode = insertbeginninglist newnodenext node  insert current //</h3>
<p><i>
 <b>function</b> insertBeginning(List list, Node newNode) // insert node before current first node
     newNode.next   := list.firstNode
     list.firstNode := newNode
</i></p>
<h3>removing demonstrates node similarly former diagram track find again functions</h3>
<p>Similarly, we have functions for removing the node after a given node, and for removing a node from the beginning of the list. The diagram demonstrates the former. To find and remove a particular node, one must again keep track of the previous element.</p>
<h3>obsoletenode nodenext = nodenextnext removeafternode destroy past remove  //</h3>
<p><i>
 <b>function</b> removeAfter(Node node) // remove node past this one
     obsoleteNode := node.next
     node.next := node.next.next
     destroy obsoleteNode
</i></p>
<h3>obsoletenode listfirstnode // = removebeginninglist deleted destroy listfirstnodenext past remove</h3>
<p><i>
 <b>function</b> removeBeginning(List list) // remove first node
     obsoleteNode := list.firstNode
     list.firstNode := list.firstNode.next // point past deleted node
     destroy obsoleteNode
</i></p>
<h3>removebeginning sets notice listfirstnode removing null last when node that</h3>
<p>Notice that <strong>removeBeginning()</strong> sets <strong>list.firstNode</strong> to <strong>null</strong> when removing the last node in the list.</p>
<h3>insertbefore removebefore cant iterate backwards since possible we efficient operations</h3>
<p>Since we can't iterate backwards, efficient <strong>insertBefore</strong> or <strong>removeBefore</strong> operations are not possible.</p>
<h3>appending append tail asymptotic inefficient linearly kept traverse provided complexity</h3>
<p>Appending one linked list to another can be inefficient unless a reference to the tail is kept as part of the List structure, because we must traverse the entire first list in order to find the tail, and then append the second list to this. Thus, if two linearly linked lists are each of length , list appending has asymptotic time complexity of . In the Lisp family of languages, list appending is provided by the <strong>append</strong> procedure.</p>
<h3>cases insertbeginning eliminated renders special ensures listfirstnodenext removebeginning dummy unnecessary</h3>
<p>Many of the special cases of linked list operations can be eliminated by including a dummy element at the front of the list. This ensures that there are no special cases for the beginning of the list and renders both <strong>insertBeginning()</strong> and <strong>removeBeginning()</strong> unnecessary. In this case, the first useful data in the list will be found at <strong>list.<b>firstNode</b>.next</strong>.</p>
<h3>back front continuous node last stores circle circularly added after</h3>
<p>In a circularly linked list, all nodes are linked in a continuous circle, without using null. For lists with a front and a back (such as a queue), one stores a reference to the last node in the list. The next node after the last node is the first node. Elements can be added to the back of the list and removed from the front in constant time.</p>
<h3>circularly either doubly singly linked lists can or be</h3>
<p>Circularly linked lists can be either singly or doubly linked.</p>
<h3>lastnode empty representation we special significantly traverse simplifies here non-empty</h3>
<p>Both types of circularly linked lists benefit from the ability to traverse the full list beginning at any given node. This often allows us to avoid storing firstNode and lastNode, although if the list may be empty we need a special representation for the empty list, such as a lastNode variable which points to some node in the list or is null if it's empty; we use such a lastNode here. This representation significantly simplifies adding and removing nodes with a non-empty list, but empty lists are then a special case.</p>
<h3>somenode iterates assuming non-empty starting code through singly circular some</h3>
<p>Assuming that someNode is some node in a non-empty circular singly linked list, this code iterates through that list starting with someNode:</p>
<h3>somenode ≠ do = iteratesomenode nodevalue something node nodenext </h3>
<p><i>
 <b>function</b> iterate(someNode)
   <b>if</b> someNode ≠ <b>null</b>
     node := someNode
     <b>do</b>
       do something with node.value
       node := node.next
     <b>while</b> node ≠ someNode
</i></p>
<h3>test loop whenever moved fail somenode had notice ≠ procedure</h3>
<p>Notice that the test "<b>while</b> node ≠ someNode" must be at the end of the loop. If the test was moved to the beginning of the loop, the procedure would fail whenever the list had only one node.</p>
<h3>assumes node inserts newnode given after function circular null into</h3>
<p>This function inserts a node "newNode" into a circular linked list after a given node "node". If "node" is null, it assumes that the list is empty.</p>
<h3>= newnode newnodenext nodenext insertafternode node else  function null</h3>
<p><i>
 <b>function</b> insertAfter(Node node, Node newNode)
     <b>if</b> node = <b>null</b>
       newNode.next := newNode
     <b>else</b>
       newNode.next := node.next
       node.next := newNode
</i></p>
<h3>append l suppose pointing variable newnode end do circular null</h3>
<p>Suppose that "L" is a variable pointing to the last node of a circular linked list (or null if the list is empty). To append "newNode" to the end of the list, one may do</p>
<h3>newnode insertafterl l  =</h3>
<p><i>
 insertAfter(L, newNode)
 L := newNode
</i></p>
<h3>newnode insert beginning do one may at list to</h3>
<p>To insert "newNode" at the beginning of the list, one may do</p>
<h3>l newnode = insertafterl  null if</h3>
<p><i>
 insertAfter(L, newNode)
 <b>if</b> L = <b>null</b>
   L := newNode
</i></p>
<h2>Linked lists using arrays of nodes</h2>
<h3>records not indicating parallel possibly supported array replacing still keep</h3>
<p>Languages that do not support any type of reference can still create links by replacing pointers with array indices. The approach is to keep an array of records, where each record has integer fields indicating the index of the next (and possibly previous) node in the array. Not all nodes in the array need be used. If records are also not supported, parallel arrays can often be used instead.</p>
<h3>consider uses instead pointers following record example arrays an linked</h3>
<p>As an example, consider the following linked list record that uses arrays instead of pointers:</p>
<h3>entry  integer // real next double-linked balance prev string</h3>
<p><i>
 <b>record</b> Entry {
    integer next; // index of next entry in array
    integer prev; // previous entry (if double-linked)
    string name;
    real balance;
 }
</i></p>
<h3>creating variable built integer store index an first structures element</h3>
<p>By creating an array of these structures, and an integer variable to store the index of the first element, a linked list can be built:</p>
<h3>listhead entry  records integer</h3>
<p><i>
integer listHead
Entry Records
</i></p>
<h3>formed next prev placing within cell between links given previous</h3>
<p>Links between elements are formed by placing the array index of the next (or previous) cell into the Next or Prev field within a given element. For example:</p>
<h3>entries would cells could entry available increased listfree 7 free</h3>
<p>In the above example, <strong>ListHead</strong> would be set to 2, the location of the first entry in the list. Notice that entry 3 and 5 through 7 are not part of the list. These cells are available for any additions to the list. By creating a <strong>ListFree</strong> integer variable, a free list could be created to keep track of what cells are available. If all entries are in use, the size of the array would have to be increased or some elements would have to be deleted before new entries could be stored in the list.</p>
<h3>account names display traverse balance code following would list and</h3>
<p>The following code would traverse the list and display names and account balance:</p>
<h3>i print // = recordsbalance ≥ recordsnext recordsname listhead entry</h3>
<p><i>
i := listHead
<b>while</b> i ≥ 0 // loop through the list
     print i, Records.name, Records.balance // print entry
     i := Records.next
</i></p>
<h3>faced choice approach include advantages when with this</h3>
<p>When faced with a choice, the advantages of this approach include:</p>
<h3>private manages issues creates leads main disadvantage approach following space</h3>
<p>This approach has one main disadvantage, however: it creates and manages a private memory space for its nodes. This leads to the following issues:</p>
<h3>reasons mitigated mainly allocation disadvantages maximum created approach known support</h3>
<p>For these reasons, this approach is mainly used for languages that do not support dynamic memory allocation. These disadvantages are also mitigated if the maximum size of the list is known at the time the array is created.</p>
<h2>Language support</h2>
<h3>cons reference functional constructed build scheme languages car many purpose</h3>
<p>Many programming languages such as Lisp and Scheme have singly linked lists built in. In many functional languages, these lists are constructed from nodes, each called a cons or cons cell. The cons has two fields: the car, a reference to the data for that node, and the cdr, a reference to the next node. Although cons cells can be used to build other data structures, this is their primary purpose.</p>
<h3>templates adts languages building together typically built lists references records</h3>
<p>In languages that support abstract data types or templates, linked list ADTs or templates are available for building linked lists. In other languages, linked lists are typically built using references together with records.</p>
<h2>Internal and external storage</h2>
<h3>storage internal store reference management requiring overall simplifying called constructing</h3>
<p>When constructing a linked list, one is faced with the choice of whether to store the data of the list directly in the linked list nodes, called internal storage, or merely to store a reference to the data, called external storage. Internal storage has the advantage of making access to the data more efficient, requiring less storage overall, having better locality of reference, and simplifying memory management for the list (its data is allocated and deallocated at the same time as the list nodes).</p>
<h3>storage multiple cells additional internal lists create references external same</h3>
<p>External storage, on the other hand, has the advantage of being more generic, in that the same data structure and machine code can be used for a linked list no matter what the size of the data is. It also makes it easy to place the same data in multiple linked lists. Although with internal storage the same data can be placed in multiple lists by including multiple next references in the node data structure, it would then be necessary to create separate routines to add or delete cells based on each field. It is possible to create additional linked lists of elements that use internal storage by using external storage, and having the cells of the additional linked lists store references to the nodes of the linked list containing the data.</p>
<h3>included storage internal external slightly set then likewise fine best</h3>
<p>In general, if a set of data structures needs to be included in multiple linked lists, external storage is the best approach. If a set of data structures need to be included in only one linked list, then internal storage is slightly better, unless a generic linked list package using external storage is available. Likewise, if different sets of data that can be stored in the same data structure are to be included in a single linked list, then internal storage would be fine.</p>
<h3>message routines generic type messages received then structures process separate</h3>
<p>Another approach that can be used with some languages involves having different data structures, but all have the initial fields, including the next (and prev if double linked list) references in the same location. After defining separate structures for each type of data, a generic structure can be defined that contains the minimum amount of data shared by all the other structures and contained at the top (beginning) of the structures. Then generic routines can be created that use the minimal structure to perform linked list type operations, but separate routines can then handle the specific data. This approach is often used in message parsing routines, where several types of messages are received, but all start with the same set of fields, usually including a field for message type. The generic routines are used to add new messages to a queue when they are received, and remove them from the queue in order to process the message. The message type field is then used to call the correct routine to process the specific type of message.</p>
<h3>wanted look you might suppose internal like families create members</h3>
<p>Suppose you wanted to create a linked list of families and their members. Using internal storage, the structure might look like the following:</p>
<h3>family member string  // members record next firstname lastname</h3>
<p><i>
 <b>record</b> member { // member of a family
     member next;
     string firstName;
     integer age;
 }
 <b>record</b> family { // the family itself
     family next;
     string lastName;
     string address;
     member members // head of list of members of this family
 }
</i></p>
<h3>write complete could print internal families members we storage their</h3>
<p>To print a complete list of families and their members using internal storage, we could write:</p>
<h3>amember afamily // = families about ≠ print loop information</h3>
<p><i>
 aFamily := Families // start at head of families list
 <b>while</b> aFamily ≠ <b>null</b> // loop through list of families
     print information about family
     aMember := aFamily.members // get head of list of this family's members
     <b>while</b> aMember ≠ <b>null</b> // loop through list of members
         print information about member
         aMember := aMember.next
     aFamily := aFamily.next
</i></p>
<h3>create external following would we storage using structures</h3>
<p>Using external storage, we would create the following structures:</p>
<h3>// family  string record generic member members node structure</h3>
<p><i>
 <b>record</b> node { // generic link structure
     node next;
     pointer data // generic pointer for data at node
 }
 <b>record</b> member { // structure for family member
     string firstName;
     integer age
 }
 <b>record</b> family { // structure for family
     string lastName;
     string address;
     node members // head of list of members of this family
 }
</i></p>
<h3>write complete could print families external members we storage their</h3>
<p>To print a complete list of families and their members using external storage, we could write:</p>
<h3>// = memnode famnode family families extract about ≠ member</h3>
<p><i>
 famNode := Families // start at head of families list
 <b>while</b> famNode ≠ <b>null</b> // loop through list of families
     aFamily := (family) famNode.data // extract family from node
     print information about family
     memNode := aFamily.members // get list of family members
     <b>while</b> memNode ≠ <b>null</b> // loop through list of members
         aMember := (member)memNode.data // extract member from node
         print information about member
         memNode := memNode.next
     famNode := famNode.next
</i></p>
<h3>step cast parametric proper extract notice using family within extra</h3>
<p>Notice that when using external storage, an extra step is needed to extract the record from the node and cast it into the proper data type. This is because both the list of families and the list of members within the family are stored in two linked lists using the same data structure (node), and this language does not have parametric types.</p>
<h3>number member families known storage long compile run fine belong</h3>
<p>As long as the number of families that a member can belong to is known at compile time, internal storage works fine. If, however, a member needed to be included in an arbitrary number of families, with the specific number known only at run time, external storage would be necessary.</p>
<h3>search discussed improve normally sorted time disadvantages variants primary ways</h3>
<p>Finding a specific element in a linked list, even if it is sorted, normally requires O(n) time (linear search). This is one of the primary disadvantages of linked lists over other data structures. In addition to the variants discussed above, below are two simple ways to improve search time.</p>
<h3>heuristic simple unordered quickest handy move-to-front moves ensures scheme recently</h3>
<p>In an unordered list, one simple heuristic for decreasing average search time is the move-to-front heuristic, which simply moves an element to the beginning of the list once it is found. This scheme, handy for creating simple caches, ensures that the most recently used items are also the quickest to find again.</p>
<h3>indexes build red-black index tree disadvantage least updated again table</h3>
<p>Another common approach is to "index" a linked list using a more efficient external data structure. For example, one can build a red-black tree or hash table whose elements are references to the linked list nodes. Multiple such indexes can be built on a single list. The disadvantage is that these indexes may need to be updated each time a node is added or removed (or at least, before that index is used again).</p>
<h3>random access worst-case skew binary read modify logarithmic head/cons time</h3>
<p>A random access list is a list with support for fast random access to read or modify any element in the list. One possible implementation is a skew binary random access list using the skew binary number system, which involves a list of trees with special properties; this allows worst-case constant time head/cons operations, and worst-case logarithmic time random access to an element by index. Random access lists can be implemented as persistent data structures.</p>
<h3>immutable viewed likewise o1 tail lists random support head access</h3>
<p>Random access lists can be viewed as immutable linked lists in that they likewise support the same O(1) head and tail operations.</p>
<h3>min-list complexities mutation extension yields provides minimum entire additional random</h3>
<p>A simple extension to random access lists is the min-list, which provides an additional operation that yields the minimum element in the entire list in constant time (without mutation complexities).</p>
<h2>Related data structures</h2>
<h3>restrict supported queues simply stacks implemented both often type using</h3>
<p>Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported.</p>
<h3>layer skip layers continues jumping descending augmented down quickly actual</h3>
<p>The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list.</p>
<h3>result subtrees nature tree seen contents themselves binary node together</h3>
<p>A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node.</p>
<h3>improved metadata reduced needs unrolled contiguous leads performance each cache</h3>
<p>An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list.</p>
<h3>hash table chains position store items same use lists may</h3>
<p>A hash table may use linked lists to store the chains of items that hash to the same position in the hash table.</p>
<h3>ordering shares heap calculated datas almost indexes properties using instead</h3>
<p>A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index.</p>
<h3>reduces self-organizing keeping rearranges heuristic commonly times accessed retrieval nodes</h3>
<p>A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.</p>
