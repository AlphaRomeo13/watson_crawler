<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Convex optimization</span></h1>
<p><b>Convex minimization</b>, a subfield of <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">optimization</a>, studies the problem of minimizing <a href="/wiki/Convex_function" title="Convex function">convex functions</a> over <a href="/wiki/Convex_set" title="Convex set">convex sets</a>. The convexity property can make optimization in some sense "easier" than the general case - for example, any <a href="/wiki/Local_optimum" title="Local optimum">local minimum</a> must be a <a href="/wiki/Global_optimum" title="Global optimum">global minimum</a>.</p>
<p>Given a <a href="/wiki/Real_number" title="Real number">real</a> <a href="/wiki/Vector_space" title="Vector space">vector space</a> <img class="mwe-math-fallback-image-inline tex" alt="X" src="//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png"> together with a <a href="/wiki/Convex_function" title="Convex function">convex</a>, real-valued <a href="/wiki/Function_(mathematics)" title="Function (mathematics)">function</a></p>
<p>defined on a <a href="/wiki/Convex_set" title="Convex set">convex subset</a> <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{X}" src="//upload.wikimedia.org/math/5/4/8/548ad3254513a0a221b4b07fd87e5d9a.png"> of <img class="mwe-math-fallback-image-inline tex" alt="X" src="//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png">, the problem is to find any point <img class="mwe-math-fallback-image-inline tex" alt="x^\ast" src="//upload.wikimedia.org/math/2/8/8/2885fbb8b21dad521828d020c70b5568.png"> in <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{X}" src="//upload.wikimedia.org/math/5/4/8/548ad3254513a0a221b4b07fd87e5d9a.png"> for which the number <img class="mwe-math-fallback-image-inline tex" alt="f(x)" src="//upload.wikimedia.org/math/5/0/b/50bbd36e1fd2333108437a2ca378be62.png"> is smallest, i.e., a point <img class="mwe-math-fallback-image-inline tex" alt="x^\ast" src="//upload.wikimedia.org/math/2/8/8/2885fbb8b21dad521828d020c70b5568.png"> such that</p>
<p>The convexity of <img class="mwe-math-fallback-image-inline tex" alt="f" src="//upload.wikimedia.org/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png"> makes the powerful tools of <a href="/wiki/Convex_analysis" title="Convex analysis">convex analysis</a> applicable. In finite-dimensional <a href="/wiki/Normed_space" title="Normed space" class="mw-redirect">normed spaces</a>, the <a href="/wiki/Hahn%E2%80%93Banach_theorem" title="Hahn–Banach theorem">Hahn–Banach theorem</a> and the existence of <a href="/wiki/Subgradient" title="Subgradient" class="mw-redirect">subgradients</a> lead to a particularly satisfying theory of <a href="/wiki/Necessary_and_sufficient_conditions" title="Necessary and sufficient conditions" class="mw-redirect">necessary and sufficient conditions</a> for optimality, a <a href="/wiki/Dual_problem" title="Dual problem" class="mw-redirect">duality theory</a> generalizing that for <a href="/wiki/Linear_programming" title="Linear programming">linear programming</a>, and effective computational methods.</p>
<p>Convex minimization has applications in a wide range of disciplines, such as automatic <a href="/wiki/Control_systems" title="Control systems" class="mw-redirect">control systems</a>, estimation and <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, communications and networks, electronic <a href="/wiki/Circuit_design" title="Circuit design">circuit design</a>, data analysis and modeling, <a href="/wiki/Statistics" title="Statistics">statistics</a> (<a href="/wiki/Optimal_design" title="Optimal design">optimal design</a>), and <a href="/wiki/Finance" title="Finance">finance</a>. With recent improvements in computing and in optimization theory, convex minimization is nearly as straightforward as <a href="/wiki/Linear_programming" title="Linear programming">linear programming</a>. Many optimization problems can be reformulated as convex minimization problems. For example, the problem of <i>maximizing</i> a <i>concave</i> function <i>f</i> can be re-formulated equivalently as a problem of <i>minimizing</i> the function -<i>f</i>, which is <i>convex</i>.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Convex_optimization_problem"><span class="tocnumber">1</span> <span class="toctext">Convex optimization problem</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Theory"><span class="tocnumber">2</span> <span class="toctext">Theory</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Standard_form"><span class="tocnumber">3</span> <span class="toctext">Standard form</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Examples"><span class="tocnumber">4</span> <span class="toctext">Examples</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Lagrange_multipliers"><span class="tocnumber">5</span> <span class="toctext">Lagrange multipliers</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Methods"><span class="tocnumber">6</span> <span class="toctext">Methods</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Convex_minimization_with_good_complexity:_Self-concordant_barriers"><span class="tocnumber">7</span> <span class="toctext">Convex minimization with good complexity: Self-concordant barriers</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Quasiconvex_minimization"><span class="tocnumber">8</span> <span class="toctext">Quasiconvex minimization</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Convex_maximization"><span class="tocnumber">9</span> <span class="toctext">Convex maximization</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#Extensions"><span class="tocnumber">10</span> <span class="toctext">Extensions</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#See_also"><span class="tocnumber">11</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Notes"><span class="tocnumber">12</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#References"><span class="tocnumber">13</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#External_links"><span class="tocnumber">14</span> <span class="toctext">External links</span></a></li>
</ul>
<p></p>
<h2><span class="mw-headline" id="Convex_optimization_problem">Convex optimization problem</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=1" title="Edit section: Convex optimization problem">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>An <i>optimization problem</i> (also referred to as a <i>mathematical programming problem</i> or <i>minimization problem</i>) of finding some <img class="mwe-math-fallback-image-inline tex" alt="x^\ast \in \mathcal{X}" src="//upload.wikimedia.org/math/7/a/b/7ab2b524ce2a695903b81d45d27d5242.png"> such that</p>
<p>where <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{X} \subset \mathbb{R}^n" src="//upload.wikimedia.org/math/d/0/1/d01e9255365440ae709190fafc071951.png"> is the <i>feasible set</i> and <img class="mwe-math-fallback-image-inline tex" alt="f(x):\mathbb{R}^n \rightarrow \mathbb{R}" src="//upload.wikimedia.org/math/5/3/a/53ac789bc0592a5c2410a35572b949eb.png"> is the <i>objective</i>, is called <i>convex</i> if <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{X}" src="//upload.wikimedia.org/math/5/4/8/548ad3254513a0a221b4b07fd87e5d9a.png"> is a closed convex set and <img class="mwe-math-fallback-image-inline tex" alt="f(x)" src="//upload.wikimedia.org/math/5/0/b/50bbd36e1fd2333108437a2ca378be62.png"> is convex on <img class="mwe-math-fallback-image-inline tex" alt="\mathbb{R}^n" src="//upload.wikimedia.org/math/3/0/c/30c28f76ef7517dbd19df4d4c683dbe6.png">. <sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> <sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup></p>
<p>Alternatively, an optimization problem on the form</p>
<p>is called convex if the functions <img class="mwe-math-fallback-image-inline tex" alt="f, g_1 \ldots g_m : \mathbb{R}^n \rightarrow \mathbb{R}" src="//upload.wikimedia.org/math/0/1/3/013e8459903dd3671a3866c599631fa4.png"> are convex.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup></p>
<h2><span class="mw-headline" id="Theory">Theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=2" title="Edit section: Theory">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The following statements are true about the convex minimization problem:</p>
<ul>
<li>if a <a href="/wiki/Local_minimum" title="Local minimum" class="mw-redirect">local minimum</a> exists, then it is a <a href="/wiki/Global_minimum" title="Global minimum" class="mw-redirect">global minimum</a>.</li>
<li>the set of all (global) minima is convex.</li>
<li>for each <i>strictly</i> convex function, if the function has a minimum, then the minimum is unique.</li>
</ul>
<p>These results are used by the theory of convex minimization along with geometric notions from <a href="/wiki/Functional_analysis" title="Functional analysis">functional analysis</a> (in Hilbert spaces) such as the <a href="/wiki/Hilbert_projection_theorem" title="Hilbert projection theorem">Hilbert projection theorem</a>, the <a href="/wiki/Separating_hyperplane_theorem" title="Separating hyperplane theorem" class="mw-redirect">separating hyperplane theorem</a>, and <a href="/wiki/Farkas%27_lemma" title="Farkas' lemma">Farkas' lemma</a>.</p>
<h2><span class="mw-headline" id="Standard_form">Standard form</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=3" title="Edit section: Standard form">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>Standard form</i> is the usual and most intuitive form of describing a convex minimization problem. It consists of the following three parts:</p>
<ul>
<li>A <b>convex function</b> <img class="mwe-math-fallback-image-inline tex" alt="f(x): \mathbb{R}^n \to \mathbb{R}" src="//upload.wikimedia.org/math/b/3/6/b361fec9d2a9fc944eaf26b86ff64c4c.png"> to be minimized over the variable <img class="mwe-math-fallback-image-inline tex" alt="x" src="//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png"></li>
<li><b>Inequality constraints</b> of the form <img class="mwe-math-fallback-image-inline tex" alt="g_i(x) \leq 0" src="//upload.wikimedia.org/math/1/e/5/1e5c02949a9393acaf52697d9f0ca24a.png">, where the functions <img class="mwe-math-fallback-image-inline tex" alt="g_i" src="//upload.wikimedia.org/math/8/a/0/8a063395ded15845176d0b1e07824cbf.png"> are convex</li>
<li><b>Equality constraints</b> of the form <img class="mwe-math-fallback-image-inline tex" alt="h_i(x) = 0" src="//upload.wikimedia.org/math/5/e/f/5ef0ff15c1cae15346f52bf7bf225c2e.png">, where the functions <img class="mwe-math-fallback-image-inline tex" alt="h_i" src="//upload.wikimedia.org/math/9/1/4/914767ffc65896751e16a5b9fbf3fc4a.png"> are <a href="/wiki/Affine_transformation" title="Affine transformation">affine</a>. In practice, the terms "linear" and "affine" are often used interchangeably. Such constraints can be expressed in the form <img class="mwe-math-fallback-image-inline tex" alt="h_i(x) = a_i^T x + b_i" src="//upload.wikimedia.org/math/7/b/1/7b1fc94595ab143235f2f6c58041a301.png">, where <img class="mwe-math-fallback-image-inline tex" alt="a_i" src="//upload.wikimedia.org/math/d/8/d/d8dd7d0f3eb7145ca41c711457b7eb8f.png"> is a column-vector and <img class="mwe-math-fallback-image-inline tex" alt="b_i" src="//upload.wikimedia.org/math/c/9/f/c9f6d8557ce40f989fa727b5c0bb1ddf.png"> a real number.</li>
</ul>
<p>A convex minimization problem is thus written as</p>
<p>Note that every equality constraint <img class="mwe-math-fallback-image-inline tex" alt="h(x) = 0" src="//upload.wikimedia.org/math/a/8/7/a877f3c76508a0ccf6fbaaa64ea62d49.png"> can be equivalently replaced by a pair of inequality constraints <img class="mwe-math-fallback-image-inline tex" alt="h(x)\leq 0" src="//upload.wikimedia.org/math/2/2/b/22bb7366806b4ad3012332ef01c5577e.png"> and <img class="mwe-math-fallback-image-inline tex" alt="-h(x)\leq 0" src="//upload.wikimedia.org/math/e/0/e/e0e7faeaa79fa168208f25151fc27263.png">. Therefore, for theoretical purposes, equality constraints are redundant; however, it can be beneficial to treat them specially in practice.</p>
<p>Following from this fact, it is easy to understand why <img class="mwe-math-fallback-image-inline tex" alt="h_i(x) = 0" src="//upload.wikimedia.org/math/5/e/f/5ef0ff15c1cae15346f52bf7bf225c2e.png"> has to be affine as opposed to merely being convex. If <img class="mwe-math-fallback-image-inline tex" alt="h_i(x)" src="//upload.wikimedia.org/math/b/6/0/b6014190523793fa63b0a09b29ffa72b.png"> is convex, <img class="mwe-math-fallback-image-inline tex" alt="h_i(x) \leq 0" src="//upload.wikimedia.org/math/d/e/f/def6dc5a17c877ad67e395fee163e3a6.png"> is convex, but <img class="mwe-math-fallback-image-inline tex" alt="-h_i(x) \leq 0" src="//upload.wikimedia.org/math/3/8/8/3887509328b50e6740a2571abb95eec6.png"> is <i>concave</i>. Therefore, the only way for <img class="mwe-math-fallback-image-inline tex" alt="h_i(x) = 0" src="//upload.wikimedia.org/math/5/e/f/5ef0ff15c1cae15346f52bf7bf225c2e.png"> to be convex is for <img class="mwe-math-fallback-image-inline tex" alt="h_i(x)" src="//upload.wikimedia.org/math/b/6/0/b6014190523793fa63b0a09b29ffa72b.png"> to be affine.</p>
<h2><span class="mw-headline" id="Examples">Examples</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=4" title="Edit section: Examples">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The following problems are all convex minimization problems, or can be transformed into convex minimizations problems via a change of variables:</p>
<ul>
<li><a href="/wiki/Least_squares" title="Least squares">Least squares</a></li>
<li><a href="/wiki/Linear_programming" title="Linear programming">Linear programming</a></li>
<li>Convex <a href="/wiki/Quadratic_programming" title="Quadratic programming">quadratic minimization</a> with linear constraints</li>
<li><a href="/wiki/Quadratically_constrained_quadratic_programming" title="Quadratically constrained quadratic programming" class="mw-redirect">Quadratically constrained Convex-quadratic minimization with convex quadratic constraints</a></li>
<li><a href="/wiki/Conic_optimization" title="Conic optimization">Conic optimization</a></li>
<li><a href="/wiki/Geometric_programming" title="Geometric programming">Geometric programming</a></li>
<li><a href="/wiki/Second_order_cone_programming" title="Second order cone programming" class="mw-redirect">Second order cone programming</a></li>
<li><a href="/wiki/Semidefinite_programming" title="Semidefinite programming">Semidefinite programming</a></li>
<li><a href="/wiki/Entropy_maximization" title="Entropy maximization">Entropy maximization</a> with appropriate constraints</li>
</ul>
<h2><span class="mw-headline" id="Lagrange_multipliers">Lagrange multipliers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=5" title="Edit section: Lagrange multipliers">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Consider a convex minimization problem given in standard form by a cost function <img class="mwe-math-fallback-image-inline tex" alt="f(x)" src="//upload.wikimedia.org/math/5/0/b/50bbd36e1fd2333108437a2ca378be62.png"> and inequality constraints <img class="mwe-math-fallback-image-inline tex" alt="g_i(x)\leq 0" src="//upload.wikimedia.org/math/1/e/5/1e5c02949a9393acaf52697d9f0ca24a.png">, where <img class="mwe-math-fallback-image-inline tex" alt="i=1\ldots m" src="//upload.wikimedia.org/math/b/0/a/b0a9350785c6e7def240390c52a71dd7.png">. Then the domain <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{X}" src="//upload.wikimedia.org/math/5/4/8/548ad3254513a0a221b4b07fd87e5d9a.png"> is:</p>
<p>The <a href="/wiki/Lagrange_multipliers" title="Lagrange multipliers" class="mw-redirect">Lagrangian function</a> for the problem is</p>
<p>For each point <i>x</i> in <i>X</i> that minimizes <i>f</i> over <i>X</i>, there exist real numbers <i>λ</i><sub>0</sub>, ..., <i>λ</i><sub>m</sub>, called <a href="/wiki/Lagrange_multipliers" title="Lagrange multipliers" class="mw-redirect">Lagrange multipliers</a>, that satisfy these conditions simultaneously:</p>
<ol>
<li><i>x</i> minimizes <i>L</i>(<i>y</i>, λ<sub>0</sub>, λ<sub>1</sub>, ..., λ<sub>m</sub>) over all <i>y</i> in <i>X</i>,</li>
<li>λ<sub>0</sub> ≥ 0, λ<sub>1</sub> ≥ 0, ..., λ<sub><i>m</i></sub> ≥ 0, with at least one λ<sub><i>k</i></sub>&gt;0,</li>
<li><i>λ</i><sub>1</sub><i>g</i><sub>1</sub>(<i>x</i>) = 0, ..., <i>λ</i><sub><i>m</i></sub><i>g</i><sub><i>m</i></sub>(<i>x</i>) = 0 (complementary slackness).</li>
</ol>
<p>If there exists a "strictly feasible point", i.e., a point <i>z</i> satisfying</p>
<p>then the statement above can be upgraded to assert that λ<sub>0</sub>=1.</p>
<p>Conversely, if some <i>x</i> in <i>X</i> satisfies 1-3 for <a href="/wiki/Scalar_(mathematics)" title="Scalar (mathematics)">scalars</a> λ<sub>0</sub>, ..., λ<sub><i>m</i></sub> with λ<sub>0</sub> = 1, then <i>x</i> is certain to minimize <i>f</i> over <i>X</i>.</p>
<h2><span class="mw-headline" id="Methods">Methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=6" title="Edit section: Methods">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Convex minimization problems can be solved by the following contemporary methods:<sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup></p>
<ul>
<li>"Bundle methods" (Wolfe, Lemaréchal, Kiwiel), and</li>
<li><a href="/wiki/Subgradient_method#Subgradient-projection_.26_bundle_methods" title="Subgradient method">Subgradient projection</a> methods (Polyak),</li>
<li><a href="/wiki/Interior-point_methods" title="Interior-point methods" class="mw-redirect">Interior-point methods</a> (Nemirovskii and Nesterov).</li>
</ul>
<p>Other methods of interest:</p>
<ul>
<li><a href="/wiki/Cutting-plane_methods" title="Cutting-plane methods" class="mw-redirect">Cutting-plane methods</a></li>
<li><a href="/wiki/Ellipsoid_method" title="Ellipsoid method">Ellipsoid method</a></li>
<li><a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient method</a></li>
<li><a href="/wiki/Drift_plus_penalty" title="Drift plus penalty">Dual subgradients and the drift-plus-penalty method</a></li>
</ul>
<p>Subgradient methods can be implemented simply and so are widely used.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup> Dual subgradient methods are subgradient methods applied to a <a href="/wiki/Duality_(optimization)" title="Duality (optimization)">dual problem</a>. The <a href="/wiki/Drift_plus_penalty" title="Drift plus penalty">drift-plus-penalty</a> method is similar to the dual subgradient method, but takes a time average of the primal variables.</p>
<h2><span class="mw-headline" id="Convex_minimization_with_good_complexity:_Self-concordant_barriers">Convex minimization with good complexity: Self-concordant barriers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=7" title="Edit section: Convex minimization with good complexity: Self-concordant barriers">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The efficiency of iterative methods is poor for the class of convex problems, because this class includes "bad guys" whose minimum cannot be approximated without a large number of function and subgradient evaluations;<sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup> thus, to have practically appealing efficiency results, it is necessary to make additional restrictions on the class of problems. Two such classes are problems special <a href="/wiki/Barrier_function" title="Barrier function">barrier functions</a>, first <i>self-concordant</i> barrier functions, according to the theory of Nesterov and Nemirovskii, and second <i>self-regular</i> barrier functions according to the theory of Terlaky and coauthors.</p>
<h2><span class="mw-headline" id="Quasiconvex_minimization">Quasiconvex minimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=8" title="Edit section: Quasiconvex minimization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Problems with convex level sets can be efficiently minimized, in theory. Yuri Nesterov proved that quasi-convex minimization problems could be solved efficiently, and his results were extended by Kiwiel.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7"><span>[</span>7<span>]</span></a></sup> However, such theoretically "efficient" methods use "divergent-series" <a href="/wiki/Gradient_descent#Stepsize_rules" title="Gradient descent">stepsize rules</a>, which were first developed for classical <a href="/wiki/Subgradient_method" title="Subgradient method">subgradient methods</a>. Classical subgradient methods using divergent-series rules are much slower than modern methods of convex minimization, such as subgradient projection methods, <a href="/wiki/Bundle_method" title="Bundle method" class="mw-redirect">bundle methods</a> of descent, and nonsmooth <a href="/w/index.php?title=Filter_method&amp;action=edit&amp;redlink=1" class="new" title="Filter method (page does not exist)">filter methods</a>.</p>
<p>Solving even close-to-convex but non-convex problems can be computationally intractable. Minimizing a unimodal function is intractable, regardless of the smoothness of the function, according to results of Ivanov.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup></p>
<h2><span class="mw-headline" id="Convex_maximization">Convex maximization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=9" title="Edit section: Convex maximization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Conventionally, the definition of the convex optimization problem (we recall) requires that the objective function <i>f</i> to be <i>minimized</i> and the feasible set be convex. In the special case of linear programming (LP), the objective function is both concave and convex, and so LP can also consider the problem of maximizing an objective function without confusion. However, for most convex minimization problems, the objective function is not concave, and therefore a problem and then such problems are formulated in the standard form of convex optimization problems, that is, minimizing the convex objective function.</p>
<p>For nonlinear convex minimization, the associated maximization problem obtained by substituting the <a href="/wiki/Supremum" title="Supremum" class="mw-redirect">supremum</a> operator for the <a href="/wiki/Infimum" title="Infimum" class="mw-redirect">infimum</a> operator is not a problem of convex optimization, as conventionally defined. However, it is studied in the larger field of convex optimization as a problem of convex maximization.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup></p>
<p>The convex maximization problem is especially important for studying the existence of maxima. Consider the restriction of a convex function to a <a href="/wiki/Compact_set" title="Compact set" class="mw-redirect">compact</a> convex set: Then, on that set, the function attains its constrained <i>maximum</i> only on the boundary.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup> Such results, called "<a href="/wiki/Maximum_principle" title="Maximum principle">maximum principles</a>", are useful in the theory of <a href="/wiki/Harmonic_functions" title="Harmonic functions" class="mw-redirect">harmonic functions</a>, <a href="/wiki/Potential_theory" title="Potential theory">potential theory</a>, and <a href="/wiki/Partial_differential_equation" title="Partial differential equation">partial differential equations</a>.</p>
<p>The problem of minimizing a <a href="/wiki/Quadratic_polynomial" title="Quadratic polynomial" class="mw-redirect">quadratic</a> <a href="/wiki/Multivariate_polynomial" title="Multivariate polynomial" class="mw-redirect">multivariate polynomial</a> on a <a href="/wiki/Cube" title="Cube">cube</a> is <a href="/wiki/NP-hard" title="NP-hard">NP-hard</a>.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span>[</span>11<span>]</span></a></sup> In fact, in the <a href="/wiki/Quadratic_programming" title="Quadratic programming">quadratic minimization</a> problem, if the matrix has only one negative <a href="/wiki/Eigenvalue" title="Eigenvalue" class="mw-redirect">eigenvalue</a>, is <a href="/wiki/NP-hard" title="NP-hard">NP-hard</a>.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12"><span>[</span>12<span>]</span></a></sup></p>
<h2><span class="mw-headline" id="Extensions">Extensions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=10" title="Edit section: Extensions">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Advanced treatments consider convex functions that can attain positive infinity, also; the <a href="/wiki/Characteristic_function_(convex_analysis)" title="Characteristic function (convex analysis)">indicator function</a> of convex analysis is zero for every <img class="mwe-math-fallback-image-inline tex" alt="x\in\mathcal{X}" src="//upload.wikimedia.org/math/6/2/4/624cf12f420fb0f373cda9f7b216b2f3.png"> and positive infinity otherwise.</p>
<p>Extensions of convex functions include <a href="/wiki/Biconvex_optimization" title="Biconvex optimization">biconvex</a>, <a href="/wiki/Pseudo-convex_function" title="Pseudo-convex function" class="mw-redirect">pseudo-convex</a>, and <a href="/wiki/Quasi-convex_function" title="Quasi-convex function" class="mw-redirect">quasi-convex functions</a>. Partial extensions of the theory of convex analysis and iterative methods for approximately solving non-convex minimization problems occur in the field of <a href="/wiki/Convexity_(mathematics)#Generalizations_and_extensions_for_convexity" title="Convexity (mathematics)" class="mw-redirect">generalized convexity</a> ("abstract convex analysis").</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=11" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Duality_(optimization)" title="Duality (optimization)">Duality</a></li>
<li><a href="/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" title="Karush–Kuhn–Tucker conditions">Karush–Kuhn–Tucker conditions</a></li>
<li><a href="/wiki/Optimization_problem" title="Optimization problem">Optimization problem</a></li>
<li><a href="/wiki/Proximal_Gradient_Methods" title="Proximal Gradient Methods">Proximal Gradient Methods</a></li>
</ul>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=12" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><span class="citation book">Hiriart-Urruty, Jean-Baptiste; Lemaréchal, Claude (1996). <a rel="nofollow" class="external text" href="http://books.google.de/books?id=Gdl4Jc3RVjcC&amp;printsec=frontcover&amp;dq=lemarechal+convex+analysis+and+minimization&amp;hl=de&amp;sa=X&amp;ei=E602T4GXGMzQsgaPtJ2VDA&amp;ved=0CDUQ6AEwAA#v=onepage&amp;q=convex%20minimization&amp;f=false"><i>Convex analysis and minimization algorithms: Fundamentals</i></a>. p. 291.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.aufirst=Jean-Baptiste&amp;rft.au=Hiriart-Urruty%2C+Jean-Baptiste&amp;rft.aulast=Hiriart-Urruty&amp;rft.au=Lemar%C3%A9chal%2C+Claude&amp;rft.btitle=Convex+analysis+and+minimization+algorithms%3A+Fundamentals&amp;rft.date=1996&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fbooks.google.de%2Fbooks%3Fid%3DGdl4Jc3RVjcC%26printsec%3Dfrontcover%26dq%3Dlemarechal%2Bconvex%2Banalysis%2Band%2Bminimization%26hl%3Dde%26sa%3DX%26ei%3DE602T4GXGMzQsgaPtJ2VDA%26ved%3D0CDUQ6AEwAA%23v%3Donepage%26q%3Dconvex%2520minimization%26f%3Dfalse&amp;rft.pages=291&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><span class="citation book">Ben-Tal, Aharon; Nemirovskiĭ, Arkadiĭ Semenovich (2001). <a rel="nofollow" class="external text" href="http://books.google.de/books?id=M3MqpEJ3jzQC&amp;printsec=frontcover&amp;dq=Lectures+on+Modern+Convex+Optimization:+Analysis,+Algorithms,&amp;hl=de&amp;sa=X&amp;ei=26c2T6G7HYrIswac0d2uDA&amp;ved=0CDIQ6AEwAA#v=onepage&amp;q=convex%20programming&amp;f=false"><i>Lectures on modern convex optimization: analysis, algorithms, and engineering applications</i></a>. pp. 335–336.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.au=Ben-Tal%2C+Aharon&amp;rft.aufirst=Aharon&amp;rft.aulast=Ben-Tal&amp;rft.au=Nemirovski%C4%AD%2C+Arkadi%C4%AD+Semenovich&amp;rft.btitle=Lectures+on+modern+convex+optimization%3A+analysis%2C+algorithms%2C+and+engineering+applications&amp;rft.date=2001&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fbooks.google.de%2Fbooks%3Fid%3DM3MqpEJ3jzQC%26printsec%3Dfrontcover%26dq%3DLectures%2Bon%2BModern%2BConvex%2BOptimization%3A%2BAnalysis%2C%2BAlgorithms%2C%26hl%3Dde%26sa%3DX%26ei%3D26c2T6G7HYrIswac0d2uDA%26ved%3D0CDIQ6AEwAA%23v%3Donepage%26q%3Dconvex%2520programming%26f%3Dfalse&amp;rft.pages=335-336&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Boyd/Vandenberghe, p. 7</span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">For methods for convex minimization, see the volumes by Hiriart-Urruty and Lemaréchal (bundle) and the textbooks by <a href="/wiki/Andrzej_Piotr_Ruszczy%C5%84ski" title="Andrzej Piotr Ruszczyński">Ruszczyński</a> and Boyd and Vandenberghe (interior point).</span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Bertsekas</span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><a href="#CITEREFHiriart-UrrutyLemar.C3.A9chal1993">Hiriart-Urruty &amp; Lemaréchal (1993</a>, Example XV.1.1.2, p. 277) discuss a "bad guy" constructed by Arkadi Nemirovskii.</span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">In <a href="/wiki/Computational_complexity" title="Computational complexity" class="mw-redirect">theory</a>, quasiconvex programming and convex programming problems can be solved in reasonable amount of time, where the number of iterations grows like a polynomial in the dimension of the problem (and in the reciprocal of the approximation error tolerated):</span>
<p><span class="reference-text"><span class="citation news">Kiwiel, Krzysztof C. (2001). "Convergence and efficiency of subgradient methods for quasiconvex minimization". <i>Mathematical Programming (Series A)</i> <b>90</b> (1) (Berlin, Heidelberg: Springer). pp. 1–25. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2FPL00011414">10.1007/PL00011414</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0025-5610">0025-5610</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1819784">1819784</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.atitle=Convergence+and+efficiency+of+subgradient+methods+for+quasiconvex+minimization&amp;rft.aufirst=Krzysztof+C.&amp;rft.au=Kiwiel%2C+Krzysztof+C.&amp;rft.aulast=Kiwiel&amp;rft.date=2001&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2FPL00011414&amp;rft.issn=0025-5610&amp;rft.issue=1&amp;rft.jtitle=Mathematical+Programming++%28Series+A%29&amp;rft.mr=1819784&amp;rft.pages=1-25&amp;rft.place=Berlin%2C+Heidelberg&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=90" class="Z3988"><span style="display:none;"> </span></span> Kiwiel acknowledges that <a href="/w/index.php?title=Yuri_Nesterov_(mathematician)&amp;action=edit&amp;redlink=1" class="new" title="Yuri Nesterov (mathematician) (page does not exist)">Yuri Nesterov</a> first established that quasiconvex minimization problems can be solved efficiently.</span></p>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Nemirovskii and Judin</span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Convex maximization is mentioned in the subsection on convex optimization in this textbook: <a rel="nofollow" class="external text" href="http://books.google.se/books?id=9sbsMkuFzhYC&amp;pg=PA206&amp;dq=%22convex+maximization%22,+%22convex+minimization%22+OR+%22convex+optimization%22&amp;hl=sv&amp;sa=X&amp;ei=YswrT8-kGqfV4QTKs8CwDg&amp;ved=0CF8Q6AEwCA#v=onepage&amp;q=%22convex%20maximization%22%2C%20%22convex%20minimization%22%20OR%20%22convex%20optimization%22&amp;f=false">Ulrich Faigle, Walter Kern, and George Still. <i>Algorithmic principles of mathematical programming</i>. Springer-Verlag. Texts in Mathematics. Chapter 10.2, Subsection "Convex optimization", pages 205-206.</a></span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Theorem 32.1 in Rockafellar's <i>Convex Analysis</i> states this <a href="/wiki/Maximum_principle" title="Maximum principle">maximum principle</a> for extended real-valued functions.</span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">Sahni, S. "Computationally related problems," in SIAM Journal on Computing, 3, 262--279, 1974.</span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">Quadratic programming with one negative eigenvalue is NP-hard, Panos M. Pardalos and Stephen A. Vavasis in <i>Journal of Global Optimization</i>, Volume 1, Number 1, 1991, pg.15-22.</span></li>
</ol>
<p><span class="reference-text"><span class="citation news">Kiwiel, Krzysztof C. (2001). "Convergence and efficiency of subgradient methods for quasiconvex minimization". <i>Mathematical Programming (Series A)</i> <b>90</b> (1) (Berlin, Heidelberg: Springer). pp. 1–25. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2FPL00011414">10.1007/PL00011414</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0025-5610">0025-5610</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1819784">1819784</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.atitle=Convergence+and+efficiency+of+subgradient+methods+for+quasiconvex+minimization&amp;rft.aufirst=Krzysztof+C.&amp;rft.au=Kiwiel%2C+Krzysztof+C.&amp;rft.aulast=Kiwiel&amp;rft.date=2001&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2FPL00011414&amp;rft.issn=0025-5610&amp;rft.issue=1&amp;rft.jtitle=Mathematical+Programming++%28Series+A%29&amp;rft.mr=1819784&amp;rft.pages=1-25&amp;rft.place=Berlin%2C+Heidelberg&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=90" class="Z3988"><span style="display:none;"> </span></span> Kiwiel acknowledges that <a href="/w/index.php?title=Yuri_Nesterov_(mathematician)&amp;action=edit&amp;redlink=1" class="new" title="Yuri Nesterov (mathematician) (page does not exist)">Yuri Nesterov</a> first established that quasiconvex minimization problems can be solved efficiently.</span></p>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=13" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><span class="citation book"><a href="/wiki/Dimitri_Bertsekas" title="Dimitri Bertsekas">Bertsekas, Dimitri</a> (2003). <i>Convex Analysis and Optimization</i>. Athena Scientific.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.au=Bertsekas%2C+Dimitri&amp;rft.aufirst=Dimitri&amp;rft.aulast=Bertsekas&amp;rft.btitle=Convex+Analysis+and+Optimization&amp;rft.date=2003&amp;rft.genre=book&amp;rft.pub=Athena+Scientific&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><span class="citation book">Boyd, Stephen P.; Vandenberghe, Lieven (2004). <a rel="nofollow" class="external text" href="http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"><i>Convex Optimization</i></a> (pdf). Cambridge University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-521-83378-3" title="Special:BookSources/978-0-521-83378-3">978-0-521-83378-3</a><span class="reference-accessdate">. Retrieved October 15, 2011</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.au=Boyd%2C+Stephen+P.&amp;rft.aufirst=Stephen+P.&amp;rft.aulast=Boyd&amp;rft.au=Vandenberghe%2C+Lieven&amp;rft.btitle=Convex+Optimization&amp;rft.date=2004&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.stanford.edu%2F~boyd%2Fcvxbook%2Fbv_cvxbook.pdf&amp;rft.isbn=978-0-521-83378-3&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><a href="/wiki/Jonathan_M._Borwein" title="Jonathan M. Borwein" class="mw-redirect">Borwein, Jonathan</a>, and Lewis, Adrian. (2000). <i>Convex Analysis and Nonlinear Optimization</i>. Springer.</li>
</ul>
<ul>
<li>Hiriart-Urruty, Jean-Baptiste, and <a href="/wiki/Claude_Lemar%C3%A9chal" title="Claude Lemaréchal">Lemaréchal, Claude</a>. (2004). <i>Fundamentals of Convex analysis</i>. Berlin: Springer.</li>
</ul>
<ul>
<li><span class="citation book">Hiriart-Urruty, Jean-Baptiste; <a href="/wiki/Claude_Lemar%C3%A9chal" title="Claude Lemaréchal">Lemaréchal, Claude</a> (1993). <i>Convex analysis and minimization algorithms, Volume I: Fundamentals</i>. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] <b>305</b>. Berlin: Springer-Verlag. pp. xviii+417. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/3-540-56850-6" title="Special:BookSources/3-540-56850-6">3-540-56850-6</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1261420">1261420</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.aufirst=Jean-Baptiste&amp;rft.au=Hiriart-Urruty%2C+Jean-Baptiste&amp;rft.aulast=Hiriart-Urruty&amp;rft.au=Lemar%C3%A9chal%2C+Claude&amp;rft.btitle=Convex+analysis+and+minimization+algorithms%2C+Volume%26nbsp%3BI%3A+Fundamentals&amp;rft.date=1993&amp;rft.genre=book&amp;rft.isbn=3-540-56850-6&amp;rft.mr=1261420&amp;rft.pages=xviii%2B417&amp;rft.place=Berlin&amp;rft.pub=Springer-Verlag&amp;rft.series=Grundlehren+der+Mathematischen+Wissenschaften+%5BFundamental+Principles+of+Mathematical+Sciences%5D&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.volume=305" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span id="CITEREFHiriart-UrrutyLemar.C3.A9chal1993" class="citation book">Hiriart-Urruty, Jean-Baptiste; Lemaréchal, Claude (1993). <i>Convex analysis and minimization algorithms, Volume II: Advanced theory and bundle methods</i>. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] <b>306</b>. Berlin: Springer-Verlag. pp. xviii+346. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/3-540-56852-2" title="Special:BookSources/3-540-56852-2">3-540-56852-2</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1295240">1295240</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.aufirst=Jean-Baptiste&amp;rft.au=Hiriart-Urruty%2C+Jean-Baptiste&amp;rft.aulast=Hiriart-Urruty&amp;rft.au=Lemar%C3%A9chal%2C+Claude&amp;rft.btitle=Convex+analysis+and+minimization+algorithms%2C+Volume%26nbsp%3BII%3A+Advanced+theory+and+bundle+methods&amp;rft.date=1993&amp;rft.genre=book&amp;rft.isbn=3-540-56852-2&amp;rft.mr=1295240&amp;rft.pages=xviii%2B346&amp;rft.place=Berlin&amp;rft.pub=Springer-Verlag&amp;rft.series=Grundlehren+der+Mathematischen+Wissenschaften+%5BFundamental+Principles+of+Mathematical+Sciences%5D&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.volume=306" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><span id="CITEREFKiwiel1985" class="citation book">Kiwiel, Krzysztof C. (1985). <i>Methods of Descent for Nondifferentiable Optimization</i>. Lecture Notes in Mathematics. New York: Springer-Verlag. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-540-15642-0" title="Special:BookSources/978-3-540-15642-0">978-3-540-15642-0</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.aufirst=Krzysztof+C.&amp;rft.au=Kiwiel%2C+Krzysztof+C.&amp;rft.aulast=Kiwiel&amp;rft.btitle=Methods+of+Descent+for+Nondifferentiable+Optimization&amp;rft.date=1985&amp;rft.genre=book&amp;rft.isbn=978-3-540-15642-0&amp;rft.place=New+York&amp;rft.pub=Springer-Verlag&amp;rft.series=Lecture+Notes+in+Mathematics&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><span class="citation book"><a href="/wiki/Claude_Lemar%C3%A9chal" title="Claude Lemaréchal">Lemaréchal, Claude</a> (2001). "Lagrangian relaxation". In Michael Jünger and Denis Naddef. <i>Computational combinatorial optimization: Papers from the Spring School held in Schloß Dagstuhl, May 15–19, 2000</i>. Lecture Notes in Computer Science <b>2241</b>. Berlin: Springer-Verlag. pp. 112–156. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2F3-540-45586-8_4">10.1007/3-540-45586-8_4</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/3-540-42877-1" title="Special:BookSources/3-540-42877-1">3-540-42877-1</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1900016">1900016</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.atitle=Computational+combinatorial+optimization%3A+Papers+from+the+Spring+School+held+in+Schlo%C3%9F+Dagstuhl%2C+May%26nbsp%3B15%E2%80%9319%2C%26nbsp%3B2000&amp;rft.aufirst=Claude&amp;rft.aulast=Lemar%C3%A9chal&amp;rft.au=Lemar%C3%A9chal%2C+Claude&amp;rft.btitle=Lagrangian+relaxation&amp;rft.date=2001&amp;rft.genre=bookitem&amp;rft_id=info%3Adoi%2F10.1007%2F3-540-45586-8_4&amp;rft.isbn=3-540-42877-1&amp;rft.mr=1900016&amp;rft.pages=112-156&amp;rft.place=Berlin&amp;rft.pub=Springer-Verlag&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.volume=2241" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li>Nesterov, Y. and Nemirovsky, A. (1994). 'Interior Point Polynomial Methods in Convex Programming. <i>SIAM</i></li>
</ul>
<ul>
<li>Nesterov, Yurii. (2004). <i>Introductory Lectures on Convex Optimization</i>, Kluwer Academic Publishers</li>
</ul>
<ul>
<li><span class="citation book"><a href="/wiki/R._Tyrrell_Rockafellar" title="R. Tyrrell Rockafellar">Rockafellar, R. T.</a> (1970). <i>Convex analysis</i>. Princeton: Princeton University Press.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.aufirst=R.+T.&amp;rft.aulast=Rockafellar&amp;rft.au=Rockafellar%2C+R.+T.&amp;rft.btitle=Convex+analysis&amp;rft.date=1970&amp;rft.genre=book&amp;rft.place=Princeton&amp;rft.pub=Princeton+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><span class="citation book"><a href="/wiki/Andrzej_Piotr_Ruszczy%C5%84ski" title="Andrzej Piotr Ruszczyński">Ruszczyński, Andrzej</a> (2006). <i>Nonlinear Optimization</i>. Princeton University Press.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvex+optimization&amp;rft.aufirst=Andrzej&amp;rft.aulast=Ruszczy%C5%84ski&amp;rft.au=Ruszczy%C5%84ski%2C+Andrzej&amp;rft.btitle=Nonlinear+Optimization&amp;rft.date=2006&amp;rft.genre=book&amp;rft.pub=Princeton+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convex_optimization&amp;action=edit&amp;section=14" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li>Stephen Boyd and Lieven Vandenberghe, <a rel="nofollow" class="external text" href="http://www.stanford.edu/~boyd/cvxbook/"><i>Convex optimization</i></a> (book in pdf)</li>
<li><a rel="nofollow" class="external text" href="http://www.stanford.edu/class/ee364a/">EE364a: Convex Optimization I</a> and <a rel="nofollow" class="external text" href="http://www.stanford.edu/class/ee364b/">EE364b: Convex Optimization II</a>, Stanford course homepages</li>
<li><a rel="nofollow" class="external text" href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-253-convex-analysis-and-optimization-spring-2010/">6.253: Convex Analysis and Optimization</a>, an MIT OCW course homepage</li>
<li>Brian Borchers, <a rel="nofollow" class="external text" href="http://infohost.nmt.edu/~borchers/presentation.pdf">An overview of software for convex optimization</a></li>
</ul>
<ul>
<li class="nv-view"><a href="/wiki/Template:Optimization_algorithms" title="Template:Optimization algorithms"><span title="View this template" style=";;background:none transparent;border:none;;">v</span></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Optimization_algorithms" title="Template talk:Optimization algorithms"><span title="Discuss this template" style=";;background:none transparent;border:none;;">t</span></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Optimization_algorithms&amp;action=edit"><span title="Edit this template" style=";;background:none transparent;border:none;;">e</span></a></li>
</ul>
<ul>
<li><a href="/wiki/Golden_section_search" title="Golden section search">Golden section search</a></li>
<li><a href="/wiki/Powell%27s_method" title="Powell's method">Interpolation methods</a></li>
<li><a href="/wiki/Line_search" title="Line search">Line search</a></li>
<li><a href="/wiki/Nelder%E2%80%93Mead_method" title="Nelder–Mead method">Nelder–Mead method</a></li>
<li><a href="/wiki/Successive_parabolic_interpolation" title="Successive parabolic interpolation">Successive parabolic interpolation</a></li>
</ul>
<ul>
<li><a href="/wiki/Trust_region" title="Trust region">Trust region</a></li>
<li><a href="/wiki/Wolfe_conditions" title="Wolfe conditions">Wolfe conditions</a></li>
</ul>
<ul>
<li><a href="/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" title="Broyden–Fletcher–Goldfarb–Shanno algorithm">BFGS</a> and <a href="/wiki/Limited-memory_BFGS" title="Limited-memory BFGS">L-BFGS</a></li>
<li><a href="/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula" title="Davidon–Fletcher–Powell formula">DFP</a></li>
<li><a href="/wiki/SR1_formula" title="SR1 formula" class="mw-redirect">Symmetric rank-one (SR1)</a></li>
</ul>
<ul>
<li><a href="/wiki/Gauss%E2%80%93Newton_algorithm" title="Gauss–Newton algorithm">Gauss–Newton</a></li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient</a></li>
<li><a href="/wiki/Levenberg%E2%80%93Marquardt_algorithm" title="Levenberg–Marquardt algorithm">Levenberg–Marquardt</a></li>
<li><a href="/wiki/Nonlinear_conjugate_gradient_method" title="Nonlinear conjugate gradient method">Conjugate gradient</a></li>
</ul>
<ul>
<li><a href="/wiki/Newton%27s_method_in_optimization" title="Newton's method in optimization">Newton's method</a></li>
</ul>
<ul>
<li><a href="/wiki/Barrier_function" title="Barrier function">Barrier methods</a></li>
<li><a href="/wiki/Penalty_method" title="Penalty method">Penalty methods</a></li>
</ul>
<ul>
<li><a href="/wiki/Augmented_Lagrangian_method" title="Augmented Lagrangian method">Augmented Lagrangian methods</a></li>
<li><a href="/wiki/Sequential_quadratic_programming" title="Sequential quadratic programming">Sequential quadratic programming</a></li>
<li><a href="/wiki/Successive_linear_programming" title="Successive linear programming">Successive linear programming</a></li>
</ul>
<ul>
<li><a href="/wiki/Cutting-plane_method" title="Cutting-plane method">Cutting-plane method</a></li>
<li><a href="/wiki/Frank%E2%80%93Wolfe_algorithm" title="Frank–Wolfe algorithm">Reduced gradient (Frank–Wolfe)</a></li>
<li><a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient method</a></li>
</ul>
<ul>
<li><a href="/wiki/Ellipsoid_method" title="Ellipsoid method">Ellipsoid algorithm of Khachiyan</a></li>
<li><a href="/wiki/Karmarkar%27s_algorithm" title="Karmarkar's algorithm">Projective algorithm of Karmarkar</a></li>
</ul>
<ul>
<li><a href="/wiki/Simplex_algorithm" title="Simplex algorithm">Simplex algorithm of Dantzig</a></li>
<li><a href="/wiki/Revised_simplex_algorithm" title="Revised simplex algorithm" class="mw-redirect">Revised simplex algorithm</a></li>
<li><a href="/wiki/Criss-cross_algorithm" title="Criss-cross algorithm">Criss-cross algorithm</a></li>
<li><a href="/wiki/Lemke%27s_algorithm" title="Lemke's algorithm">Principal pivoting algorithm of Lemke</a></li>
</ul>
<ul>
<li><a href="/wiki/Approximation_algorithm" title="Approximation algorithm">Approximation algorithm</a></li>
<li><a href="/wiki/Dynamic_programming" title="Dynamic programming">Dynamic programming</a></li>
<li><a href="/wiki/Greedy_algorithm" title="Greedy algorithm">Greedy algorithm</a></li>
<li><a href="/wiki/Integer_programming" title="Integer programming">Integer programming</a>
<ul>
<li><a href="/wiki/Branch_and_bound" title="Branch and bound">Branch &amp; bound</a> or <a href="/wiki/Branch_and_cut" title="Branch and cut">cut</a></li>
</ul>
</li>
</ul>
<ul>
<li><a href="/wiki/Branch_and_bound" title="Branch and bound">Branch &amp; bound</a> or <a href="/wiki/Branch_and_cut" title="Branch and cut">cut</a></li>
</ul>
<ul>
<li><a href="/wiki/Bellman%E2%80%93Ford_algorithm" title="Bellman–Ford algorithm">Bellman–Ford</a></li>
<li><a href="/wiki/Bor%C5%AFvka%27s_algorithm" title="Borůvka's algorithm">Borůvka</a></li>
<li><a href="/wiki/Dijkstra%27s_algorithm" title="Dijkstra's algorithm">Dijkstra</a></li>
<li><a href="/wiki/Floyd%E2%80%93Warshall_algorithm" title="Floyd–Warshall algorithm">Floyd–Warshall</a></li>
<li><a href="/wiki/Johnson%27s_algorithm" title="Johnson's algorithm">Johnson</a></li>
<li><a href="/wiki/Kruskal%27s_algorithm" title="Kruskal's algorithm">Kruskal</a></li>
</ul>
<ul>
<li><a href="/wiki/Dinic%27s_algorithm" title="Dinic's algorithm">Dinic</a></li>
<li><a href="/wiki/Edmonds%E2%80%93Karp_algorithm" title="Edmonds–Karp algorithm">Edmonds–Karp</a></li>
<li><a href="/wiki/Ford%E2%80%93Fulkerson_algorithm" title="Ford–Fulkerson algorithm">Ford–Fulkerson</a></li>
<li><a href="/wiki/Push-relabel_maximum_flow_algorithm" title="Push-relabel maximum flow algorithm" class="mw-redirect">Push-relabel maximum flow</a></li>
</ul>
<ul>
<li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithm</a></li>
<li><a href="/wiki/Hill_climbing" title="Hill climbing">Hill climbing</a></li>
<li><a href="/wiki/Local_search_(optimization)" title="Local search (optimization)">Local search</a></li>
<li><a href="/wiki/Simulated_annealing" title="Simulated annealing">Simulated annealing</a></li>
<li><a href="/wiki/Tabu_search" title="Tabu search">Tabu search</a></li>
</ul>
<ul>
<li><b>Categories</b>
<ul>
<li><a href="/wiki/Category:Optimization_algorithms_and_methods" title="Category:Optimization algorithms and methods">Algorithms and methods</a></li>
<li><a href="/wiki/Category:Heuristic_algorithms" title="Category:Heuristic algorithms">Heuristics</a></li>
</ul>
</li>
<li><b><a href="/wiki/Comparison_of_optimization_software" title="Comparison of optimization software">Software</a></b></li>
</ul>
<ul>
<li><a href="/wiki/Category:Optimization_algorithms_and_methods" title="Category:Optimization algorithms and methods">Algorithms and methods</a></li>
<li><a href="/wiki/Category:Heuristic_algorithms" title="Category:Heuristic algorithms">Heuristics</a></li>
</ul>
