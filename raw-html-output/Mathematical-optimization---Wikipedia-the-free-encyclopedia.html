<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Mathematical optimization</span></h1>
<p>In <a href="/wiki/Mathematics" title="Mathematics">mathematics</a>, <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <a href="/wiki/Economics" title="Economics">economics</a>, or <a href="/wiki/Management_science" title="Management science" class="mw-redirect">management science</a>, <b>mathematical optimization</b> (alternatively, <b>optimization</b> or <b>mathematical programming</b>) is the selection of a best element (with regard to some criteria) from some set of available alternatives.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup></p>
<p>In the simplest case, an <a href="/wiki/Optimization_problem" title="Optimization problem">optimization problem</a> consists of <a href="/wiki/Maxima_and_minima" title="Maxima and minima">maximizing or minimizing</a> a <a href="/wiki/Function_of_a_real_variable" title="Function of a real variable">real function</a> by systematically choosing <a href="/wiki/Argument_of_a_function" title="Argument of a function">input</a> values from within an allowed set and computing the <a href="/wiki/Value_(mathematics)" title="Value (mathematics)">value</a> of the function. The generalization of optimization theory and techniques to other formulations comprises a large area of <a href="/wiki/Applied_mathematics" title="Applied mathematics">applied mathematics</a>. More generally, optimization includes finding "best available" values of some objective function given a defined <a href="/wiki/Domain_of_a_function" title="Domain of a function">domain</a> (or a set of constraints), including a variety of different types of objective functions and different types of domains.</p>
<p></p>
<h2>Contents</h2>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Optimization_problems"><span class="tocnumber">1</span> <span class="toctext">Optimization problems</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Notation"><span class="tocnumber">2</span> <span class="toctext">Notation</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Minimum_and_maximum_value_of_a_function"><span class="tocnumber">2.1</span> <span class="toctext">Minimum and maximum value of a function</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Optimal_input_arguments"><span class="tocnumber">2.2</span> <span class="toctext">Optimal input arguments</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#History"><span class="tocnumber">3</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Major_subfields"><span class="tocnumber">4</span> <span class="toctext">Major subfields</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Multi-objective_optimization"><span class="tocnumber">4.1</span> <span class="toctext">Multi-objective optimization</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Multi-modal_optimization"><span class="tocnumber">4.2</span> <span class="toctext">Multi-modal optimization</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#Classification_of_critical_points_and_extrema"><span class="tocnumber">5</span> <span class="toctext">Classification of critical points and extrema</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Feasibility_problem"><span class="tocnumber">5.1</span> <span class="toctext">Feasibility problem</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Existence"><span class="tocnumber">5.2</span> <span class="toctext">Existence</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Necessary_conditions_for_optimality"><span class="tocnumber">5.3</span> <span class="toctext">Necessary conditions for optimality</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Sufficient_conditions_for_optimality"><span class="tocnumber">5.4</span> <span class="toctext">Sufficient conditions for optimality</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Sensitivity_and_continuity_of_optima"><span class="tocnumber">5.5</span> <span class="toctext">Sensitivity and continuity of optima</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Calculus_of_optimization"><span class="tocnumber">5.6</span> <span class="toctext">Calculus of optimization</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#Computational_optimization_techniques"><span class="tocnumber">6</span> <span class="toctext">Computational optimization techniques</span></a>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#Optimization_algorithms"><span class="tocnumber">6.1</span> <span class="toctext">Optimization algorithms</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Iterative_methods"><span class="tocnumber">6.2</span> <span class="toctext">Iterative methods</span></a>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Global_convergence"><span class="tocnumber">6.2.1</span> <span class="toctext">Global convergence</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-20"><a href="#Heuristics"><span class="tocnumber">6.3</span> <span class="toctext">Heuristics</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-21"><a href="#Applications"><span class="tocnumber">7</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-22"><a href="#Mechanics_and_engineering"><span class="tocnumber">7.1</span> <span class="toctext">Mechanics and engineering</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Economics"><span class="tocnumber">7.2</span> <span class="toctext">Economics</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Operations_research"><span class="tocnumber">7.3</span> <span class="toctext">Operations research</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Control_engineering"><span class="tocnumber">7.4</span> <span class="toctext">Control engineering</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Petroleum_engineering"><span class="tocnumber">7.5</span> <span class="toctext">Petroleum engineering</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Molecular_modeling"><span class="tocnumber">7.6</span> <span class="toctext">Molecular modeling</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-28"><a href="#Solvers"><span class="tocnumber">8</span> <span class="toctext">Solvers</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-30"><a href="#Notes"><span class="tocnumber">10</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-31"><a href="#Further_reading"><span class="tocnumber">11</span> <span class="toctext">Further reading</span></a>
<ul>
<li class="toclevel-2 tocsection-32"><a href="#Comprehensive"><span class="tocnumber">11.1</span> <span class="toctext">Comprehensive</span></a>
<ul>
<li class="toclevel-3 tocsection-33"><a href="#Undergraduate_level"><span class="tocnumber">11.1.1</span> <span class="toctext">Undergraduate level</span></a></li>
<li class="toclevel-3 tocsection-34"><a href="#Graduate_level"><span class="tocnumber">11.1.2</span> <span class="toctext">Graduate level</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-35"><a href="#Continuous_optimization"><span class="tocnumber">11.2</span> <span class="toctext">Continuous optimization</span></a></li>
<li class="toclevel-2 tocsection-36"><a href="#Combinatorial_optimization"><span class="tocnumber">11.3</span> <span class="toctext">Combinatorial optimization</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-37"><a href="#Journals"><span class="tocnumber">12</span> <span class="toctext">Journals</span></a></li>
<li class="toclevel-1 tocsection-38"><a href="#External_links"><span class="tocnumber">13</span> <span class="toctext">External links</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Minimum_and_maximum_value_of_a_function"><span class="tocnumber">2.1</span> <span class="toctext">Minimum and maximum value of a function</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Optimal_input_arguments"><span class="tocnumber">2.2</span> <span class="toctext">Optimal input arguments</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Multi-objective_optimization"><span class="tocnumber">4.1</span> <span class="toctext">Multi-objective optimization</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Multi-modal_optimization"><span class="tocnumber">4.2</span> <span class="toctext">Multi-modal optimization</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Feasibility_problem"><span class="tocnumber">5.1</span> <span class="toctext">Feasibility problem</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Existence"><span class="tocnumber">5.2</span> <span class="toctext">Existence</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Necessary_conditions_for_optimality"><span class="tocnumber">5.3</span> <span class="toctext">Necessary conditions for optimality</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Sufficient_conditions_for_optimality"><span class="tocnumber">5.4</span> <span class="toctext">Sufficient conditions for optimality</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Sensitivity_and_continuity_of_optima"><span class="tocnumber">5.5</span> <span class="toctext">Sensitivity and continuity of optima</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Calculus_of_optimization"><span class="tocnumber">5.6</span> <span class="toctext">Calculus of optimization</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#Optimization_algorithms"><span class="tocnumber">6.1</span> <span class="toctext">Optimization algorithms</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Iterative_methods"><span class="tocnumber">6.2</span> <span class="toctext">Iterative methods</span></a>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Global_convergence"><span class="tocnumber">6.2.1</span> <span class="toctext">Global convergence</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-20"><a href="#Heuristics"><span class="tocnumber">6.3</span> <span class="toctext">Heuristics</span></a></li>
</ul>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Global_convergence"><span class="tocnumber">6.2.1</span> <span class="toctext">Global convergence</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-22"><a href="#Mechanics_and_engineering"><span class="tocnumber">7.1</span> <span class="toctext">Mechanics and engineering</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Economics"><span class="tocnumber">7.2</span> <span class="toctext">Economics</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Operations_research"><span class="tocnumber">7.3</span> <span class="toctext">Operations research</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Control_engineering"><span class="tocnumber">7.4</span> <span class="toctext">Control engineering</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Petroleum_engineering"><span class="tocnumber">7.5</span> <span class="toctext">Petroleum engineering</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Molecular_modeling"><span class="tocnumber">7.6</span> <span class="toctext">Molecular modeling</span></a></li>
</ul>
<ul>
<li class="toclevel-2 tocsection-32"><a href="#Comprehensive"><span class="tocnumber">11.1</span> <span class="toctext">Comprehensive</span></a>
<ul>
<li class="toclevel-3 tocsection-33"><a href="#Undergraduate_level"><span class="tocnumber">11.1.1</span> <span class="toctext">Undergraduate level</span></a></li>
<li class="toclevel-3 tocsection-34"><a href="#Graduate_level"><span class="tocnumber">11.1.2</span> <span class="toctext">Graduate level</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-35"><a href="#Continuous_optimization"><span class="tocnumber">11.2</span> <span class="toctext">Continuous optimization</span></a></li>
<li class="toclevel-2 tocsection-36"><a href="#Combinatorial_optimization"><span class="tocnumber">11.3</span> <span class="toctext">Combinatorial optimization</span></a></li>
</ul>
<ul>
<li class="toclevel-3 tocsection-33"><a href="#Undergraduate_level"><span class="tocnumber">11.1.1</span> <span class="toctext">Undergraduate level</span></a></li>
<li class="toclevel-3 tocsection-34"><a href="#Graduate_level"><span class="tocnumber">11.1.2</span> <span class="toctext">Graduate level</span></a></li>
</ul>
<p></p>
<h2><span class="mw-headline" id="Optimization_problems">Optimization problems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=1" title="Edit section: Optimization problems">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>An optimization problem can be represented in the following way:</p>
<p>Such a formulation is called an <b><a href="/wiki/Optimization_problem" title="Optimization problem">optimization problem</a></b> or a <b>mathematical programming problem</b> (a term not directly related to <a href="/wiki/Computer_programming" title="Computer programming">computer programming</a>, but still in use for example in <a href="/wiki/Linear_programming" title="Linear programming">linear programming</a> – see <a href="#History">History</a> below). Many real-world and theoretical problems may be modeled in this general framework. Problems formulated using this technique in the fields of <a href="/wiki/Physics" title="Physics">physics</a> and <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> may refer to the technique as <b>energy minimization</b>, speaking of the value of the function <i>f</i> as representing the energy of the <a href="/wiki/System" title="System">system</a> being <a href="/wiki/Mathematical_model" title="Mathematical model">modeled</a>.</p>
<p>Typically, <i>A</i> is some <a href="/wiki/Subset" title="Subset">subset</a> of the <a href="/wiki/Euclidean_space" title="Euclidean space">Euclidean space</a> <b>R</b><sup><i>n</i></sup>, often specified by a set of <i><a href="/wiki/Constraint_(mathematics)" title="Constraint (mathematics)">constraints</a></i>, equalities or inequalities that the members of <i>A</i> have to satisfy. The <a href="/wiki/Domain_(mathematics)" title="Domain (mathematics)" class="mw-redirect">domain</a> <i>A</i> of <i>f</i> is called the <i>search space</i> or the <i>choice set</i>, while the elements of <i>A</i> are called <i><a href="/wiki/Candidate_solution" title="Candidate solution">candidate solutions</a></i> or <i>feasible solutions</i>.</p>
<p>The function <i>f</i> is called, variously, an <b>objective function</b>, a <b><a href="/wiki/Loss_function" title="Loss function">loss function</a></b> or <b>cost function</b> (minimization),<sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup> <b>indirect utility function</b> (minimization),<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup> a <b>utility function</b> (maximization), a <b>fitness function</b> (maximization), or, in certain fields, an <b>energy function</b>, or <b>energy <a href="/wiki/Functional_(mathematics)" title="Functional (mathematics)">functional</a></b>. A feasible solution that minimizes (or maximizes, if that is the goal) the objective function is called an <i>optimal solution</i>.</p>
<p>By convention, the standard form of an optimization problem is stated in terms of minimization. Generally, unless both the objective function and the <a href="/wiki/Feasible_region" title="Feasible region">feasible region</a> are <a href="/wiki/Convex_function" title="Convex function">convex</a> in a minimization problem, there may be several local minima, where a <i>local minimum</i> x<sup>*</sup> is defined as a point for which there exists some δ &gt; 0 so that for all x such that</p>
<p>the expression</p>
<p>holds; that is to say, on some region around x<sup>*</sup> all of the function values are greater than or equal to the value at that point. Local maxima are defined similarly.</p>
<p>A large number of algorithms proposed for solving non-convex problems – including the majority of commercially available solvers – are not capable of making a distinction between local optimal solutions and rigorous optimal solutions, and will treat the former as actual solutions to the original problem. The branch of <a href="/wiki/Applied_mathematics" title="Applied mathematics">applied mathematics</a> and <a href="/wiki/Numerical_analysis" title="Numerical analysis">numerical analysis</a> that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a non-convex problem is called <a href="/wiki/Global_optimization" title="Global optimization">global optimization</a>.</p>
<h2><span class="mw-headline" id="Notation">Notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=2" title="Edit section: Notation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Optimization problems are often expressed with special notation. Here are some examples.</p>
<h3><span class="mw-headline" id="Minimum_and_maximum_value_of_a_function">Minimum and maximum value of a function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=3" title="Edit section: Minimum and maximum value of a function">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Consider the following notation:</p>
<p>This denotes the minimum <a href="/wiki/Value_(mathematics)" title="Value (mathematics)">value</a> of the objective function <img class="mwe-math-fallback-image-inline tex" alt="x^2 + 1" src="//upload.wikimedia.org/math/1/9/c/19cb4465624f301eeb8f8a6d1d60276b.png">, when choosing <i>x</i> from the set of <a href="/wiki/Real_number" title="Real number">real numbers</a> <img class="mwe-math-fallback-image-inline tex" alt="\mathbb R" src="//upload.wikimedia.org/math/0/c/9/0c95a37acc94ef8c093ce39c36e07886.png">. The minimum value in this case is <img class="mwe-math-fallback-image-inline tex" alt="1" src="//upload.wikimedia.org/math/c/4/c/c4ca4238a0b923820dcc509a6f75849b.png">, occurring at <img class="mwe-math-fallback-image-inline tex" alt="x = 0" src="//upload.wikimedia.org/math/e/1/1/e11729b0b65ecade3fc272548a3883fc.png">.</p>
<p>Similarly, the notation</p>
<p>asks for the maximum value of the objective function 2<i>x</i>, where <i>x</i> may be any real number. In this case, there is no such maximum as the objective function is unbounded, so the answer is "<a href="/wiki/Infinity" title="Infinity">infinity</a>" or "undefined".</p>
<h3><span class="mw-headline" id="Optimal_input_arguments">Optimal input arguments</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=4" title="Edit section: Optimal input arguments">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Consider the following notation:</p>
<p>or equivalently</p>
<p>This represents the value (or values) of the <a href="/wiki/Argument_of_a_function" title="Argument of a function">argument</a> <i>x</i> in the <a href="/wiki/Interval_(mathematics)" title="Interval (mathematics)">interval</a> <img class="mwe-math-fallback-image-inline tex" alt="(-\infty,-1]" src="//upload.wikimedia.org/math/2/b/b/2bbda46f09c030f245f9afe1fa0eb85f.png"> that minimizes (or minimize) the objective function <i>x</i><sup>2</sup> + 1 (the actual minimum value of that function is not what the problem asks for). In this case, the answer is <i>x</i> = -1, since <i>x</i> = 0 is infeasible, i.e. does not belong to the <a href="/wiki/Feasible_set" title="Feasible set" class="mw-redirect">feasible set</a>.</p>
<p>Similarly,</p>
<p>or equivalently</p>
<p>represents the <img class="mwe-math-fallback-image-inline tex" alt="(x,y)" src="//upload.wikimedia.org/math/9/0/c/90cbc22edf225adf8a68974f51227f05.png"> pair (or pairs) that maximizes (or maximize) the value of the objective function <img class="mwe-math-fallback-image-inline tex" alt="x\cos(y)" src="//upload.wikimedia.org/math/3/1/6/316799a17e084a5141faa77538e39aa2.png">, with the added constraint that <i>x</i> lie in the interval <img class="mwe-math-fallback-image-inline tex" alt="[-5,5]" src="//upload.wikimedia.org/math/7/2/8/728629d5db271bc36f160c71b422fa90.png"> (again, the actual maximum value of the expression does not matter). In this case, the solutions are the pairs of the form (5, 2k<a href="/wiki/Pi" title="Pi">π</a>) and (−5,(2k+1)π), where <i>k</i> ranges over all <a href="/wiki/Integer" title="Integer">integers</a>.</p>
<p><b>Arg min</b> and <b>arg max</b> are sometimes also written <b>argmin</b> and <b>argmax</b>, and stand for <b>argument of the minimum</b> and <b>argument of the maximum</b>.</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=5" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Pierre_de_Fermat" title="Pierre de Fermat">Fermat</a> and <a href="/wiki/Joseph_Louis_Lagrange" title="Joseph Louis Lagrange" class="mw-redirect">Lagrange</a> found calculus-based formulas for identifying optima, while <a href="/wiki/Isaac_Newton" title="Isaac Newton">Newton</a> and <a href="/wiki/Carl_Friedrich_Gauss" title="Carl Friedrich Gauss">Gauss</a> proposed iterative methods for moving towards an optimum. Historically, the first term for optimization was "<a href="/wiki/Linear_programming" title="Linear programming">linear programming</a>", which was due to <a href="/wiki/George_Dantzig" title="George Dantzig">George B. Dantzig</a>, although much of the theory had been introduced by <a href="/wiki/Leonid_Kantorovich" title="Leonid Kantorovich">Leonid Kantorovich</a> in 1939. Dantzig published the <a href="/wiki/Simplex_algorithm" title="Simplex algorithm">Simplex algorithm</a> in 1947, and <a href="/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a> developed the theory of <a href="/wiki/Linear_programming#Duality" title="Linear programming">duality</a> in the same year.</p>
<p>The term, <i>programming</i>, in this context does not refer to <a href="/wiki/Computer_programming" title="Computer programming">computer programming</a>. Rather, the term comes from the use of <i>program</i> by the United States military to refer to proposed training and <a href="/wiki/Logistics" title="Logistics">logistics</a> schedules, which were the problems Dantzig studied at that time.</p>
<p>Later important researchers in mathematical optimization include the following:</p>
<ul>
<li><a href="/wiki/Richard_Bellman" title="Richard Bellman" class="mw-redirect">Richard Bellman</a></li>
<li><a href="/wiki/Roger_Fletcher_(mathematician)" title="Roger Fletcher (mathematician)">Roger Fletcher</a></li>
<li><a href="/wiki/Ronald_A._Howard" title="Ronald A. Howard">Ronald A. Howard</a></li>
<li><a href="/wiki/Narendra_Karmarkar" title="Narendra Karmarkar">Narendra Karmarkar</a></li>
<li><a href="/wiki/William_Karush" title="William Karush">William Karush</a></li>
<li><a href="/wiki/Leonid_Khachiyan" title="Leonid Khachiyan">Leonid Khachiyan</a></li>
<li><a href="/wiki/Bernard_Koopman" title="Bernard Koopman">Bernard Koopman</a></li>
<li><a href="/wiki/Harold_Kuhn" title="Harold Kuhn" class="mw-redirect">Harold Kuhn</a></li>
<li><a href="/wiki/Joseph_Louis_Lagrange" title="Joseph Louis Lagrange" class="mw-redirect">Joseph Louis Lagrange</a></li>
<li><a href="/wiki/L%C3%A1szl%C3%B3_Lov%C3%A1sz" title="László Lovász">László Lovász</a></li>
</ul>
<ul>
<li><a href="/wiki/Arkadi_Nemirovski" title="Arkadi Nemirovski">Arkadi Nemirovski</a></li>
<li><a href="/wiki/Yurii_Nesterov" title="Yurii Nesterov">Yurii Nesterov</a></li>
<li><a href="/w/index.php?title=Boris_Polyak&amp;action=edit&amp;redlink=1" class="new" title="Boris Polyak (page does not exist)">Boris Polyak</a></li>
<li><a href="/wiki/Lev_Pontryagin" title="Lev Pontryagin">Lev Pontryagin</a></li>
<li><a href="/w/index.php?title=James_Renegar&amp;action=edit&amp;redlink=1" class="new" title="James Renegar (page does not exist)">James Renegar</a></li>
<li><a href="/wiki/R._Tyrrell_Rockafellar" title="R. Tyrrell Rockafellar">R. Tyrrell Rockafellar</a></li>
<li><a href="/w/index.php?title=Cornelis_Roos&amp;action=edit&amp;redlink=1" class="new" title="Cornelis Roos (page does not exist)">Cornelis Roos</a></li>
<li><a href="/wiki/Naum_Z._Shor" title="Naum Z. Shor">Naum Z. Shor</a></li>
<li><a href="/w/index.php?title=Michael_J._Todd_(mathematician)&amp;action=edit&amp;redlink=1" class="new" title="Michael J. Todd (mathematician) (page does not exist)">Michael J. Todd</a></li>
<li><a href="/wiki/Albert_W._Tucker" title="Albert W. Tucker">Albert Tucker</a></li>
</ul>
<h2><span class="mw-headline" id="Major_subfields">Major subfields</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=6" title="Edit section: Major subfields">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Convex_programming" title="Convex programming" class="mw-redirect">Convex programming</a> studies the case when the objective function is <a href="/wiki/Convex_function" title="Convex function">convex</a> (minimization) or <a href="/wiki/Concave_function" title="Concave function">concave</a> (maximization) and the constraint set is <a href="/wiki/Convex_set" title="Convex set">convex</a>. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming.
<ul>
<li><a href="/wiki/Linear_programming" title="Linear programming">Linear programming</a> (LP), a type of convex programming, studies the case in which the objective function <i>f</i> is linear and the set of constraints is specified using only linear equalities and inequalities. Such a set is called a <a href="/wiki/Polyhedron" title="Polyhedron">polyhedron</a> or a <a href="/wiki/Polytope" title="Polytope">polytope</a> if it is <a href="/wiki/Bounded_set" title="Bounded set">bounded</a>.</li>
<li><a href="/wiki/Second_order_cone_programming" title="Second order cone programming" class="mw-redirect">Second order cone programming</a> (SOCP) is a convex program, and includes certain types of quadratic programs.</li>
<li><a href="/wiki/Semidefinite_programming" title="Semidefinite programming">Semidefinite programming</a> (SDP) is a subfield of convex optimization where the underlying variables are <a href="/wiki/Semidefinite" title="Semidefinite" class="mw-redirect">semidefinite</a> <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrices</a>. It is generalization of linear and convex quadratic programming.</li>
<li><a href="/wiki/Conic_programming" title="Conic programming" class="mw-redirect">Conic programming</a> is a general form of convex programming. LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone.</li>
<li><a href="/wiki/Geometric_programming" title="Geometric programming">Geometric programming</a> is a technique whereby objective and inequality constraints expressed as <a href="/wiki/Posynomials" title="Posynomials" class="mw-redirect">posynomials</a> and equality constraints as <a href="/wiki/Monomials" title="Monomials" class="mw-redirect">monomials</a> can be transformed into a convex program.</li>
</ul>
</li>
<li><a href="/wiki/Integer_programming" title="Integer programming">Integer programming</a> studies linear programs in which some or all variables are constrained to take on <a href="/wiki/Integer" title="Integer">integer</a> values. This is not convex, and in general much more difficult than regular linear programming.</li>
<li><a href="/wiki/Quadratic_programming" title="Quadratic programming">Quadratic programming</a> allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities. For specific forms of the quadratic term, this is a type of convex programming.</li>
<li><a href="/wiki/Fractional_programming" title="Fractional programming">Fractional programming</a> studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem.</li>
<li><a href="/wiki/Nonlinear_programming" title="Nonlinear programming">Nonlinear programming</a> studies the general case in which the objective function or the constraints or both contain nonlinear parts. This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it.</li>
<li><a href="/wiki/Stochastic_programming" title="Stochastic programming">Stochastic programming</a> studies the case in which some of the constraints or parameters depend on <a href="/wiki/Random_variable" title="Random variable">random variables</a>.</li>
<li><a href="/wiki/Robust_optimization" title="Robust optimization">Robust programming</a> is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization targets to find solutions that are valid under all possible realizations of the uncertainties.</li>
<li><a href="/wiki/Combinatorial_optimization" title="Combinatorial optimization">Combinatorial optimization</a> is concerned with problems where the set of feasible solutions is discrete or can be reduced to a <a href="/wiki/Discrete_mathematics" title="Discrete mathematics">discrete</a> one.</li>
<li><a href="/wiki/Stochastic_optimization" title="Stochastic optimization">Stochastic optimization</a> for use with random (noisy) function measurements or random inputs in the search process.</li>
<li><a href="/wiki/Infinite-dimensional_optimization" title="Infinite-dimensional optimization">Infinite-dimensional optimization</a> studies the case when the set of feasible solutions is a subset of an infinite-<a href="/wiki/Dimension" title="Dimension">dimensional</a> space, such as a space of functions.</li>
<li><a href="/wiki/Heuristic_(computer_science)" title="Heuristic (computer science)">Heuristics</a> and <a href="/wiki/Metaheuristic" title="Metaheuristic">metaheuristics</a> make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems.</li>
<li><a href="/wiki/Constraint_satisfaction" title="Constraint satisfaction">Constraint satisfaction</a> studies the case in which the objective function <i>f</i> is constant (this is used in <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>, particularly in <a href="/wiki/Automated_reasoning" title="Automated reasoning">automated reasoning</a>).
<ul>
<li><a href="/wiki/Constraint_programming" title="Constraint programming">Constraint programming</a>.</li>
</ul>
</li>
<li>Disjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling.</li>
</ul>
<ul>
<li><a href="/wiki/Linear_programming" title="Linear programming">Linear programming</a> (LP), a type of convex programming, studies the case in which the objective function <i>f</i> is linear and the set of constraints is specified using only linear equalities and inequalities. Such a set is called a <a href="/wiki/Polyhedron" title="Polyhedron">polyhedron</a> or a <a href="/wiki/Polytope" title="Polytope">polytope</a> if it is <a href="/wiki/Bounded_set" title="Bounded set">bounded</a>.</li>
<li><a href="/wiki/Second_order_cone_programming" title="Second order cone programming" class="mw-redirect">Second order cone programming</a> (SOCP) is a convex program, and includes certain types of quadratic programs.</li>
<li><a href="/wiki/Semidefinite_programming" title="Semidefinite programming">Semidefinite programming</a> (SDP) is a subfield of convex optimization where the underlying variables are <a href="/wiki/Semidefinite" title="Semidefinite" class="mw-redirect">semidefinite</a> <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrices</a>. It is generalization of linear and convex quadratic programming.</li>
<li><a href="/wiki/Conic_programming" title="Conic programming" class="mw-redirect">Conic programming</a> is a general form of convex programming. LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone.</li>
<li><a href="/wiki/Geometric_programming" title="Geometric programming">Geometric programming</a> is a technique whereby objective and inequality constraints expressed as <a href="/wiki/Posynomials" title="Posynomials" class="mw-redirect">posynomials</a> and equality constraints as <a href="/wiki/Monomials" title="Monomials" class="mw-redirect">monomials</a> can be transformed into a convex program.</li>
</ul>
<ul>
<li><a href="/wiki/Constraint_programming" title="Constraint programming">Constraint programming</a>.</li>
</ul>
<p>In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time):</p>
<ul>
<li><a href="/wiki/Calculus_of_variations" title="Calculus of variations">Calculus of variations</a> seeks to optimize an objective defined over many points in time, by considering how the objective function changes if there is a small change in the choice path.</li>
<li><a href="/wiki/Optimal_control" title="Optimal control">Optimal control</a> theory is a generalization of the calculus of variations.</li>
<li><a href="/wiki/Dynamic_programming" title="Dynamic programming">Dynamic programming</a> studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the <a href="/wiki/Bellman_equation" title="Bellman equation">Bellman equation</a>.</li>
<li><a href="/wiki/Mathematical_programming_with_equilibrium_constraints" title="Mathematical programming with equilibrium constraints">Mathematical programming with equilibrium constraints</a> is where the constraints include <a href="/wiki/Variational_inequalities" title="Variational inequalities" class="mw-redirect">variational inequalities</a> or <a href="/wiki/Complementarity_theory" title="Complementarity theory">complementarities</a>.</li>
</ul>
<h3><span class="mw-headline" id="Multi-objective_optimization">Multi-objective optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=7" title="Edit section: Multi-objective optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Adding more than one objective to an optimization problem adds complexity. For example, to optimize a structural design, one would want a design that is both light and rigid. Because these two objectives conflict, a trade-off exists. There will be one lightest design, one stiffest design, and an infinite number of designs that are some compromise of weight and stiffness. The set of trade-off designs that cannot be improved upon according to one criterion without hurting another criterion is known as the <a href="/wiki/Pareto_set" title="Pareto set" class="mw-redirect">Pareto set</a>. The curve created plotting weight against stiffness of the best designs is known as the <a href="/wiki/Pareto_frontier" title="Pareto frontier" class="mw-redirect">Pareto frontier</a>.</p>
<p>A design is judged to be "Pareto optimal" (equivalently, "Pareto efficient" or in the Pareto set) if it is not dominated by any other design: If it is worse than another design in some respects and no better in any respect, then it is dominated and is not Pareto optimal.</p>
<p>The choice among "Pareto optimal" solutions to determine the "favorite solution" is delegated to the decision maker. In other words, defining the problem as multiobjective optimization signals that some information is missing: desirable objectives are given but not their detailed combination. In some cases, the missing information can be derived by interactive sessions with the decision maker.</p>
<p>Multi-objective optimization problems have been generalized further to <a href="/wiki/Vector_optimization" title="Vector optimization">vector optimization</a> problems where the (partial) ordering is no longer given by the Pareto ordering.</p>
<h3><span class="mw-headline" id="Multi-modal_optimization">Multi-modal optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=8" title="Edit section: Multi-modal optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Optimization problems are often multi-modal; that is, they possess multiple good solutions. They could all be globally good (same cost function value) or there could be a mix of globally good and locally good solutions. Obtaining all (or at least some of) the multiple solutions is the goal of a multi-modal optimizer.</p>
<p>Classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm. <a href="/wiki/Evolutionary_Algorithm" title="Evolutionary Algorithm" class="mw-redirect">Evolutionary Algorithms</a> are however a very popular approach to obtain multiple solutions in a multi-modal optimization task.</p>
<h2><span class="mw-headline" id="Classification_of_critical_points_and_extrema">Classification of critical points and extrema</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=9" title="Edit section: Classification of critical points and extrema">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Feasibility_problem">Feasibility problem</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=10" title="Edit section: Feasibility problem">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <b><a href="/wiki/Satisfiability_problem" title="Satisfiability problem" class="mw-redirect">satisfiability problem</a></b>, also called the <b>feasibility problem</b>, is just the problem of finding any <a href="/wiki/Feasible_solution" title="Feasible solution" class="mw-redirect">feasible solution</a> at all without regard to objective value. This can be regarded as the special case of mathematical optimization where the objective value is the same for every solution, and thus any solution is optimal.</p>
<p>Many optimization algorithms need to start from a feasible point. One way to obtain such a point is to <a href="/wiki/Relaxation_(approximation)" title="Relaxation (approximation)">relax</a> the feasibility conditions using a <a href="/wiki/Slack_variable" title="Slack variable">slack variable</a>; with enough slack, any starting point is feasible. Then, minimize that slack variable until slack is null or negative.</p>
<h3><span class="mw-headline" id="Existence">Existence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=11" title="Edit section: Existence">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Extreme_value_theorem" title="Extreme value theorem">extreme value theorem</a> of <a href="/wiki/Karl_Weierstrass" title="Karl Weierstrass">Karl Weierstrass</a> states that a continuous real-valued function on a compact set attains its maximum and minimum value. More generally, a lower semi-continuous function on a compact set attains its minimum; an upper semi-continuous function on a compact set attains its maximum.</p>
<h3><span class="mw-headline" id="Necessary_conditions_for_optimality">Necessary conditions for optimality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=12" title="Edit section: Necessary conditions for optimality">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Fermat%27s_theorem_(stationary_points)" title="Fermat's theorem (stationary points)">One of Fermat's theorems</a> states that optima of unconstrained problems are found at <a href="/wiki/Stationary_point" title="Stationary point">stationary points</a>, where the first derivative or the gradient of the objective function is zero (see <a href="/wiki/First_derivative_test" title="First derivative test">first derivative test</a>). More generally, they may be found at <a href="/wiki/Critical_point_(mathematics)" title="Critical point (mathematics)">critical points</a>, where the first derivative or gradient of the objective function is zero or is undefined, or on the boundary of the choice set. An equation (or set of equations) stating that the first derivative(s) equal(s) zero at an interior optimum is called a 'first-order condition' or a set of first-order conditions.</p>
<p>Optima of equality-constrained problems can be found by the <a href="/wiki/Lagrange_multiplier" title="Lagrange multiplier">Lagrange multiplier</a> method. The optima of problems with equality and/or inequality constraints can be found using the '<a href="/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" title="Karush–Kuhn–Tucker conditions">Karush–Kuhn–Tucker conditions</a>'.</p>
<h3><span class="mw-headline" id="Sufficient_conditions_for_optimality">Sufficient conditions for optimality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=13" title="Edit section: Sufficient conditions for optimality">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While the first derivative test identifies points that might be extrema, this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither. When the objective function is twice differentiable, these cases can be distinguished by checking the second derivative or the matrix of second derivatives (called the <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessian matrix</a>) in unconstrained problems, or the matrix of second derivatives of the objective function and the constraints called the <a href="/wiki/Hessian_matrix#Bordered_Hessian" title="Hessian matrix">bordered Hessian</a> in constrained problems. The conditions that distinguish maxima, or minima, from other stationary points are called 'second-order conditions' (see '<a href="/wiki/Second_derivative_test" title="Second derivative test">Second derivative test</a>'). If a candidate solution satisfies the first-order conditions, then satisfaction of the second-order conditions as well is sufficient to establish at least local optimality.</p>
<h3><span class="mw-headline" id="Sensitivity_and_continuity_of_optima">Sensitivity and continuity of optima</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=14" title="Edit section: Sensitivity and continuity of optima">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Envelope_theorem" title="Envelope theorem">envelope theorem</a> describes how the value of an optimal solution changes when an underlying <a href="/wiki/Parameter" title="Parameter">parameter</a> changes. The process of computing this change is called <a href="/wiki/Comparative_statics" title="Comparative statics">comparative statics</a>.</p>
<p>The <a href="/wiki/Maximum_theorem" title="Maximum theorem">maximum theorem</a> of <a href="/wiki/Claude_Berge" title="Claude Berge">Claude Berge</a> (1963) describes the continuity of an optimal solution as a function of underlying parameters.</p>
<h3><span class="mw-headline" id="Calculus_of_optimization">Calculus of optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=15" title="Edit section: Calculus of optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>For unconstrained problems with twice-differentiable functions, some <a href="/wiki/Critical_point_(mathematics)" title="Critical point (mathematics)">critical points</a> can be found by finding the points where the <a href="/wiki/Gradient" title="Gradient">gradient</a> of the objective function is zero (that is, the stationary points). More generally, a zero <a href="/wiki/Subgradient" title="Subgradient" class="mw-redirect">subgradient</a> certifies that a local minimum has been found for <a href="/wiki/Convex_optimization" title="Convex optimization">minimization problems with convex</a> <a href="/wiki/Convex_function" title="Convex function">functions</a> and other <a href="/wiki/Rademacher%27s_theorem" title="Rademacher's theorem">locally</a> <a href="/wiki/Lipschitz_function" title="Lipschitz function" class="mw-redirect">Lipschitz functions</a>.</p>
<p>Further, critical points can be classified using the <a href="/wiki/Positive_definite_matrix" title="Positive definite matrix" class="mw-redirect">definiteness</a> of the <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessian matrix</a>: If the Hessian is <i>positive</i> definite at a critical point, then the point is a local minimum; if the Hessian matrix is negative definite, then the point is a local maximum; finally, if indefinite, then the point is some kind of <a href="/wiki/Saddle_point" title="Saddle point">saddle point</a>.</p>
<p>Constrained problems can often be transformed into unconstrained problems with the help of <a href="/wiki/Lagrange_multiplier" title="Lagrange multiplier">Lagrange multipliers</a>. <a href="/wiki/Lagrangian_relaxation" title="Lagrangian relaxation">Lagrangian relaxation</a> can also provide approximate solutions to difficult constrained problems.</p>
<p>When the objective function is <a href="/wiki/Convex_function" title="Convex function">convex</a>, then any local minimum will also be a global minimum. There exist efficient numerical techniques for minimizing convex functions, such as <a href="/wiki/Interior-point_method" title="Interior-point method" class="mw-redirect">interior-point methods</a>.</p>
<h2><span class="mw-headline" id="Computational_optimization_techniques">Computational optimization techniques</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=16" title="Edit section: Computational optimization techniques">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>To solve problems, researchers may use <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> that terminate in a finite number of steps, or <a href="/wiki/Iterative_method" title="Iterative method">iterative methods</a> that converge to a solution (on some specified class of problems), or <a href="/wiki/Heuristic_algorithm" title="Heuristic algorithm" class="mw-redirect">heuristics</a> that may provide approximate solutions to some problems (although their iterates need not converge).</p>
<h3><span class="mw-headline" id="Optimization_algorithms">Optimization algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=17" title="Edit section: Optimization algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><a href="/wiki/Simplex_algorithm" title="Simplex algorithm">Simplex algorithm</a> of <a href="/wiki/George_Dantzig" title="George Dantzig">George Dantzig</a>, designed for <a href="/wiki/Linear_programming" title="Linear programming">linear programming</a>.</li>
<li>Extensions of the simplex algorithm, designed for <a href="/wiki/Quadratic_programming" title="Quadratic programming">quadratic programming</a> and for <a href="/wiki/Linear-fractional_programming" title="Linear-fractional programming">linear-fractional programming</a>.</li>
<li>Variants of the simplex algorithm that are especially suited for <a href="/wiki/Flow_network" title="Flow network">network optimization</a>.</li>
<li><a href="/wiki/Combinatorial_optimization" title="Combinatorial optimization">Combinatorial algorithms</a></li>
</ul>
<h3><span class="mw-headline" id="Iterative_methods">Iterative methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=18" title="Edit section: Iterative methods">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Iterative_methods" title="Iterative methods" class="mw-redirect">iterative methods</a> used to solve problems of <a href="/wiki/Nonlinear_programming" title="Nonlinear programming">nonlinear programming</a> differ according to whether they <a href="/wiki/Subroutine" title="Subroutine">evaluate</a> <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessians</a>, gradients, or only function values. While evaluating Hessians (H) and gradients (G) improves the rate of convergence, for functions for which these quantities exist and vary sufficiently smoothly, such evaluations increase the <a href="/wiki/Computational_complexity" title="Computational complexity" class="mw-redirect">computational complexity</a> (or computational cost) of each iteration. In some cases, the computational complexity may be excessively high.</p>
<p>One major criterion for optimizers is just the number of required function evaluations as this often is already a large computational effort, usually much more effort than within the optimizer itself, which mainly has to operate over the N variables. The derivatives provide detailed information for such optimizers, but are even harder to calculate, e.g. approximating the gradient takes at least N+1 function evaluations. For approximations of the 2nd derivatives (collected in the Hessian matrix) the number of function evaluations is in the order of N². Newton's method requires the 2nd order derivates, so for each iteration the number of function calls is in the order of N², but for a simpler pure gradient optimizer it is only N. However, gradient optimizers need usually more iterations than Newton's algorithm. Which one is best with respect to the number of function calls depends on the problem itself.</p>
<ul>
<li>Methods that evaluate Hessians (or approximate Hessians, using <a href="/wiki/Finite_difference" title="Finite difference">finite differences</a>):
<ul>
<li><a href="/wiki/Newton%27s_method_in_optimization" title="Newton's method in optimization">Newton's method</a>
<ul>
<li><a href="/wiki/Sequential_quadratic_programming" title="Sequential quadratic programming">Sequential quadratic programming</a>: A Newton-based method for small-medium scale <i>constrained</i> problems. Some versions can handle large-dimensional problems.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><a href="/wiki/Newton%27s_method_in_optimization" title="Newton's method in optimization">Newton's method</a>
<ul>
<li><a href="/wiki/Sequential_quadratic_programming" title="Sequential quadratic programming">Sequential quadratic programming</a>: A Newton-based method for small-medium scale <i>constrained</i> problems. Some versions can handle large-dimensional problems.</li>
</ul>
</li>
</ul>
<ul>
<li><a href="/wiki/Sequential_quadratic_programming" title="Sequential quadratic programming">Sequential quadratic programming</a>: A Newton-based method for small-medium scale <i>constrained</i> problems. Some versions can handle large-dimensional problems.</li>
</ul>
<ul>
<li>Methods that evaluate gradients or approximate gradients using finite differences (or even subgradients):
<ul>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton methods</a>: Iterative methods for medium-large problems (e.g. N&lt;1000).</li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient methods</a>: <a href="/wiki/Iterative_method" title="Iterative method">Iterative methods</a> for large problems. (In theory, these methods terminate in a finite number of steps with quadratic objective functions, but this finite termination is not observed in practice on finite–precision computers.)</li>
<li><a href="/wiki/Interior_point_methods" title="Interior point methods" class="mw-redirect">Interior point methods</a>: This is a large class of methods for constrained optimization. Some interior-point methods use only (sub)gradient information, and others of which require the evaluation of Hessians.</li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a> (alternatively, "steepest descent" or "steepest ascent"): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems.</li>
<li><a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient methods</a> - An iterative method for large <a href="/wiki/Rademacher%27s_theorem" title="Rademacher's theorem">locally</a> <a href="/wiki/Lipschitz_continuity" title="Lipschitz continuity">Lipschitz functions</a> using <a href="/wiki/Subgradient" title="Subgradient" class="mw-redirect">generalized gradients</a>. Following Boris T. Polyak, subgradient–projection methods are similar to conjugate–gradient methods.</li>
<li>Bundle method of descent: An iterative method for small–medium sized problems with locally Lipschitz functions, particularly for <a href="/wiki/Convex_optimization" title="Convex optimization">convex minimization</a> problems. (Similar to conjugate gradient methods)</li>
<li><a href="/wiki/Ellipsoid_method" title="Ellipsoid method">Ellipsoid method</a>: An iterative method for small problems with <a href="/wiki/Quasiconvex_function" title="Quasiconvex function">quasiconvex</a> objective functions and of great theoretical interest, particularly in establishing the polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.</li>
<li><a href="/wiki/Frank%E2%80%93Wolfe_algorithm" title="Frank–Wolfe algorithm">Reduced gradient method (Frank–Wolfe)</a> for approximate minimization of specially structured problems with <a href="/w/index.php?title=Linear_constraints&amp;action=edit&amp;redlink=1" class="new" title="Linear constraints (page does not exist)">linear constraints</a>, especially with traffic networks. For general unconstrained problems, this method reduces to the gradient method, which is regarded as obsolete (for almost all problems).</li>
<li><a href="/wiki/Simultaneous_perturbation_stochastic_approximation" title="Simultaneous perturbation stochastic approximation">Simultaneous perturbation stochastic approximation</a> (SPSA) method for stochastic optimization; uses random (efficient) gradient approximation.</li>
</ul>
</li>
</ul>
<ul>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton methods</a>: Iterative methods for medium-large problems (e.g. N&lt;1000).</li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient methods</a>: <a href="/wiki/Iterative_method" title="Iterative method">Iterative methods</a> for large problems. (In theory, these methods terminate in a finite number of steps with quadratic objective functions, but this finite termination is not observed in practice on finite–precision computers.)</li>
<li><a href="/wiki/Interior_point_methods" title="Interior point methods" class="mw-redirect">Interior point methods</a>: This is a large class of methods for constrained optimization. Some interior-point methods use only (sub)gradient information, and others of which require the evaluation of Hessians.</li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a> (alternatively, "steepest descent" or "steepest ascent"): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems.</li>
<li><a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient methods</a> - An iterative method for large <a href="/wiki/Rademacher%27s_theorem" title="Rademacher's theorem">locally</a> <a href="/wiki/Lipschitz_continuity" title="Lipschitz continuity">Lipschitz functions</a> using <a href="/wiki/Subgradient" title="Subgradient" class="mw-redirect">generalized gradients</a>. Following Boris T. Polyak, subgradient–projection methods are similar to conjugate–gradient methods.</li>
<li>Bundle method of descent: An iterative method for small–medium sized problems with locally Lipschitz functions, particularly for <a href="/wiki/Convex_optimization" title="Convex optimization">convex minimization</a> problems. (Similar to conjugate gradient methods)</li>
<li><a href="/wiki/Ellipsoid_method" title="Ellipsoid method">Ellipsoid method</a>: An iterative method for small problems with <a href="/wiki/Quasiconvex_function" title="Quasiconvex function">quasiconvex</a> objective functions and of great theoretical interest, particularly in establishing the polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.</li>
<li><a href="/wiki/Frank%E2%80%93Wolfe_algorithm" title="Frank–Wolfe algorithm">Reduced gradient method (Frank–Wolfe)</a> for approximate minimization of specially structured problems with <a href="/w/index.php?title=Linear_constraints&amp;action=edit&amp;redlink=1" class="new" title="Linear constraints (page does not exist)">linear constraints</a>, especially with traffic networks. For general unconstrained problems, this method reduces to the gradient method, which is regarded as obsolete (for almost all problems).</li>
<li><a href="/wiki/Simultaneous_perturbation_stochastic_approximation" title="Simultaneous perturbation stochastic approximation">Simultaneous perturbation stochastic approximation</a> (SPSA) method for stochastic optimization; uses random (efficient) gradient approximation.</li>
</ul>
<ul>
<li>Methods that evaluate only function values: If a problem is continuously differentiable, then gradients can be approximated using finite differences, in which case a gradient-based method can be used.
<ul>
<li><a href="/wiki/Interpolation" title="Interpolation">Interpolation</a> methods</li>
<li><a href="/wiki/Pattern_search_(optimization)" title="Pattern search (optimization)">Pattern search</a> methods, which have better convergence properties than the <a href="/wiki/Nelder%E2%80%93Mead_method" title="Nelder–Mead method">Nelder–Mead heuristic (with simplices)</a>, which is listed below.</li>
</ul>
</li>
</ul>
<ul>
<li><a href="/wiki/Interpolation" title="Interpolation">Interpolation</a> methods</li>
<li><a href="/wiki/Pattern_search_(optimization)" title="Pattern search (optimization)">Pattern search</a> methods, which have better convergence properties than the <a href="/wiki/Nelder%E2%80%93Mead_method" title="Nelder–Mead method">Nelder–Mead heuristic (with simplices)</a>, which is listed below.</li>
</ul>
<h4><span class="mw-headline" id="Global_convergence">Global convergence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=19" title="Edit section: Global convergence">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. The first and still popular method for ensuring convergence relies on <a href="/wiki/Line_search" title="Line search">line searches</a>, which optimize a function along one dimension. A second and increasingly popular method for ensuring convergence uses <a href="/wiki/Trust_region" title="Trust region">trust regions</a>. Both line searches and trust regions are used in modern methods of <a href="/wiki/Subgradient_method" title="Subgradient method">non-differentiable optimization</a>. Usually a global optimizer is much slower than advanced local optimizers (such as <a href="/wiki/BFGS_method" title="BFGS method" class="mw-redirect">BFGS</a>), so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points.</p>
<h3><span class="mw-headline" id="Heuristics">Heuristics</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=20" title="Edit section: Heuristics">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Besides (finitely terminating) <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> and (convergent) <a href="/wiki/Iterative_method" title="Iterative method">iterative methods</a>, there are <a href="/wiki/Heuristic_algorithm" title="Heuristic algorithm" class="mw-redirect">heuristics</a> that can provide approximate solutions to some optimization problems:</p>
<ul>
<li><a href="/wiki/Memetic_algorithm" title="Memetic algorithm">Memetic algorithm</a></li>
<li><a href="/wiki/Differential_evolution" title="Differential evolution">Differential evolution</a></li>
<li><a href="/wiki/Evolutionary_algorithms" title="Evolutionary algorithms" class="mw-redirect">Evolutionary algorithms</a></li>
<li><a href="/wiki/Dynamic_relaxation" title="Dynamic relaxation">Dynamic relaxation</a></li>
<li><a href="/wiki/Genetic_algorithms" title="Genetic algorithms" class="mw-redirect">Genetic algorithms</a></li>
<li><a href="/wiki/Hill_climbing" title="Hill climbing">Hill climbing</a> with random restart</li>
<li><a href="/wiki/Nelder-Mead_method" title="Nelder-Mead method" class="mw-redirect">Nelder-Mead simplicial heuristic</a>: A popular heuristic for approximate minimization (without calling gradients)</li>
<li><a href="/wiki/Particle_swarm_optimization" title="Particle swarm optimization">Particle swarm optimization</a></li>
<li><a href="/wiki/Artificial_bee_colony_optimization" title="Artificial bee colony optimization" class="mw-redirect">Artificial bee colony optimization</a></li>
<li><a href="/wiki/Simulated_annealing" title="Simulated annealing">Simulated annealing</a></li>
<li><a href="/wiki/Tabu_search" title="Tabu search">Tabu search</a></li>
<li><a href="/wiki/Reactive_Search_Optimization" title="Reactive Search Optimization" class="mw-redirect">Reactive Search Optimization (RSO)</a><sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup> implemented in <a href="/wiki/LIONsolver" title="LIONsolver">LIONsolver</a></li>
</ul>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=21" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Mechanics_and_engineering">Mechanics and engineering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=22" title="Edit section: Mechanics and engineering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Problems in <a href="/wiki/Rigid_body_dynamics" title="Rigid body dynamics">rigid body dynamics</a> (in particular articulated rigid body dynamics) often require mathematical programming techniques, since you can view rigid body dynamics as attempting to solve an <a href="/wiki/Ordinary_differential_equation" title="Ordinary differential equation">ordinary differential equation</a> on a constraint manifold; the constraints are various nonlinear geometric constraints such as "these two points must always coincide", "this surface must not penetrate any other", or "this point must always lie somewhere on this curve". Also, the problem of computing contact forces can be done by solving a <a href="/wiki/Linear_complementarity_problem" title="Linear complementarity problem">linear complementarity problem</a>, which can also be viewed as a QP (quadratic programming) problem.</p>
<p>Many design problems can also be expressed as optimization programs. This application is called design optimization. One subset is the <a href="/wiki/Engineering_optimization" title="Engineering optimization">engineering optimization</a>, and another recent and growing subset of this field is <a href="/wiki/Multidisciplinary_design_optimization" title="Multidisciplinary design optimization">multidisciplinary design optimization</a>, which, while useful in many problems, has in particular been applied to <a href="/wiki/Aerospace_engineering" title="Aerospace engineering">aerospace engineering</a> problems.</p>
<h3><span class="mw-headline" id="Economics">Economics</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=23" title="Edit section: Economics">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Economics" title="Economics">Economics</a> is closely enough linked to optimization of <a href="/wiki/Agent_(economics)" title="Agent (economics)">agents</a> that an influential definition relatedly describes economics <i>qua</i> science as the "study of human behavior as a relationship between ends and <a href="/wiki/Scarce" title="Scarce" class="mw-redirect">scarce</a> means" with alternative uses.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup> Modern optimization theory includes traditional optimization theory but also overlaps with <a href="/wiki/Game_theory" title="Game theory">game theory</a> and the study of economic <a href="/wiki/Equilibrium_(economics)" title="Equilibrium (economics)" class="mw-redirect">equilibria</a>. The <i><a href="/wiki/Journal_of_Economic_Literature" title="Journal of Economic Literature">Journal of Economic Literature</a></i> <a href="/wiki/JEL_classification_codes" title="JEL classification codes">codes</a> classify mathematical programming, optimization techniques, and related topics under <a href="/wiki/JEL_classification_codes#Mathematical_and_quantitative_methods_JEL:_C_Subcategories" title="JEL classification codes">JEL:C61-C63</a>.</p>
<p>In microeconomics, the <a href="/wiki/Utility_maximization_problem" title="Utility maximization problem">utility maximization problem</a> and its <a href="/wiki/Dual_problem" title="Dual problem" class="mw-redirect">dual problem</a>, the <a href="/wiki/Expenditure_minimization_problem" title="Expenditure minimization problem">expenditure minimization problem</a>, are economic optimization problems. Insofar as they behave consistently, <a href="/wiki/Consumer" title="Consumer">consumers</a> are assumed to maximize their <a href="/wiki/Utility" title="Utility">utility</a>, while <a href="/wiki/Firm" title="Firm" class="mw-redirect">firms</a> are usually assumed to maximize their <a href="/wiki/Profit_(economics)" title="Profit (economics)">profit</a>. Also, agents are often modeled as being <a href="/wiki/Risk_aversion" title="Risk aversion">risk-averse</a>, thereby preferring to avoid risk. <a href="/wiki/Asset_pricing" title="Asset pricing" class="mw-redirect">Asset prices</a> are also modeled using optimization theory, though the underlying mathematics relies on optimizing <a href="/wiki/Stochastic_process" title="Stochastic process">stochastic processes</a> rather than on static optimization. <a href="/wiki/Trade" title="Trade">Trade</a> theory also uses optimization to explain trade patterns between nations. The optimization of <a href="/wiki/Portfolio_(finance)" title="Portfolio (finance)">market portfolios</a> is an example of multi-objective optimization in economics.</p>
<p>Since the 1970s, economists have modeled dynamic decisions over time using <a href="/wiki/Control_theory" title="Control theory">control theory</a>. For example, microeconomists use <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic</a> <a href="/wiki/Search_theory" title="Search theory">search models</a> to study <a href="/wiki/Labor_economics" title="Labor economics" class="mw-redirect">labor-market behavior</a>.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup> A crucial distinction is between deterministic and stochastic models.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7"><span>[</span>7<span>]</span></a></sup> <a href="/wiki/Macroeconomics" title="Macroeconomics">Macroeconomists</a> build <a href="/wiki/Dynamic_stochastic_general_equilibrium" title="Dynamic stochastic general equilibrium">dynamic stochastic general equilibrium (DSGE)</a> models that describe the dynamics of the whole economy as the result of the interdependent optimizing decisions of workers, consumers, investors, and governments.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup><sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup></p>
<h3><span class="mw-headline" id="Operations_research">Operations research</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=24" title="Edit section: Operations research">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Another field that uses optimization techniques extensively is <a href="/wiki/Operations_research" title="Operations research">operations research</a>.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup> Operations research also uses stochastic modeling and simulation to support improved decision-making. Increasingly, operations research uses <a href="/wiki/Stochastic_programming" title="Stochastic programming">stochastic programming</a> to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and <a href="/wiki/Stochastic_optimization" title="Stochastic optimization">stochastic optimization</a> methods.</p>
<h3><span class="mw-headline" id="Control_engineering">Control engineering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=25" title="Edit section: Control engineering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Mathematical optimization is used in much modern controller design. High-level controllers such as <a href="/wiki/Model_predictive_control" title="Model predictive control">Model predictive control</a> (MPC) or Real-Time Optimization (RTO) employ mathematical optimization. These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical optimization problem including constraints and a model of the system to be controlled.</p>
<h3><span class="mw-headline" id="Petroleum_engineering">Petroleum engineering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=26" title="Edit section: Petroleum engineering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Nonlinear optimization methods are used to construct computational models of oil reservoirs.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span>[</span>11<span>]</span></a></sup></p>
<h3><span class="mw-headline" id="Molecular_modeling">Molecular modeling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=27" title="Edit section: Molecular modeling">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Nonlinear optimization methods are widely used in <a href="/wiki/Conformational_analysis" title="Conformational analysis" class="mw-redirect">conformational analysis</a>.</p>
<h2><span class="mw-headline" id="Solvers">Solvers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=28" title="Edit section: Solvers">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=29" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Brachistochrone" title="Brachistochrone" class="mw-redirect">Brachistochrone</a></li>
<li><a href="/wiki/Curve_fitting" title="Curve fitting">Curve fitting</a></li>
<li><a href="/wiki/Goal_programming" title="Goal programming">Goal programming</a></li>
<li><a href="/wiki/List_of_publications_in_mathematics#Optimization" title="List of publications in mathematics" class="mw-redirect">Important publications in optimization</a></li>
</ul>
<ul>
<li><a href="/wiki/Least_squares" title="Least squares">Least squares</a></li>
<li><a href="/wiki/Mathematical_Optimization_Society" title="Mathematical Optimization Society">Mathematical Optimization Society</a> (formerly Mathematical Programming Society)</li>
</ul>
<ul>
<li><a href="/wiki/Category:Optimization_algorithms_and_methods" title="Category:Optimization algorithms and methods">Mathematical optimization algorithms</a></li>
<li><a href="/wiki/Category:Mathematical_optimization_software" title="Category:Mathematical optimization software">Mathematical optimization software</a></li>
<li><a href="/wiki/Process_optimization" title="Process optimization">Process optimization</a></li>
<li><a href="/wiki/Variational_calculus" title="Variational calculus" class="mw-redirect">Variational calculus</a></li>
</ul>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=30" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">"<a rel="nofollow" class="external text" href="http://glossary.computing.society.informs.org/index.php?page=nature.html">The Nature of Mathematical Programming</a>," <i>Mathematical Programming Glossary</i>, INFORMS Computing Society.</span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">W. Erwin Diewert (2008). "cost functions," <i>The New Palgrave Dictionary of Economics</i>, 2nd Edition <a rel="nofollow" class="external text" href="http://www.dictionaryofeconomics.com/article?id=pde2008_C000390&amp;edition=current&amp;q=">Contents</a>.</span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><a href="/w/index.php?title=Peter_Kenneth_Newmand&amp;action=edit&amp;redlink=1" class="new" title="Peter Kenneth Newmand (page does not exist)">Peter Newman</a> (2008). "indirect utility function," <i>The New Palgrave Dictionary of Economics</i>, 2nd Edition. <a rel="nofollow" class="external text" href="http://www.dictionaryofeconomics.com/article?id=pde2008_I000066&amp;edition=">Contents.</a></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><span class="citation book">Battiti, Roberto; Mauro Brunato; Franco Mascia (2008). <a rel="nofollow" class="external text" href="http://reactive-search.org/thebook"><i>Reactive Search and Intelligent Optimization</i></a>. <a href="/wiki/Springer_Verlag" title="Springer Verlag" class="mw-redirect">Springer Verlag</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-387-09623-0" title="Special:BookSources/978-0-387-09623-0">978-0-387-09623-0</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Battiti%2C+Roberto&amp;rft.aufirst=Roberto&amp;rft.au=Franco+Mascia&amp;rft.aulast=Battiti&amp;rft.au=Mauro+Brunato&amp;rft.btitle=Reactive+Search+and+Intelligent+Optimization&amp;rft.date=2008&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Freactive-search.org%2Fthebook&amp;rft.isbn=978-0-387-09623-0&amp;rft.pub=Springer+Verlag&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><a href="/wiki/Lionel_Robbins" title="Lionel Robbins" class="mw-redirect">Lionel Robbins</a> (1935, 2nd ed.) <i><a href="/wiki/An_Essay_on_the_Nature_and_Significance_of_Economic_Science#Major_propositions" title="An Essay on the Nature and Significance of Economic Science">An Essay on the Nature and Significance of Economic Science</a></i>, Macmillan, p. 16.</span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><a href="/wiki/Avinash_Dixit" title="Avinash Dixit">A. K. Dixit</a> ([1976] 1990). <i>Optimization in Economic Theory</i>, 2nd ed., Oxford. <a rel="nofollow" class="external text" href="http://books.google.com/books?id=dHrsHz0VocUC&amp;pg=find&amp;pg=PA194=false#v=onepage&amp;q&amp;f=false">Description</a> and contents <a rel="nofollow" class="external text" href="http://books.google.com/books?id=dHrsHz0VocUC&amp;pg=PR7&amp;lpg=PR6&amp;dq=false&amp;lr=#v=onepage&amp;q=false&amp;f=false">preview</a>.</span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">A.G. Malliaris (2008). "stochastic optimal control," <i>The New Palgrave Dictionary of Economics</i>, 2nd Edition. <a rel="nofollow" class="external text" href="http://www.dictionaryofeconomics.com/article?id=pde2008_S000269&amp;edition=&amp;field=keyword&amp;q=Taylor's%20th&amp;topicid=&amp;result_number=1">Abstract</a>.</span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><a href="/wiki/Julio_Rotemberg" title="Julio Rotemberg">Julio Rotemberg</a> and <a href="/wiki/Michael_Woodford_(economist)" title="Michael Woodford (economist)">Michael Woodford</a> (1997), "An Optimization-based Econometric Framework for the Evaluation of Monetary Policy.<i>NBER Macroeconomics Annual</i>, 12, pp. <a rel="nofollow" class="external text" href="http://people.hbs.edu/jrotemberg/PublishedArticles/OptimizBasedEconometric_97.pdf">297-346.</a></span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">From <i><a href="/wiki/The_New_Palgrave_Dictionary_of_Economics" title="The New Palgrave Dictionary of Economics">The New Palgrave Dictionary of Economics</a></i> (2008), 2nd Edition with Abstract links:<br>
   • "<a rel="nofollow" class="external text" href="http://www.dictionaryofeconomics.com/article?id=pde2008_N000148&amp;edition=current&amp;q=optimization&amp;topicid=&amp;result_number=1">numerical optimization methods in economics</a>" by Karl Schmedders<br>
   • "<a rel="nofollow" class="external text" href="http://www.dictionaryofeconomics.com/article?id=pde2008_C000348&amp;edition=current&amp;q=optimization&amp;topicid=&amp;result_number=4">convex programming</a>" by <a href="/wiki/Lawrence_E._Blume" title="Lawrence E. Blume">Lawrence E. Blume</a><br>
   • "<a rel="nofollow" class="external text" href="http://www.dictionaryofeconomics.com/article?id=pde2008_A000133&amp;edition=current&amp;q=optimization&amp;topicid=&amp;result_number=20">Arrow–Debreu model of general equilibrium</a>" by <a href="/wiki/John_Geanakoplos" title="John Geanakoplos">John Geanakoplos</a>.</span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://www.seophonist-wahl.de/">"New force on the political scene: the Seophonisten"</a>. <a rel="nofollow" class="external free" href="http://www.seophonist-wahl.de">http://www.seophonist-wahl.de</a><span class="reference-accessdate">. Retrieved 14 September 2013</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.btitle=New+force+on+the+political+scene%3A+the+Seophonisten&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.seophonist-wahl.de%2F&amp;rft.pub=http%3A%2F%2Fwww.seophonist-wahl.de&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm, Journal of Petroleum Science and Engineering, <a rel="nofollow" class="external free" href="http://www.sciencedirect.com/science/article/pii/S0920410513003227">http://www.sciencedirect.com/science/article/pii/S0920410513003227</a></span></li>
</ol>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=31" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Comprehensive">Comprehensive</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=32" title="Edit section: Comprehensive">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Undergraduate_level">Undergraduate level</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=33" title="Edit section: Undergraduate level">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul>
<li><span class="citation book">Bradley, S.; Hax, A.; <a href="/wiki/Thomas_Magnanti" title="Thomas Magnanti" class="mw-redirect">Magnanti, T.</a> (1977). <i>Applied mathematical programming</i>. Addison Wesley.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Bradley%2C+S.&amp;rft.aufirst=S.&amp;rft.au=Hax%2C+A.&amp;rft.aulast=Bradley&amp;rft.au=Magnanti%2C+T.&amp;rft.btitle=Applied+mathematical+programming&amp;rft.date=1977&amp;rft.genre=book&amp;rft.pub=Addison+Wesley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Rardin, Ronald L. (1997). <i>Optimization in operations research</i>. Prentice Hall. p. 919. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-02-398415-5" title="Special:BookSources/0-02-398415-5">0-02-398415-5</a>. "copyright: 1998"</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=Ronald+L.&amp;rft.aulast=Rardin&amp;rft.au=Rardin%2C+Ronald+L.&amp;rft.btitle=Optimization+in+operations+research&amp;rft.date=1997&amp;rft.genre=book&amp;rft.isbn=0-02-398415-5&amp;rft.pages=919&amp;rft.pub=Prentice+Hall&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span id="CITEREFStrang1986" class="citation book"><a href="/wiki/Gilbert_Strang" title="Gilbert Strang">Strang, Gilbert</a> (1986). <a rel="nofollow" class="external text" href="http://www.wellesleycambridge.com/tocs/toc-appl"><i>Introduction to applied mathematics</i></a>. Wellesley, MA: Wellesley-Cambridge Press (Strang's publishing company). pp. xii+758. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-9614088-0-4" title="Special:BookSources/0-9614088-0-4">0-9614088-0-4</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=870634">870634</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=Gilbert&amp;rft.aulast=Strang&amp;rft.au=Strang%2C+Gilbert&amp;rft.btitle=Introduction+to+applied+mathematics&amp;rft.date=1986&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.wellesleycambridge.com%2Ftocs%2Ftoc-appl&amp;rft.isbn=0-9614088-0-4&amp;rft.mr=870634&amp;rft.pages=xii%2B758&amp;rft.place=Wellesley%2C+MA&amp;rft.pub=Wellesley-Cambridge+Press+%28Strang%27s+publishing+company%29&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<h4><span class="mw-headline" id="Graduate_level">Graduate level</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=34" title="Edit section: Graduate level">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul>
<li><span id="CITEREFMagnanti1989" class="citation book"><a href="/wiki/Thomas_L._Magnanti" title="Thomas L. Magnanti">Magnanti, Thomas L.</a> (1989). "Twenty years of mathematical programming". In Cornet, Bernard; Tulkens, Henry. <i>Contributions to Operations Research and Economics: The twentieth anniversary of CORE (Papers from the symposium held in Louvain-la-Neuve, January 1987)</i>. Cambridge, MA: MIT Press. pp. 163–227. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-262-03149-3" title="Special:BookSources/0-262-03149-3">0-262-03149-3</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1104662">1104662</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.atitle=Contributions+to+Operations%26nbsp%3BResearch+and+Economics%3A+The+twentieth+anniversary+of+CORE+%28Papers+from+the+symposium+held+in+Louvain-la-Neuve%2C+January%26nbsp%3B1987%29&amp;rft.aufirst=Thomas+L.&amp;rft.aulast=Magnanti&amp;rft.au=Magnanti%2C+Thomas+L.&amp;rft.btitle=Twenty+years+of+mathematical+programming&amp;rft.date=1989&amp;rft.genre=bookitem&amp;rft.isbn=0-262-03149-3&amp;rft.mr=1104662&amp;rft.pages=163-227&amp;rft.place=Cambridge%2C%26nbsp%3BMA&amp;rft.pub=MIT+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><span id="CITEREFMinoux1986" class="citation book"><a href="/w/index.php?title=Michel_Minoux&amp;action=edit&amp;redlink=1" class="new" title="Michel Minoux (page does not exist)">Minoux, M.</a> (1986). <i>Mathematical programming: Theory and algorithms</i>. Egon Balas foreword) (Translated by Steven Vajda from the (1983 Paris: Dunod) French ed.). Chichester: A Wiley-Interscience Publication. John Wiley &amp; Sons, Ltd. pp. xxviii+489. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-471-90170-9" title="Special:BookSources/0-471-90170-9">0-471-90170-9</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=2571910">2571910</a>. (2008 Second ed., in French: <i>Programmation mathématique: Théorie et algorithmes</i>. Editions Tec &amp; Doc, Paris, 2008. xxx+711 pp. <a href="/wiki/Special:BookSources/9782743010003" class="internal mw-magiclink-isbn">ISBN 978-2-7430-1000-3</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=M.&amp;rft.aulast=Minoux&amp;rft.au=Minoux%2C+M.&amp;rft.btitle=Mathematical+programming%3A+Theory+and+algorithms&amp;rft.date=1986&amp;rft.edition=Translated++by+Steven+Vajda+from+the+%281983+Paris%3A+Dunod%29+French&amp;rft.genre=book&amp;rft.isbn=0-471-90170-9&amp;rft.mr=2571910&amp;rft.pages=xxviii%2B489&amp;rft.place=Chichester&amp;rft.pub=A+Wiley-Interscience+Publication.+John+Wiley+%26+Sons%2C+Ltd.&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<ul>
<li><span id="CITEREFNemhauserRinnooy_KanTodd1989" class="citation book"><a href="/wiki/George_L._Nemhauser" title="George L. Nemhauser" class="mw-redirect">Nemhauser, G. L.</a>; Rinnooy Kan, A. H. G.; <a href="/w/index.php?title=Michael_J._Todd_(mathematician)&amp;action=edit&amp;redlink=1" class="new" title="Michael J. Todd (mathematician) (page does not exist)">Todd, M. J.</a>, eds. (1989). <i>Optimization</i>. Handbooks in Operations Research and Management Science <b>1</b>. Amsterdam: North-Holland Publishing Co. pp. xiv+709. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-444-87284-1" title="Special:BookSources/0-444-87284-1">0-444-87284-1</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1105099">1105099</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.btitle=Optimization&amp;rft.date=1989&amp;rft.genre=book&amp;rft.isbn=0-444-87284-1&amp;rft.mr=1105099&amp;rft.pages=xiv%2B709&amp;rft.place=Amsterdam&amp;rft.pub=North-Holland+Publishing+Co.&amp;rft.series=Handbooks+in+Operations+Research+and+Management+Science&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.volume=1" class="Z3988"><span style="display:none;"> </span></span>
<ul>
<li><a href="/w/index.php?title=J._E._Dennis,_Jr.&amp;action=edit&amp;redlink=1" class="new" title="J. E. Dennis, Jr. (page does not exist)">J. E. Dennis, Jr.</a> and <a href="/w/index.php?title=Robert_B._Schnabel&amp;action=edit&amp;redlink=1" class="new" title="Robert B. Schnabel (page does not exist)">Robert B. Schnabel</a>, A view of unconstrained optimization (pp. 1–72);</li>
<li><a href="/w/index.php?title=Donald_Goldfarb&amp;action=edit&amp;redlink=1" class="new" title="Donald Goldfarb (page does not exist)">Donald Goldfarb</a> and <a href="/w/index.php?title=Michael_J._Todd_(mathematician)&amp;action=edit&amp;redlink=1" class="new" title="Michael J. Todd (mathematician) (page does not exist)">Michael J. Todd</a>, Linear programming (pp. 73–170);</li>
<li>Philip E. Gill, Walter Murray, Michael A. Saunders, and <a href="/wiki/Margaret_H._Wright" title="Margaret H. Wright">Margaret H. Wright</a>, Constrained nonlinear programming (pp. 171–210);</li>
<li><a href="/wiki/Ravindra_K._Ahuja" title="Ravindra K. Ahuja">Ravindra K. Ahuja</a>, <a href="/wiki/Thomas_L._Magnanti" title="Thomas L. Magnanti">Thomas L. Magnanti</a>, and <a href="/wiki/James_B._Orlin" title="James B. Orlin">James B. Orlin</a>, Network flows (pp. 211–369);</li>
<li><a href="/w/index.php?title=W._R._Pulleyblank&amp;action=edit&amp;redlink=1" class="new" title="W. R. Pulleyblank (page does not exist)">W. R. Pulleyblank</a>, Polyhedral combinatorics (pp. 371–446);</li>
<li>George L. Nemhauser and Laurence A. Wolsey, Integer programming (pp. 447–527);</li>
<li><a href="/wiki/Claude_Lemar%C3%A9chal" title="Claude Lemaréchal">Claude Lemaréchal</a>, Nondifferentiable optimization (pp. 529–572);</li>
<li><a href="/wiki/Roger_J-B_Wets" title="Roger J-B Wets">Roger J-B Wets</a>, Stochastic programming (pp. 573–629);</li>
<li>A. H. G. Rinnooy Kan and G. T. Timmer, Global optimization (pp. 631–662);</li>
<li>P. L. Yu, Multiple criteria decision making: five basic concepts (pp. 663–699).</li>
</ul>
</li>
</ul>
<ul>
<li><a href="/w/index.php?title=J._E._Dennis,_Jr.&amp;action=edit&amp;redlink=1" class="new" title="J. E. Dennis, Jr. (page does not exist)">J. E. Dennis, Jr.</a> and <a href="/w/index.php?title=Robert_B._Schnabel&amp;action=edit&amp;redlink=1" class="new" title="Robert B. Schnabel (page does not exist)">Robert B. Schnabel</a>, A view of unconstrained optimization (pp. 1–72);</li>
<li><a href="/w/index.php?title=Donald_Goldfarb&amp;action=edit&amp;redlink=1" class="new" title="Donald Goldfarb (page does not exist)">Donald Goldfarb</a> and <a href="/w/index.php?title=Michael_J._Todd_(mathematician)&amp;action=edit&amp;redlink=1" class="new" title="Michael J. Todd (mathematician) (page does not exist)">Michael J. Todd</a>, Linear programming (pp. 73–170);</li>
<li>Philip E. Gill, Walter Murray, Michael A. Saunders, and <a href="/wiki/Margaret_H._Wright" title="Margaret H. Wright">Margaret H. Wright</a>, Constrained nonlinear programming (pp. 171–210);</li>
<li><a href="/wiki/Ravindra_K._Ahuja" title="Ravindra K. Ahuja">Ravindra K. Ahuja</a>, <a href="/wiki/Thomas_L._Magnanti" title="Thomas L. Magnanti">Thomas L. Magnanti</a>, and <a href="/wiki/James_B._Orlin" title="James B. Orlin">James B. Orlin</a>, Network flows (pp. 211–369);</li>
<li><a href="/w/index.php?title=W._R._Pulleyblank&amp;action=edit&amp;redlink=1" class="new" title="W. R. Pulleyblank (page does not exist)">W. R. Pulleyblank</a>, Polyhedral combinatorics (pp. 371–446);</li>
<li>George L. Nemhauser and Laurence A. Wolsey, Integer programming (pp. 447–527);</li>
<li><a href="/wiki/Claude_Lemar%C3%A9chal" title="Claude Lemaréchal">Claude Lemaréchal</a>, Nondifferentiable optimization (pp. 529–572);</li>
<li><a href="/wiki/Roger_J-B_Wets" title="Roger J-B Wets">Roger J-B Wets</a>, Stochastic programming (pp. 573–629);</li>
<li>A. H. G. Rinnooy Kan and G. T. Timmer, Global optimization (pp. 631–662);</li>
<li>P. L. Yu, Multiple criteria decision making: five basic concepts (pp. 663–699).</li>
</ul>
<ul>
<li><span id="CITEREFShapiro1979" class="citation book">Shapiro, Jeremy F. (1979). <i>Mathematical programming: Structures and algorithms</i>. New York: Wiley-Interscience [John Wiley &amp; Sons]. pp. xvi+388. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-471-77886-9" title="Special:BookSources/0-471-77886-9">0-471-77886-9</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=544669">544669</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=Jeremy+F.&amp;rft.aulast=Shapiro&amp;rft.au=Shapiro%2C+Jeremy+F.&amp;rft.btitle=Mathematical+programming%3A+Structures+and+algorithms&amp;rft.date=1979&amp;rft.genre=book&amp;rft.isbn=0-471-77886-9&amp;rft.mr=544669&amp;rft.pages=xvi%2B388&amp;rft.place=New+York&amp;rft.pub=Wiley-Interscience+%5BJohn+Wiley+%26+Sons%5D&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li>Spall, J. C. (2003), <i>Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control</i>, Wiley, Hoboken, NJ.</li>
</ul>
<h3><span class="mw-headline" id="Continuous_optimization">Continuous optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=35" title="Edit section: Continuous optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><span class="citation book">Roger Fletcher (2000). <i>Practical methods of optimization</i>. Wiley. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-471-49463-8" title="Special:BookSources/978-0-471-49463-8">978-0-471-49463-8</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aulast=Roger+Fletcher&amp;rft.au=Roger+Fletcher&amp;rft.btitle=Practical+methods+of+optimization&amp;rft.date=2000&amp;rft.genre=book&amp;rft.isbn=978-0-471-49463-8&amp;rft.pub=Wiley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Mordecai Avriel (2003). <i>Nonlinear Programming: Analysis and Methods</i>. Dover Publishing. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-486-43227-0" title="Special:BookSources/0-486-43227-0">0-486-43227-0</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aulast=Mordecai+Avriel&amp;rft.au=Mordecai+Avriel&amp;rft.btitle=Nonlinear+Programming%3A+Analysis+and+Methods&amp;rft.date=2003&amp;rft.genre=book&amp;rft.isbn=0-486-43227-0&amp;rft.pub=Dover+Publishing&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">P. E. Gill, W. Murray and M. H. Wright (1982). <i>Practical Optimization</i>. Emerald Publishing. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0122839528" title="Special:BookSources/978-0122839528">978-0122839528</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aulast=P.+E.+Gill%2C+W.+Murray+and+M.+H.+Wright&amp;rft.au=P.+E.+Gill%2C+W.+Murray+and+M.+H.+Wright&amp;rft.btitle=Practical+Optimization&amp;rft.date=1982&amp;rft.genre=book&amp;rft.isbn=978-0122839528&amp;rft.pub=Emerald+Publishing&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Xin-She Yang (2010). <i>Engineering Optimization: An Introduction with Metaheuristic Applications</i>. Wiley. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0470582466" title="Special:BookSources/978-0470582466">978-0470582466</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aulast=Xin-She+Yang&amp;rft.au=Xin-She+Yang&amp;rft.btitle=Engineering+Optimization%3A+An+Introduction+with+Metaheuristic+Applications&amp;rft.date=2010&amp;rft.genre=book&amp;rft.isbn=978-0470582466&amp;rft.pub=Wiley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Bonnans, J. Frédéric; Gilbert, J. Charles; Lemaréchal, Claude; Sagastizábal, Claudia A. (2006). <a rel="nofollow" class="external text" href="http://www.springer.com/mathematics/applications/book/978-3-540-35445-1"><i>Numerical optimization: Theoretical and practical aspects</i></a>. Universitext (Second revised ed. of translation of 1997 French ed.). Berlin: Springer-Verlag. pp. xiv+490. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2F978-3-540-35447-5">10.1007/978-3-540-35447-5</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/3-540-35445-X" title="Special:BookSources/3-540-35445-X">3-540-35445-X</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=2265882">2265882</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Bonnans%2C+J.%26nbsp%3BFr%C3%A9d%C3%A9ric&amp;rft.aufirst=J.%26nbsp%3BFr%C3%A9d%C3%A9ric&amp;rft.au=Gilbert%2C+J.%26nbsp%3BCharles&amp;rft.aulast=Bonnans&amp;rft.au=Lemar%C3%A9chal%2C+Claude&amp;rft.au=Sagastiz%C3%A1bal%2C+Claudia%26nbsp%3BA.&amp;rft.btitle=Numerical+optimization%3A+Theoretical+and+practical+aspects&amp;rft.date=2006&amp;rft.edition=Second+revised+ed.+of++translation+of+1997++French&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.springer.com%2Fmathematics%2Fapplications%2Fbook%2F978-3-540-35445-1&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-540-35447-5&amp;rft.isbn=3-540-35445-X&amp;rft.mr=2265882&amp;rft.pages=xiv%2B490&amp;rft.place=Berlin&amp;rft.pub=Springer-Verlag&amp;rft.series=Universitext&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Bonnans, J. Frédéric; Shapiro, Alexander (2000). <i>Perturbation analysis of optimization problems</i>. Springer Series in Operations Research. New York: Springer-Verlag. pp. xviii+601. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-387-98705-3" title="Special:BookSources/0-387-98705-3">0-387-98705-3</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=1756264">1756264</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Bonnans%2C+J.%26nbsp%3BFr%C3%A9d%C3%A9ric&amp;rft.aufirst=J.%26nbsp%3BFr%C3%A9d%C3%A9ric&amp;rft.aulast=Bonnans&amp;rft.au=Shapiro%2C+Alexander&amp;rft.btitle=Perturbation+analysis+of+optimization+problems&amp;rft.date=2000&amp;rft.genre=book&amp;rft.isbn=0-387-98705-3&amp;rft.mr=1756264&amp;rft.pages=xviii%2B601&amp;rft.place=New+York&amp;rft.pub=Springer-Verlag&amp;rft.series=Springer+Series+in+Operations+Research&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Boyd, Stephen P.; Vandenberghe, Lieven (2004). <a rel="nofollow" class="external text" href="http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"><i>Convex Optimization</i></a> (pdf). Cambridge University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-521-83378-3" title="Special:BookSources/978-0-521-83378-3">978-0-521-83378-3</a><span class="reference-accessdate">. Retrieved October 15, 2011</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Boyd%2C+Stephen+P.&amp;rft.aufirst=Stephen+P.&amp;rft.aulast=Boyd&amp;rft.au=Vandenberghe%2C+Lieven&amp;rft.btitle=Convex+Optimization&amp;rft.date=2004&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.stanford.edu%2F~boyd%2Fcvxbook%2Fbv_cvxbook.pdf&amp;rft.isbn=978-0-521-83378-3&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book">Jorge Nocedal and Stephen J. Wright (2006). <a rel="nofollow" class="external text" href="http://www.ece.northwestern.edu/~nocedal/book/num-opt.html"><i>Numerical Optimization</i></a>. Springer. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-387-30303-0" title="Special:BookSources/0-387-30303-0">0-387-30303-0</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Jorge+Nocedal+and+Stephen+J.+Wright&amp;rft.aulast=Jorge+Nocedal+and+Stephen+J.+Wright&amp;rft.btitle=Numerical+Optimization&amp;rft.date=2006&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.ece.northwestern.edu%2F~nocedal%2Fbook%2Fnum-opt.html&amp;rft.isbn=0-387-30303-0&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book"><a href="/wiki/Andrzej_Piotr_Ruszczy%C5%84ski" title="Andrzej Piotr Ruszczyński">Ruszczyński</a>, Andrzej (2006). <i>Nonlinear Optimization</i>. Princeton, NJ: <a href="/wiki/Princeton_University_Press" title="Princeton University Press">Princeton University Press</a>. pp. xii+454. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0691119151" title="Special:BookSources/978-0691119151">978-0691119151</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=2199043">2199043</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=Andrzej&amp;rft.aulast=Ruszczy%C5%84ski&amp;rft.au=Ruszczy%C5%84ski%2C+Andrzej&amp;rft.btitle=Nonlinear+Optimization&amp;rft.date=2006&amp;rft.genre=book&amp;rft.isbn=978-0691119151&amp;rft.mr=2199043&amp;rft.pages=xii%2B454&amp;rft.place=Princeton%2C+NJ&amp;rft.pub=Princeton+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
</ul>
<h3><span class="mw-headline" id="Combinatorial_optimization">Combinatorial optimization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=36" title="Edit section: Combinatorial optimization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li>R. K. Ahuja, <a href="/wiki/Thomas_L._Magnanti" title="Thomas L. Magnanti">Thomas L. Magnanti</a>, and <a href="/wiki/James_B._Orlin" title="James B. Orlin">James B. Orlin</a> (1993). <i>Network Flows: Theory, Algorithms, and Applications</i>. Prentice-Hall, Inc. <a href="/wiki/Special:BookSources/013617549X" class="internal mw-magiclink-isbn">ISBN 0-13-617549-X</a>.</li>
<li><a href="/wiki/William_J._Cook" title="William J. Cook">William J. Cook</a>, William H. Cunningham, William R. Pulleyblank, <a href="/wiki/Alexander_Schrijver" title="Alexander Schrijver">Alexander Schrijver</a>; <i>Combinatorial Optimization</i>; John Wiley &amp; Sons; 1 edition (November 12, 1997); <a href="/wiki/Special:BookSources/047155894X" class="internal mw-magiclink-isbn">ISBN 0-471-55894-X</a>.</li>
<li><span id="CITEREFGondranMinoux1984" class="citation book">Gondran, Michel; Minoux, Michel (1984). <i>Graphs and algorithms</i>. Wiley-Interscience Series in Discrete Mathematics (Translated by Steven Vajda from the second (<i>Collection de la Direction des Études et Recherches d'Électricité de France</i> [Collection of the Department of Studies and Research of Électricité de France], v. 37. Paris: Éditions Eyrolles 1985. xxviii+545 pp. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="http://www.ams.org/mathscinet-getitem?mr=868083">868083</a>) French ed.). Chichester: John Wiley &amp; Sons, Ltd. pp. xix+650. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-2-7430-1035-5" title="Special:BookSources/978-2-7430-1035-5">978-2-7430-1035-5</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=2552933">2552933</a>. (Fourth ed. Collection EDF R&amp;D. Paris: Editions Tec &amp; Doc 2009. xxxii+784 pp.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=Michel&amp;rft.au=Gondran%2C+Michel&amp;rft.aulast=Gondran&amp;rft.au=Minoux%2C+Michel&amp;rft.btitle=Graphs+and+algorithms&amp;rft.date=1984&amp;rft.edition=Translated+by+Steven+Vajda+from+the+second+%28%27%27Collection+de+la+Direction+des+%C3%89tudes+et+Recherches+d%27%C3%89lectricit%C3%A9+de+France%27%27+%5BCollection+of+the+Department+of+Studies+and+Research+of+%C3%89lectricit%C3%A9+de+France%5D%2C+v.+37.+Paris%3A+%C3%89ditions+Eyrolles+1985.+xxviii%2B545+pp.+MR%26nbsp%3B%5Bhttp%3A%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D868083+868083%5D%29+French&amp;rft.genre=book&amp;rft.isbn=978-2-7430-1035-5&amp;rft.mr=2552933&amp;rft.pages=xix%2B650&amp;rft.place=Chichester&amp;rft.pub=John+Wiley+%26+Sons%2C+Ltd.&amp;rft.series=Wiley-Interscience+Series+in+Discrete+Mathematics&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span class="citation book"><a href="/wiki/Eugene_Lawler" title="Eugene Lawler">Eugene Lawler</a> (2001). <i>Combinatorial Optimization: Networks and Matroids</i>. Dover. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-486-41453-1" title="Special:BookSources/0-486-41453-1">0-486-41453-1</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.au=Eugene+Lawler&amp;rft.aulast=Eugene+Lawler&amp;rft.btitle=Combinatorial+Optimization%3A+Networks+and+Matroids&amp;rft.date=2001&amp;rft.genre=book&amp;rft.isbn=0-486-41453-1&amp;rft.pub=Dover&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span></li>
<li><span id="CITEREFLawlerLenstraRinnooy_KanShmoys1985" class="citation"><a href="/wiki/Eugene_Lawler" title="Eugene Lawler">Lawler, E. L.</a>; <a href="/wiki/Jan_Karel_Lenstra" title="Jan Karel Lenstra">Lenstra, J. K.</a>; Rinnooy Kan, A. H. G.; Shmoys, D. B. (1985), <i>The traveling salesman problem: A guided tour of combinatorial optimization</i>, John Wiley &amp; Sons, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-471-90413-9" title="Special:BookSources/0-471-90413-9">0-471-90413-9</a></span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMathematical+optimization&amp;rft.aufirst=E.+L.&amp;rft.aulast=Lawler&amp;rft.au=Lawler%2C+E.+L.&amp;rft.au=Lenstra%2C+J.+K.&amp;rft.au=Rinnooy+Kan%2C+A.+H.+G.&amp;rft.au=Shmoys%2C+D.+B.&amp;rft.btitle=The+traveling+salesman+problem%3A+A+guided+tour+of+combinatorial+optimization&amp;rft.date=1985&amp;rft.genre=book&amp;rft.isbn=0-471-90413-9&amp;rft.pub=John+Wiley+%26+Sons&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;"> </span></span>.</li>
<li><a href="/wiki/Jon_Lee_(mathematician)" title="Jon Lee (mathematician)">Jon Lee</a>; <i><a rel="nofollow" class="external text" href="http://books.google.com/books?id=3pL1B7WVYnAC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false">A First Course in Combinatorial Optimization</a></i>; Cambridge University Press; 2004; <a href="/wiki/Special:BookSources/0521010128" class="internal mw-magiclink-isbn">ISBN 0-521-01012-8</a>.</li>
<li>Christos H. Papadimitriou and <a href="/wiki/Kenneth_Steiglitz" title="Kenneth Steiglitz">Kenneth Steiglitz</a> <i>Combinatorial Optimization : Algorithms and Complexity</i>; Dover Pubns; (paperback, Unabridged edition, July 1998) <a href="/wiki/Special:BookSources/0486402584" class="internal mw-magiclink-isbn">ISBN 0-486-40258-4</a>.</li>
</ul>
<h2><span class="mw-headline" id="Journals">Journals</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=37" title="Edit section: Journals">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a rel="nofollow" class="external text" href="http://www.springer.com/mathematics/journal/10589"><i>Computational Optimization and Applications</i></a></li>
<li><a rel="nofollow" class="external text" href="https://www.novapublishers.com/catalog/product_info.php?products_id=6353">Journal of Computational <i>Optimization in Economics and Finance</i></a></li>
<li><a rel="nofollow" class="external text" href="http://www.journals.elsevier.com/journal-of-economic-dynamics-and-control/"><i>Journal of Economic Dynamics and Control</i></a></li>
<li><a rel="nofollow" class="external text" href="http://www.siam.org/journals/siopt.php"><i>SIAM Journal on Optimization</i> (SIOPT)</a> and <a rel="nofollow" class="external text" href="http://www.siam.org/journals/siopt/policy.php">Editorial Policy</a></li>
<li><a rel="nofollow" class="external text" href="http://www.siam.org/journals/sicon.php"><i>SIAM Journal on Control and Optimization</i> (SICON)</a> and <a rel="nofollow" class="external text" href="http://www.siam.org/journals/sicon/policy.php">Editorial Policy</a></li>
</ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mathematical_optimization&amp;action=edit&amp;section=38" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a rel="nofollow" class="external text" href="http://www.coin-or.org/">COIN-OR</a>—Computational Infrastructure for Operations Research</li>
<li><a rel="nofollow" class="external text" href="http://plato.asu.edu/guide.html">Decision Tree for Optimization Software</a> Links to optimization source codes</li>
<li><a rel="nofollow" class="external text" href="http://www.mat.univie.ac.at/%7Eneum/glopt.html">Global optimization</a></li>
<li><a rel="nofollow" class="external text" href="http://glossary.computing.society.informs.org/">Mathematical Programming Glossary</a></li>
<li><a rel="nofollow" class="external text" href="http://www.mathprog.org/">Mathematical Programming Society</a></li>
<li><a rel="nofollow" class="external text" href="http://www-fp.mcs.anl.gov/otc/Guide/index.html">NEOS Guide</a> currently being replaced by the <a rel="nofollow" class="external text" href="http://wiki.mcs.anl.gov/neos">NEOS Wiki</a></li>
<li><a rel="nofollow" class="external text" href="http://www.optimization-online.org">Optimization Online</a> A repository for optimization e-prints</li>
<li><a rel="nofollow" class="external text" href="http://www2.arnes.si/%7Eljc3m2/igor/links.html">Optimization Related Links</a></li>
<li><a rel="nofollow" class="external text" href="http://see.stanford.edu/see/courseinfo.aspx?coll=2db7ced4-39d1-4fdb-90e8-364129597c87">Convex Optimization I</a> EE364a: Course from Stanford University</li>
<li><a rel="nofollow" class="external text" href="http://www.stanford.edu/~boyd/cvxbook">Convex Optimization – Boyd and Vandenberghe</a> Book on Convex Optimization</li>
<li><a rel="nofollow" class="external text" href="http://apmonitor.com/me575/index.php/Main/BookChapters">Book and Course</a> on Optimization Methods for Engineering Design</li>
</ul>
<ul>
<li class="nv-view"><a href="/wiki/Template:Optimization_algorithms" title="Template:Optimization algorithms"><span title="View this template" style=";;background:none transparent;border:none;;">v</span></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Optimization_algorithms" title="Template talk:Optimization algorithms"><span title="Discuss this template" style=";;background:none transparent;border:none;;">t</span></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Optimization_algorithms&amp;action=edit"><span title="Edit this template" style=";;background:none transparent;border:none;;">e</span></a></li>
</ul>
<ul>
<li><a href="/wiki/Golden_section_search" title="Golden section search">Golden section search</a></li>
<li><a href="/wiki/Powell%27s_method" title="Powell's method">Interpolation methods</a></li>
<li><a href="/wiki/Line_search" title="Line search">Line search</a></li>
<li><a href="/wiki/Nelder%E2%80%93Mead_method" title="Nelder–Mead method">Nelder–Mead method</a></li>
<li><a href="/wiki/Successive_parabolic_interpolation" title="Successive parabolic interpolation">Successive parabolic interpolation</a></li>
</ul>
<ul>
<li><a href="/wiki/Trust_region" title="Trust region">Trust region</a></li>
<li><a href="/wiki/Wolfe_conditions" title="Wolfe conditions">Wolfe conditions</a></li>
</ul>
<ul>
<li><a href="/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" title="Broyden–Fletcher–Goldfarb–Shanno algorithm">BFGS</a> and <a href="/wiki/Limited-memory_BFGS" title="Limited-memory BFGS">L-BFGS</a></li>
<li><a href="/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula" title="Davidon–Fletcher–Powell formula">DFP</a></li>
<li><a href="/wiki/SR1_formula" title="SR1 formula" class="mw-redirect">Symmetric rank-one (SR1)</a></li>
</ul>
<ul>
<li><a href="/wiki/Gauss%E2%80%93Newton_algorithm" title="Gauss–Newton algorithm">Gauss–Newton</a></li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient</a></li>
<li><a href="/wiki/Levenberg%E2%80%93Marquardt_algorithm" title="Levenberg–Marquardt algorithm">Levenberg–Marquardt</a></li>
<li><a href="/wiki/Nonlinear_conjugate_gradient_method" title="Nonlinear conjugate gradient method">Conjugate gradient</a></li>
</ul>
<ul>
<li><a href="/wiki/Newton%27s_method_in_optimization" title="Newton's method in optimization">Newton's method</a></li>
</ul>
<ul>
<li><a href="/wiki/Barrier_function" title="Barrier function">Barrier methods</a></li>
<li><a href="/wiki/Penalty_method" title="Penalty method">Penalty methods</a></li>
</ul>
<ul>
<li><a href="/wiki/Augmented_Lagrangian_method" title="Augmented Lagrangian method">Augmented Lagrangian methods</a></li>
<li><a href="/wiki/Sequential_quadratic_programming" title="Sequential quadratic programming">Sequential quadratic programming</a></li>
<li><a href="/wiki/Successive_linear_programming" title="Successive linear programming">Successive linear programming</a></li>
</ul>
<ul>
<li><a href="/wiki/Cutting-plane_method" title="Cutting-plane method">Cutting-plane method</a></li>
<li><a href="/wiki/Frank%E2%80%93Wolfe_algorithm" title="Frank–Wolfe algorithm">Reduced gradient (Frank–Wolfe)</a></li>
<li><a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient method</a></li>
</ul>
<ul>
<li><a href="/wiki/Ellipsoid_method" title="Ellipsoid method">Ellipsoid algorithm of Khachiyan</a></li>
<li><a href="/wiki/Karmarkar%27s_algorithm" title="Karmarkar's algorithm">Projective algorithm of Karmarkar</a></li>
</ul>
<ul>
<li><a href="/wiki/Simplex_algorithm" title="Simplex algorithm">Simplex algorithm of Dantzig</a></li>
<li><a href="/wiki/Revised_simplex_algorithm" title="Revised simplex algorithm" class="mw-redirect">Revised simplex algorithm</a></li>
<li><a href="/wiki/Criss-cross_algorithm" title="Criss-cross algorithm">Criss-cross algorithm</a></li>
<li><a href="/wiki/Lemke%27s_algorithm" title="Lemke's algorithm">Principal pivoting algorithm of Lemke</a></li>
</ul>
<ul>
<li><a href="/wiki/Approximation_algorithm" title="Approximation algorithm">Approximation algorithm</a></li>
<li><a href="/wiki/Dynamic_programming" title="Dynamic programming">Dynamic programming</a></li>
<li><a href="/wiki/Greedy_algorithm" title="Greedy algorithm">Greedy algorithm</a></li>
<li><a href="/wiki/Integer_programming" title="Integer programming">Integer programming</a>
<ul>
<li><a href="/wiki/Branch_and_bound" title="Branch and bound">Branch &amp; bound</a> or <a href="/wiki/Branch_and_cut" title="Branch and cut">cut</a></li>
</ul>
</li>
</ul>
<ul>
<li><a href="/wiki/Branch_and_bound" title="Branch and bound">Branch &amp; bound</a> or <a href="/wiki/Branch_and_cut" title="Branch and cut">cut</a></li>
</ul>
<ul>
<li><a href="/wiki/Bellman%E2%80%93Ford_algorithm" title="Bellman–Ford algorithm">Bellman–Ford</a></li>
<li><a href="/wiki/Bor%C5%AFvka%27s_algorithm" title="Borůvka's algorithm">Borůvka</a></li>
<li><a href="/wiki/Dijkstra%27s_algorithm" title="Dijkstra's algorithm">Dijkstra</a></li>
<li><a href="/wiki/Floyd%E2%80%93Warshall_algorithm" title="Floyd–Warshall algorithm">Floyd–Warshall</a></li>
<li><a href="/wiki/Johnson%27s_algorithm" title="Johnson's algorithm">Johnson</a></li>
<li><a href="/wiki/Kruskal%27s_algorithm" title="Kruskal's algorithm">Kruskal</a></li>
</ul>
<ul>
<li><a href="/wiki/Dinic%27s_algorithm" title="Dinic's algorithm">Dinic</a></li>
<li><a href="/wiki/Edmonds%E2%80%93Karp_algorithm" title="Edmonds–Karp algorithm">Edmonds–Karp</a></li>
<li><a href="/wiki/Ford%E2%80%93Fulkerson_algorithm" title="Ford–Fulkerson algorithm">Ford–Fulkerson</a></li>
<li><a href="/wiki/Push-relabel_maximum_flow_algorithm" title="Push-relabel maximum flow algorithm" class="mw-redirect">Push-relabel maximum flow</a></li>
</ul>
<ul>
<li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithm</a></li>
<li><a href="/wiki/Hill_climbing" title="Hill climbing">Hill climbing</a></li>
<li><a href="/wiki/Local_search_(optimization)" title="Local search (optimization)">Local search</a></li>
<li><a href="/wiki/Simulated_annealing" title="Simulated annealing">Simulated annealing</a></li>
<li><a href="/wiki/Tabu_search" title="Tabu search">Tabu search</a></li>
</ul>
<ul>
<li><b>Categories</b>
<ul>
<li><a href="/wiki/Category:Optimization_algorithms_and_methods" title="Category:Optimization algorithms and methods">Algorithms and methods</a></li>
<li><a href="/wiki/Category:Heuristic_algorithms" title="Category:Heuristic algorithms">Heuristics</a></li>
</ul>
</li>
<li><b><a href="/wiki/Comparison_of_optimization_software" title="Comparison of optimization software">Software</a></b></li>
</ul>
<ul>
<li><a href="/wiki/Category:Optimization_algorithms_and_methods" title="Category:Optimization algorithms and methods">Algorithms and methods</a></li>
<li><a href="/wiki/Category:Heuristic_algorithms" title="Category:Heuristic algorithms">Heuristics</a></li>
</ul>
<ul>
<li class="nv-view"><a href="/wiki/Template:Areas_of_mathematics" title="Template:Areas of mathematics"><span title="View this template" style=";;background:none transparent;border:none;;">v</span></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Areas_of_mathematics" title="Template talk:Areas of mathematics"><span title="Discuss this template" style=";;background:none transparent;border:none;;">t</span></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Areas_of_mathematics&amp;action=edit"><span title="Edit this template" style=";;background:none transparent;border:none;;">e</span></a></li>
</ul>
<ul>
<li><a href="/wiki/Algebra" title="Algebra">Algebra</a>
<ul>
<li><a href="/wiki/Elementary_algebra" title="Elementary algebra">elementary</a></li>
<li><a href="/wiki/Linear_algebra" title="Linear algebra">linear</a></li>
<li><a href="/wiki/Multilinear_algebra" title="Multilinear algebra">multilinear</a></li>
<li><a href="/wiki/Abstract_algebra" title="Abstract algebra">abstract</a></li>
</ul>
</li>
<li><a href="/wiki/Arithmetic" title="Arithmetic">Arithmetic</a> / <a href="/wiki/Number_theory" title="Number theory">Number theory</a></li>
<li><a href="/wiki/Calculus" title="Calculus">Calculus</a> / <a href="/wiki/Mathematical_analysis" title="Mathematical analysis">Analysis</a></li>
<li><a href="/wiki/Category_theory" title="Category theory">Category theory</a></li>
<li><a href="/wiki/Combinatorics" title="Combinatorics">Combinatorics</a></li>
<li><a href="/wiki/Theory_of_computation" title="Theory of computation">Computation</a></li>
<li><a href="/wiki/Control_theory" title="Control theory">Control theory</a></li>
<li><a href="/wiki/Differential_equation" title="Differential equation">Differential equations</a> / <a href="/wiki/Dynamical_systems_theory" title="Dynamical systems theory">Dynamical systems</a></li>
<li><a href="/wiki/Functional_analysis" title="Functional analysis">Functional analysis</a></li>
<li><a href="/wiki/Game_theory" title="Game theory">Game theory</a></li>
<li><a href="/wiki/Geometry" title="Geometry">Geometry</a>
<ul>
<li><a href="/wiki/Discrete_geometry" title="Discrete geometry">discrete</a></li>
<li><a href="/wiki/Algebraic_geometry" title="Algebraic geometry">algebraic</a></li>
<li><a href="/wiki/Differential_geometry" title="Differential geometry">differential</a></li>
<li><a href="/wiki/Finite_geometry" title="Finite geometry">finite</a></li>
</ul>
</li>
<li><a href="/wiki/Graph_theory" title="Graph theory">Graph theory</a></li>
<li><a href="/wiki/Information_theory" title="Information theory">Information theory</a></li>
<li><a href="/wiki/Lie_theory" title="Lie theory">Lie theory</a></li>
<li><a href="/wiki/Mathematical_logic" title="Mathematical logic">Mathematical logic</a></li>
<li><a href="/wiki/Mathematical_physics" title="Mathematical physics">Mathematical physics</a></li>
<li><a href="/wiki/Mathematical_statistics" title="Mathematical statistics">Mathematical statistics</a></li>
<li><a href="/wiki/Numerical_analysis" title="Numerical analysis">Numerical analysis</a></li>
<li><strong class="selflink">Optimization</strong></li>
<li><a href="/wiki/Probability_theory" title="Probability theory">Probability</a></li>
<li><a href="/wiki/Representation_theory" title="Representation theory">Representation theory</a></li>
<li><a href="/wiki/Set_theory" title="Set theory">Set theory</a></li>
<li><a href="/wiki/Topology" title="Topology">Topology</a></li>
<li><a href="/wiki/Trigonometry" title="Trigonometry">Trigonometry</a></li>
</ul>
<ul>
<li><a href="/wiki/Elementary_algebra" title="Elementary algebra">elementary</a></li>
<li><a href="/wiki/Linear_algebra" title="Linear algebra">linear</a></li>
<li><a href="/wiki/Multilinear_algebra" title="Multilinear algebra">multilinear</a></li>
<li><a href="/wiki/Abstract_algebra" title="Abstract algebra">abstract</a></li>
</ul>
<ul>
<li><a href="/wiki/Discrete_geometry" title="Discrete geometry">discrete</a></li>
<li><a href="/wiki/Algebraic_geometry" title="Algebraic geometry">algebraic</a></li>
<li><a href="/wiki/Differential_geometry" title="Differential geometry">differential</a></li>
<li><a href="/wiki/Finite_geometry" title="Finite geometry">finite</a></li>
</ul>
<ul>
<li><a href="/wiki/Pure_mathematics" title="Pure mathematics">Pure</a></li>
<li><a href="/wiki/Applied_mathematics" title="Applied mathematics">Applied</a></li>
<li><a href="/wiki/Discrete_mathematics" title="Discrete mathematics">Discrete</a></li>
<li><a href="/wiki/Computational_mathematics" title="Computational mathematics">Computational</a></li>
</ul>
<ul>
<li><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31"><b><a href="/wiki/Category:Fields_of_mathematics" title="Category:Fields of mathematics">Category</a></b></li>
<li><img alt="Portal" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28"><b><a href="/wiki/Portal:Mathematics" title="Portal:Mathematics">Mathematics portal</a></b> / <a href="/wiki/Outline_of_mathematics" title="Outline of mathematics">outline</a> / <a href="/wiki/Lists_of_mathematics_topics" title="Lists of mathematics topics">topic lists</a></li>
</ul>
<ul>
<li class="nv-view"><a href="/wiki/Template:Systems_engineering" title="Template:Systems engineering"><span title="View this template" style=";;background:none transparent;border:none;;">v</span></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Systems_engineering" title="Template talk:Systems engineering"><span title="Discuss this template" style=";;background:none transparent;border:none;;">t</span></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Systems_engineering&amp;action=edit"><span title="Edit this template" style=";;background:none transparent;border:none;;">e</span></a></li>
</ul>
<ul>
<li><a href="/wiki/Aerospace_engineering" title="Aerospace engineering">Aerospace engineering</a></li>
<li><a href="/wiki/Biological_systems_engineering" title="Biological systems engineering">Biological systems engineering</a></li>
<li><a href="/wiki/Configuration_management" title="Configuration management">Configuration management</a></li>
<li><a href="/wiki/Earth_systems_engineering_and_management" title="Earth systems engineering and management">Earth systems engineering and management</a></li>
<li><a href="/wiki/Enterprise_systems_engineering" title="Enterprise systems engineering">Enterprise systems engineering</a></li>
<li><a href="/wiki/Performance_engineering" title="Performance engineering">Performance engineering</a></li>
<li><a href="/wiki/Reliability_engineering" title="Reliability engineering">Reliability engineering</a></li>
<li><a href="/wiki/Safety_engineering" title="Safety engineering">Safety engineering</a></li>
</ul>
<ul>
<li><a href="/wiki/Requirements_analysis" title="Requirements analysis">Requirements analysis</a></li>
<li><a href="/wiki/Functional_specification" title="Functional specification">Functional specification</a></li>
<li><a href="/wiki/System_integration" title="System integration">System integration</a></li>
<li><a href="/wiki/Verification_and_validation" title="Verification and validation">Verification and validation</a></li>
<li><a href="/wiki/Design_review" title="Design review">Design review</a></li>
</ul>
<ul>
<li><a href="/wiki/Business_process" title="Business process">Business process</a></li>
<li><a href="/wiki/System" title="System">System</a></li>
<li><a href="/wiki/System_lifecycle" title="System lifecycle">System lifecycle</a></li>
<li><a href="/wiki/V-Model" title="V-Model">V-Model</a></li>
<li><a href="/wiki/Systems_development_life_cycle" title="Systems development life cycle">Systems development life cycle</a></li>
</ul>
<ul>
<li><a href="/wiki/Decision-making" title="Decision-making">Decision-making</a></li>
<li><a href="/wiki/Function_model" title="Function model">Function modelling</a></li>
<li><a href="/wiki/IDEF" title="IDEF">IDEF</a></li>
<li><strong class="selflink">Optimization</strong></li>
<li><a href="/wiki/Planning" title="Planning">Planning</a></li>
<li><a href="/wiki/Statistics" title="Statistics">Statistical analysis</a></li>
<li><a href="/wiki/System_dynamics" title="System dynamics">System dynamics</a></li>
<li><a href="/wiki/Systems_Modeling_Language" title="Systems Modeling Language">Systems Modeling Language</a></li>
<li><a href="/wiki/Systems_analysis" title="Systems analysis">Systems analysis</a></li>
<li><a href="/wiki/Systems_modeling" title="Systems modeling">Systems modeling</a></li>
<li><a href="/wiki/Work_breakdown_structure" title="Work breakdown structure">Work breakdown structure</a></li>
</ul>
<ul>
<li><a href="/wiki/James_S._Albus" title="James S. Albus">James S. Albus</a></li>
<li><a href="/wiki/Wernher_von_Braun" title="Wernher von Braun">Wernher von Braun</a></li>
<li><a href="/wiki/Harold_Chestnut" title="Harold Chestnut">Harold Chestnut</a></li>
<li><a href="/wiki/Arthur_David_Hall_III" title="Arthur David Hall III">Arthur David Hall III</a></li>
<li><a href="/wiki/Derek_Hitchins" title="Derek Hitchins">Derek Hitchins</a></li>
<li><a href="/wiki/Robert_E._Machol" title="Robert E. Machol">Robert E. Machol</a></li>
<li><a href="/wiki/Simon_Ramo" title="Simon Ramo">Simon Ramo</a></li>
<li><a href="/wiki/Joseph_Francis_Shea" title="Joseph Francis Shea">Joseph Francis Shea</a></li>
<li><a href="/wiki/John_N._Warfield" title="John N. Warfield">John N. Warfield</a></li>
</ul>
<ul>
<li><a href="/wiki/Control_engineering" title="Control engineering">Control engineering</a></li>
<li><a href="/wiki/Computer_engineering" title="Computer engineering">Computer engineering</a></li>
<li><a href="/wiki/Industrial_engineering" title="Industrial engineering">Industrial engineering</a></li>
<li><a href="/wiki/Operations_research" title="Operations research">Operations research</a></li>
<li><a href="/wiki/Project_management" title="Project management">Project management</a></li>
<li><a href="/wiki/Quality_management" title="Quality management">Quality management</a></li>
<li><a href="/wiki/Software_engineering" title="Software engineering">Software engineering</a></li>
</ul>
<ul>
<li><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31"> <b><a href="/wiki/Category:Systems_engineering" title="Category:Systems engineering">Category</a></b></li>
<li><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png" width="12" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376"> <b><a href="//commons.wikimedia.org/wiki/Category:Systems_engineering" class="extiw" title="commons:Category:Systems engineering">Commons</a></b></li>
</ul>
